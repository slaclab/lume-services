{"config":{"lang":["en"],"separator":"[\\s\\-\\.]","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LUME-services","text":"<p> <p>LUME-services provides a set of common services for model workflow orchestration:</p> <ul> <li>Contextualized file service (local/mounted for containerized services)</li> <li>Model database service for registering models and metadata</li> <li>Results database service for storing model output</li> <li>Scheduling service for deploying model runs with Prefect</li> </ul> <p>Not yet implemented but eventually this will also include HPC interfaces slurm etc.: </p> <ul> <li>Abstracted HPC service for integration with scientific computing infrastructure.</li> </ul> <p>The intent of these tools are to streamline the packaging of modeling/simulation code by providing contextual flexibility with respect to service clusters. The framework uses a configuration provided at runtime to establish connections will all services. This code design facilitate portability from local, to distributed dev, or production environments with the same code, but by setting the enironment variables. Models revisions are tracked using git.</p> <p></p> <p>The microservice interfaces developed for <code>LUME-services</code> are isolated, which allows for abstraction and modularization of updates and rollback, and prioritize scalability, maintainability, and parallelized development and maintenance. Services can be deployed in clusters of containers or on remote resources subject to user constraints. Example configurations of Docker and Kubernetes clusters shown below. * Docker:  * Kubernetes: </p> <p></p> <p>Alternatively, users can execute run workflows directly in their process by configuring a local backend.</p> <p>Features:</p> <ul> <li>Standard schema for managing model metadata</li> <li>Differentiated local and remote execution environments</li> <li>Project template for building packaged models</li> <li>Interfaces for model registry database and results database</li> <li>APIs for scheduling workflow runs, registering models, and collecting results from the database.</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>The development environment launch containerized services in Docker containers. Use of these tools requires installation of the Docker Engine</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>This package can be installed from GitHub using: <pre><code>pip install git+https://github.com/slaclab/lume-services.git\n</code></pre></p>"},{"location":"cli/","title":"LUME-services cli","text":"<p>LUME-services is packaged with a number of cli tools. See TODO for next steps.</p>"},{"location":"cli/#main","title":"main","text":"<p>Usage:</p> <pre><code>main [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/#docker","title":"docker","text":"<p>Usage:</p> <pre><code>main docker [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/#start-services","title":"start-services","text":"<p>Start cluster of docker services.</p> <p>Usage:</p> <pre><code>main docker start-services [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --project_name TEXT  Name of docker project for labeling docker-compose.\n  --timeout FLOAT      Time allotted for successful docker-compose startup.\n  --pause FLOAT        Pause between successive polls of docker-compose\n                       services.\n  --persist BOOLEAN    Persist data between sessions.\n  --help               Show this message and exit.\n</code></pre> <p>"},{"location":"config/","title":"Configuration","text":"<p>LUME-services uses injection for runtime configuration of services. This means that programs can use the same code in a number of different environments simply by setting environment variables.</p>"},{"location":"config/#the-lumeservicessettings-object","title":"The LUMEServicesSettings object","text":"<p>Applications that LUME-services tools (the MongoDB implementation of Results Database and MySQL model database), can use the configuration tools packaged with LUME-services directly by calling the configure script during code execution: <pre><code>from lume_services.config import configure, LUMEServicesSettings\nfrom lume_services.services.models.db import ModelDBConfig\nfrom lume_services.services.results.mongodb import MongodbResultsDBConfig\nfrom lume_services.services.scheduling.backends.server import (\n    PrefectAgentConfig,\n    PrefectConfig,\n    PrefectServerConfig,\n)\nfrom lume_services import config\n\nfrom lume_services.services.files.filesystems import (\n    MountedFilesystem,\n)\n\nmodel_db_config = ModelDBConfig(\n    host=\"127.0.0.1\",\n    port=3306,\n    user=\"root\",\n    password=\"test\",\n    database=\"model_db\",\n)\n\nresults_db_config = MongodbResultsDBConfig(\n    port=27017,\n    host=\"127.0.0.1\",\n    username=\"root\",\n    database=\"model_db\",\n    password=\"test\",\n)\n\nprefect_config = PrefectConfig(\n    server=PrefectServerConfig(\n        host=\"http://localhost\", \n        host_port=4200, \n    ),\n)\n\nagent=PrefectAgentConfig(\n  host=\"http://localhost\", \n  host_port=5000,\n  backend=\"server\",\n  debug=True,\n)\n\nmounted_filesystem = MountedFilesystem(\n        mount_path=\"~/sandbox/lume-services\",\n        mount_alias=\"/User/my_user/data\",\n        identifier=\"mounted\",\n        mount_type=\"DirectoryOrCreate\",\n    )\n\nsettings = config.LUMEServicesSettings(\n    model_db=model_db_config,\n    results_db=results_db_config,\n    prefect=prefect_config,\n    backend=\"docker\",\n    mounted_filesystem=mounted_filesystem,\n)\n\nconfig.configure(settings)\n</code></pre></p> <p>Configurations are held on LUMEServicesSettings objects. For more information on how configurations are handled with injection, see <code>developer/configuration</code>.</p>"},{"location":"config/#configure-from-environment","title":"Configure from environment","text":"<p>The LUME-services environment may alternatively be configured using environment variables by calling the configure method without an argument.</p> <pre><code>from lume_services.config import configure\n\nconfigure()\n</code></pre> <p>Relevant environment variables (these tables are built using script in <code>scripts/build_configuration_table.py</code> automatically generated w/ the GitHub build action defined in <code>.github/workflows/build_docs.yml</code>, see **note beneath):</p>"},{"location":"config/#base-configuration","title":"Base Configuration","text":"Name Type Default LUME_BACKEND string local"},{"location":"config/#filesystem-configuration","title":"Filesystem Configuration","text":"Name Type Default LUME_MOUNTED_FILESYSTEM__IDENTIFIER string mounted LUME_MOUNTED_FILESYSTEM__MOUNT_PATH string LUME_MOUNTED_FILESYSTEM__MOUNT_ALIAS string LUME_MOUNTED_FILESYSTEM__MOUNT_TYPE string DirectoryOrCreate"},{"location":"config/#model-database","title":"Model Database","text":"Name Type Default LUME_MODEL_DB__HOST string LUME_MODEL_DB__PORT integer LUME_MODEL_DB__USER string LUME_MODEL_DB__PASSWORD string LUME_MODEL_DB__DATABASE string LUME_MODEL_DB__CONNECTION__POOL_SIZE integer LUME_MODEL_DB__CONNECTION__POOL_PRE_PING boolean True LUME_MODEL_DB__DIALECT_STR string mysql+pymysql"},{"location":"config/#results-database","title":"Results Database","text":"Name Type Default LUME_RESULTS_DB__DATABASE string LUME_RESULTS_DB__USERNAME string LUME_RESULTS_DB__HOST string LUME_RESULTS_DB__PASSWORD string LUME_RESULTS_DB__PORT integer LUME_RESULTS_DB__AUTHMECHANISM string DEFAULT LUME_RESULTS_DB__OPTIONS object {}"},{"location":"config/#scheduling-service","title":"Scheduling Service","text":"Name Type Default LUME_PREFECT__SERVER__TAG string core-1.4.0 LUME_PREFECT__SERVER__HOST string http://localhost LUME_PREFECT__SERVER__HOST_PORT string 4200 LUME_PREFECT__SERVER__HOST_IP string 127.0.0.1 LUME_PREFECT__UI__HOST string http://localhost LUME_PREFECT__UI__HOST_PORT string 8080 LUME_PREFECT__UI__HOST_IP string 127.0.0.1 LUME_PREFECT__UI__APOLLO_URL string http://localhost:4200/graphql LUME_PREFECT__TELEMETRY__ENABLED boolean True LUME_PREFECT__AGENT__HOST string http://localhost LUME_PREFECT__AGENT__HOST_PORT string 5000 LUME_PREFECT__HOME_DIR string ~/.prefect LUME_PREFECT__DEBUG boolean False LUME_PREFECT__BACKEND string server LUME_PREFECT__IMAGE string jgarrahan/lume-services-prefect:latest LUME_PREFECT__ISOLATED boolean False <p>**note: I haven't validated these table. Sorry! You can validate the  fields here, which are constructed using Pydantic's _env_nested_delimiter configuration setting.</p>"},{"location":"config/#custom-configuration","title":"Custom configuration","text":"<p>The <code>lume-services.config.configure</code> method provides tooling for one specific architecture configuration, but other users may want to use different database implementations (see services). These applications can define their own methods using the same interface by subclassing the base class of artifacts that is injected into each service:</p> <p>base class -&gt; service injected into:</p> <ul> <li><code>lume_services.services.files.filesystems.filesystem</code> -&gt; <code>lume_services.services.files.service</code></li> <li><code>lume_services.services.results.db</code> -&gt; <code>lume_services.services.results.service</code></li> <li><code>lume_services.services.models.db.db</code> -&gt; <code>lume_services.services.models.services</code></li> <li><code>lume_services.services.scheduling.backends.backend</code> -&gt; <code>lume_services.services.scheduling.service</code></li> <li>(Downstream, for things like slurm scheduling) <code>lume_services.hpc.provider</code> -&gt; <code>lume_services.services.hpc.service</code></li> </ul>"},{"location":"demo/","title":"Demo","text":"<p>This demo walks through the creation of a model compatible with LUME-services tooling. You will:</p> <ol> <li>Create a GitHub repository for the demo model  </li> <li>Build a templated project using <code>lume-services-model-template</code> </li> <li>Register your model using the LUME-services API and store deployment information into a MySQL database  </li> <li>Run your model, storing results in MongoDB database  </li> <li>Retrieve model results using LUME-services  </li> </ol> <p>Requirements:  </p> <ul> <li>GitHub account and SSH keys installed according to their instructions.  </li> <li>Docker desktop for launching the development environment  </li> <li>Conda installation for the management of  Python environments  </li> <li>Either a DockerHub account or Stanford account to use the Stanford Container Registry provided by the code.stanford.edu effort.  </li> </ul>"},{"location":"demo/#package-a-model","title":"Package a model","text":""},{"location":"demo/#1-create-a-repository-for-your-project-on-github","title":"1. Create a repository for your project on GitHub","text":"<p>Using your GitHub account, create an empty repository named <code>my-model</code> (feel free to sub this with whatever you'd like).</p>"},{"location":"demo/#2-create-project","title":"2: Create project","text":"<p>Clone <code>lume-services-model-template</code> and navigate to repository: <pre><code>git clone https://github.com/slaclab/lume-services-model-template\ncd lume-services-model-template\n</code></pre></p> <p>Create your environment:</p> <pre><code>conda env create -f environment.yml\nconda activate lume-services-model-template\n</code></pre> <p>Create your project. The <code>-o</code> flag indicates the directory where you'd like the resulting repo to be stored. For now, let's create it in the repo root: <pre><code>cookiecutter template -o $(pwd)\n</code></pre></p> <p>Answer the prompts <pre><code>author: &lt;your name&gt;\nemail: &lt;your email address&gt;\ngithub_username: &lt;your GitHub username&gt;\ngithub_url: &lt;url of GitHub repo you've just created&gt;\nproject_name: My Model\n</code></pre> Use autogenerated in brackets by just pressing enter. <pre><code>repo_name [my-model]:\npackage [my_model]:\nmodel_class [MyModel]:\n</code></pre></p> <p><pre><code>Select container_registry:\n1 - DockerHub\n2 - Stanford Container Registry\nChoose from 1, 2 [1]:\n</code></pre> If you plan to use the Stanford Container Registry, enter <code>2</code>. Otherwise, enter <code>1</code> for DockerHub. Now, enter the username you use for the contaner registry:</p> <pre><code>container_username: &lt;your registry username&gt; \n</code></pre> <p>Use example variables file packaged with this repository. This requires the full path to the file as the hook runs in the root of the generated project and cannot reference the context of the execution directory. <pre><code>model_config_file: /full/path/to/lume-services-model-template/examples/variables.yml\n</code></pre></p> <p>Cookiecutter will render a repository using the template with the structure:</p> <p><pre><code>my-model\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 MANIFEST.in\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 _entrypoint.sh\n\u251c\u2500\u2500 conftest.py\n\u251c\u2500\u2500 dev-environment.yml\n\u251c\u2500\u2500 environment.yml\n\u251c\u2500\u2500 my_model\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 _image.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 _version.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.yml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 flow.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 model.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 test_flow.py\n\u251c\u2500\u2500 pytest.ini\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 setup.cfg\n\u251c\u2500\u2500 setup.py\n\u2514\u2500\u2500 versioneer.py\n</code></pre> You can generate this tree from your generated project by running: <pre><code>tree my-model\n</code></pre></p> <p>Now, navigate to the directory where you've created your repository: <pre><code>cd my-model\n</code></pre></p>"},{"location":"demo/#3-configure-generated-repo-to-use-github-repo-as-the-origin","title":"3. Configure generated repo to use GitHub repo as the origin:","text":"<p>Replace username and brackets in the below command: <pre><code>git remote add origin git@github.com:&lt;your GitHub username&gt;/my-model.git\ngit push --set-upstream -f origin main\n</code></pre></p> <p>Note: If your local git configuration defaults to creating a branch with name <code>master</code>, you'll have to rename the local branch to main using: <pre><code>git branch -m master main\n</code></pre></p>"},{"location":"demo/#4-set-up-model","title":"4. Set up model","text":"<p>Replace ellipses in the <code>evaluate</code> method of <code>my_model/model.py</code> with: <pre><code>self.output_variables[\"output1\"].value = np.random.uniform(\n    input_variables[\"input1\"].value,  # lower dist bound\n    input_variables[\"input2\"].value,  # upper dist bound\n    (50, 50),\n)\nself.output_variables[\"output2\"].value = input_variables[\"input1\"].value\nself.output_variables[\"output3\"].value = input_variables[\"input2\"].value\n</code></pre></p> <p>Because we've introduced numpy as new dependency, add a numpy import to the top of the file so the model.py file looks like:</p> <pre><code>import copy\nfrom typing import Dict\nimport numpy as np\nfrom lume_model.models import BaseModel\nfrom lume_model.variables import InputVariable, OutputVariable\nfrom my_model import INPUT_VARIABLES, OUTPUT_VARIABLES\n\nclass MyModel(BaseModel):\n    input_variables = copy.deepcopy(INPUT_VARIABLES)\n    output_variables = copy.deepcopy(OUTPUT_VARIABLES)\n\n    def __init__(self, **settings_kwargs):\n        \"\"\"Initialize the model. If additional settings are required, they can be\n        passed and handled here. For models that are wrapping model loads\n        from other frameworks, this can be used for loading weights, referencing\n        data files, etc.\n\n        \"\"\"\n        super().__init__()\n\n        # handle settings if any\n        # if settings_kwargs is not None:\n        # ...\n\n\n    def evaluate(\n        self, input_variables: Dict[str, InputVariable]\n    ) -&gt; Dict[str, OutputVariable]:\n        \"\"\"The evaluate method accepts input variables, performs the model execution,\n        then returns a dictionary mapping variable name to output variable.\n\n        Args:\n            input_variables (Dict[str, InputVariable]): Dictionary of LUME-model input\n                variables with values assigned.\n\n        Returns:\n            Dict[str, OutputVariable]: Dictionary of LUME-model output variables with\n                values assigned.\n\n        \"\"\"\n\n        self.output_variables[\"output1\"].value = np.random.uniform(\n            input_variables[\"input1\"].value,  # lower dist bound\n            input_variables[\"input2\"].value,  # upper dist bound\n            (50, 50),\n        )\n        self.output_variables[\"output2\"].value = input_variables[\"input1\"].value\n        self.output_variables[\"output3\"].value = input_variables[\"input2\"].value\n\n\n        return self.output_variables\n</code></pre> <p>Add <code>numpy</code> to the <code>dev-environment.yml</code>, <code>environment.yml</code>, and <code>requirements.txt</code> files.</p>"},{"location":"demo/#5-set-up-flow","title":"5. Set up flow","text":"<p>In order for our flow to run, we must edit the code in <code>my_model/flow.py</code>. First, delete the <code>preprocessing_task</code> code, as we won't be using it. Second, edit <code>format_file</code> such that the file holds a string representation of the sum of <code>output2</code> and <code>output3</code>.</p> <pre><code>@task(log_stdout=True)\ndef format_file(output_variables):\n    text = str(output_variables[\"output2\"].value + output_variables[\"output3\"].value)\n    return text\n</code></pre>"},{"location":"demo/#6-create-development-environment","title":"6. Create development environment","text":"<p>Now, create an environment for working with your model package:</p> <pre><code>conda env create -f dev-environment.yml\nconda activate my-model-dev\n</code></pre> <p>Install your package into this environment: <pre><code>pip install -e .\n</code></pre></p>"},{"location":"demo/#7-set-up-tests","title":"7. Set up tests","text":"<p>In <code>my_model/tests/test_flow.py</code> modify the <code>test_flow_execution</code> function, adding the <code>tmp_path</code> fixture to the signature and passing <code>filename=f\"{tmp_dir}/test_file.txt\"</code> and <code>filesytem_identifier=\"local\"</code> to the run method. The resulting code should look like:</p> <pre><code>def test_flow_execution(tmp_path):\n    flow.run(filename=f\"{tmp_path}/test_file.txt\", filesystem_identifier=\"local\")\n</code></pre> <p>Navigate back to your <code>my-model</code> directory. You can now test your flow locally by running: <pre><code>pytest .\n</code></pre></p>"},{"location":"demo/#8-run-tests-with-github-actions","title":"8. Run tests with GitHub actions","text":"<p>Check in all of your code, and push to GitHub.</p> <pre><code>git add .\ngit commit -m \"Check in formatted repo\"\ngit push\n</code></pre> <p>In your browser, navigate to your GitHub repository at <code>https://github.com/&lt;your GitHub username&gt;/my-model/actions</code>. The testing workflow configured in <code>.github/workflows/tests.yml</code> will run automatically on pushes to your main branch and you can monitor the success of these tests from the GitHub actions console for the repo. The workflow tests your package against a matrix of Python versions (3.7, 3.8, 3.9) on the latest Ubuntu build. You can expand this matrix if desired using GitHub actions matrix options.</p>"},{"location":"demo/#9-configure-github-actions-build","title":"9. Configure GitHub actions build","text":""},{"location":"demo/#stanford-container-registry","title":"Stanford Container Registry","text":"<p>SLAC users can take advantage of the Stanford Container Registry to store their containers. The steps for configuring your project to use the registry are as follows:</p> <p>9.1 Create an API token at https://code.stanford.edu/-/profile/personal_access_tokens. For <code>Token name</code> enter <code>My Model</code>. Optionally choose an expiration date. This can be whatever you'd like, but the GitHub action secret defined in step 3. will need to be updated with a new value after this expiration. Record this API token for use in steps 9.3 and 9.4.</p> <p></p> <p>9.2 Create a project using Stanford Code. In the <code>Project name</code> field, write <code>My Model</code>. Select internal visibility level.</p> <p></p> <p>9.3 Add the token to your GitHub repository secrets. Navigate to your repository settings. In the left sidebar, click <code>Secrets</code>, <code>Actions</code>, then <code>New repository secret</code>. Type <code>SCR_PAT</code> into the name, and your generated API key into the value. Repeat this process to set a secret named <code>STANFORD_USERNAME</code> to your Stanford username.</p> <p>9.4 Login to the container registry on your machine.</p> <p>Replace the &lt;&gt; with the appropriate values:</p> <pre><code>echo &lt;your token&gt; | docker login --username &lt;your Stanford username&gt; --password-stdin http://scr.svc.stanford.edu\n</code></pre>"},{"location":"demo/#dockerhub","title":"DockerHub","text":"<p>The LUME-services model template is pre-configured to publish the container image to DockerHub. In order to use this workflow, authentication for the repository must be configured using GitHub secrets.</p> <ol> <li>Navigate to the settings on your repository.  </li> </ol> <p></p> <ol> <li> <p>In the left sidebar, click <code>Secrets</code>, <code>Actions</code>, then <code>New repository secret</code>. Type <code>DOCKER_USERNAME</code> into the name, and your DockerHub username into the value and click <code>Add secret</code> to save. Repeat this process to create a <code>DOCKER_PASSWORD</code> secret with your DockerHub password as the value.  </p> </li> <li> <p>In the left sidebar, click the top level <code>Actions</code> (not the one under <code>Secrets</code>), and choose <code>General</code>. Go to the <code>Workflow permissions</code> section and select <code>Read and write permissions</code>. This will allow the workflow triggered by a new release to publish your model to your repository.</p> </li> </ol>"},{"location":"demo/#10-create-a-release","title":"10. Create a release","text":"<p>Create a tagged release for your model. Navigate to <code>https://github.com/&lt;your GitHub username&gt;/my-model/releases</code> -&gt; <code>Draft a new release</code></p> <p></p> <p>Under choose tag, type <code>v0.0.1</code> (this is a development tag, semantic versioning for releases formally starts at v0.1). You can enter the same for the title and may enter some description, but this is optional. Check the pre-release box at the bottom of the page and click the button to publish your release.</p> <p>The release will trigger a GitHub action workflow for your project, which you can monitor at <code>https://github.com/&lt;your GitHub username&gt;/my-model/actions</code>. Once the build completes, your image will be available at <code>https://hub.docker.com/u/&lt;your DockerHub username&gt;</code> or <code>https://code.stanford.edu/&lt;your Stanford username&gt;/my-model/container_registry</code>.</p>"},{"location":"demo/#deploying-a-model-to-production","title":"Deploying a model to production:","text":"<p>The below steps mimic a production deployment workflow.</p>"},{"location":"demo/#11-start-services-with-docker-compose","title":"11. Start services with docker-compose","text":"<p>LUME-services is packaged with a command line utility for launching the development environment, a docker-compose application with all services packaged and configurable via environment variables.</p> <p></p> <p>Create a new console window. If you haven't, clone the lume-services repository. <pre><code>git clone https://github.com/jacquelinegarrahan/lume-services\n</code></pre> Navigate to your local <code>lume-services</code> repository and activate your environment.  <pre><code>cd lume-services\nconda activate my-model-dev\n</code></pre></p> <p>Configure your environment variables.</p> <pre><code>source docs/examples/demo.env\n</code></pre> <p>Ensure docker has the ability to bind mount files from your conda environment. If using docker desktop, go to Preferences -&gt; Resources -&gt; File Sharing and ensure the path to <code>/opt/miniconda3/envs/my-model-dev/lib/python3.9/site-packages/lume_services</code> is included within your rules (adjusting the path to your local installation as necessary). If this path is not available to docker, then you will likely encounter <code>Error response from daemon: invalid mount config for type \"bind\": bind source path does not exist</code> when attempting to start up lume-services below.</p> <p>If you are using the Stanford Container Registry, you'll have to set additional environment variables, you <code>STANFORD_USERNAME</code> and <code>SCR_PAT</code> from step 9. <pre><code>export STANFORD_USERNAME=&lt;your_stanford_username&gt;\nexport SCR_PAT=&lt;your PAT from step 9&gt;\n</code></pre></p> <p>Next start up your services: <pre><code>lume-services docker start-services\n</code></pre></p> <p>If you have existing services running on the ports defined in <code>docs/examples/demo.env</code>, you may need to edit the file to use free ports.  </p> <p>Once the console logs a message about passed health checks, you've started all services successfully. You can inspect the services using Docker Desktop:</p> <p></p> <p>And access the UI using your browser at http://localhost:8080.</p>"},{"location":"demo/#12-run-the-notebook-and-register-your-model","title":"12. Run the notebook and register your model","text":"<p>Create a new console window. Navigate to your local <code>lume-services</code> repository and activate your environment.  <pre><code>cd lume-services\nconda activate my-model-dev\n</code></pre> Source the appropriate environment:</p> <p><pre><code>source docs/examples/demo.env\n</code></pre> Open the demo notebook and continue the remainder of the demo by running each cell: <pre><code>jupyter lab docs/examples/Demo.ipynb\n</code></pre></p>"},{"location":"model_packaging/","title":"Model packaging (WIP docs)","text":"<p>The LUME project provides a standard interface for formatting models for use with LUME services in LUME-model. LUME-services relies on this interface for templating the user packages served over orchestration tooling. The formatting strategy consists of:</p> <ol> <li>Model variables</li> <li>Model execution class</li> </ol>"},{"location":"model_packaging/#variables","title":"Variables","text":"<p>LUME-model variables define the inputs and outputs to the model and enforce minimal metadata on the variables. The variables are divided into categories, with <code>InputVariable</code> and <code>OutputVariable</code> as bases, then further subdivided by data.</p> <p>Variables may be defined either programatically: <pre><code>from lume_model.variables import ScalarInputVariable, ScalarOutputVariable\n\ninput_variable = ScalarInputVariable(name=\"test_input\", default=0.1, value_range=[1, 2])\noutput_variable = ScalarOutputVariable(name=\"test_output\")\n</code></pre></p> <p>Or using a YAML spec:</p> <p><code>my_variables.yml</code> <pre><code>input_variables:\n  input1:\n      name: input1\n      type: scalar\n      default: 1\n      range: [0, 256]\n\n  input2:\n      name: input2\n      type: scalar\n      default: 2.0\n      range: [0, 256]\n\noutput_variables:\n  output1:\n    name: output1\n    type: image\n    x_label: \"value1\"\n    y_label: \"value2\"\n    axis_units: [\"mm\", \"mm\"]\n    x_min: 0\n    x_max: 10\n    y_min: 0\n    y_max: 10\n\n  output2:\n    name: output2\n    type: scalar\n\n  output3:\n    name: output3\n    type: scalar\n</code></pre></p> <p>And subsequently loaded using: <pre><code>from lume_model.utils import variables_from_yaml\n\nwith open(\"my_variables.yml\", \"r\") as f:\n  input_variables, output_variables = variables_from_yaml(f)\n</code></pre></p>"},{"location":"model_packaging/#model","title":"Model","text":"<p>The LUME-model class provides a minimal and extensible base for formatting models. The \u2018evaluate\u2019 method must be implemented on the subclass and accept a dictionary mapping input variable name to LUME-model input variable. The output of the <code>evaluate</code> method will be the dictionary output. The example packaged in the demo implements the following:</p> <pre><code>import copy\nfrom typing import Dict\nimport numpy as np\nfrom lume_model.models import BaseModel\nfrom lume_model.variables import InputVariable, OutputVariable\nfrom my_model import INPUT_VARIABLES, OUTPUT_VARIABLES\n\nclass MyModel(BaseModel):\n    input_variables = copy.deepcopy(INPUT_VARIABLES)\n    output_variables = copy.deepcopy(OUTPUT_VARIABLES)\n\n    def __init__(self, **settings_kwargs):\n        \"\"\"Initialize the model. If additional settings are required, they can be\n        passed and handled here. For models that are wrapping model loads\n        from other frameworks, this can be used for loading weights, referencing\n        data files, etc.\n\n        \"\"\"\n        super().__init__()\n\n        # handle settings if any\n        # if settings_kwargs is not None:\n        # ...\n\n\n    def evaluate(\n        self, input_variables: Dict[str, InputVariable]\n    ) -&gt; Dict[str, OutputVariable]:\n        \"\"\"The evaluate method accepts input variables, performs the model execution,\n        then returns a dictionary mapping variable name to output variable.\n\n        Args:\n            input_variables (Dict[str, InputVariable]): Dictionary of LUME-model input\n                variables with values assigned.\n\n        Returns:\n            Dict[str, OutputVariable]: Dictionary of LUME-model output variables with\n                values assigned.\n\n        \"\"\"\n\n        self.output_variables[\"output1\"].value = np.random.uniform(\n            input_variables[\"input1\"].value,  # lower dist bound\n            input_variables[\"input2\"].value,  # upper dist bound\n            (50, 50),\n        )\n        self.output_variables[\"output2\"].value = input_variables[\"input1\"].value\n        self.output_variables[\"output3\"].value = input_variables[\"input2\"].value\n\n\n        return self.output_variables\n</code></pre> <p>The class can be extended to absorb other methods, accept custom kwargs, etc.</p>"},{"location":"model_packaging/#python-package","title":"Python Package","text":"<p>The LUME-services-model-template provides a cookiecutter. The cookiecutter generates a LUME-services-compatible Python project with the following structure:</p> <pre><code>&lt;your project&gt;\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 MANIFEST.in\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 _entrypoint.sh\n\u251c\u2500\u2500 conftest.py\n\u251c\u2500\u2500 dev-environment.yml\n\u251c\u2500\u2500 environment.yml\n\u251c\u2500\u2500 &lt;package name&gt;\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 _image.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 _version.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.yml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 flow.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 model.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 test_flow.py\n\u251c\u2500\u2500 pytest.ini\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 setup.cfg\n\u251c\u2500\u2500 setup.py\n\u2514\u2500\u2500 versioneer.py\n</code></pre> <p>The following steps describe use of this template.</p>"},{"location":"model_packaging/#1-create-a-repository-for-your-project-on-github","title":"1. Create a repository for your project on GitHub","text":"<p>Using your GitHub account, create an empty repository with your project name.</p>"},{"location":"model_packaging/#2-compile-a-yaml-for-your-model-input-output-variables","title":"2. Compile a YAML for your model input + output variables","text":"<p>Create a YAML using the LUME-model variable spec.</p>"},{"location":"model_packaging/#2-create-project-using-template","title":"2: Create project using template","text":"<p>Clone <code>lume-services-model-template</code> and navigate to repository: <pre><code>git clone https://github.com/slaclab/lume-services-model-template\ncd lume-services-model-template\n</code></pre></p> <p>Create the template environment:</p> <pre><code>conda env create -f environment.yml\nconda activate lume-services-model-template\n</code></pre> <p>Create your project. The <code>-o</code> flag indicates the directory where you'd like the resulting repo to be stored. For now, let's create it in the repo root: <pre><code>cookiecutter template -o $(pwd)\n</code></pre></p> <p>Answer the prompts <pre><code>author: &lt;your name&gt;\nemail: &lt;your email address&gt;\ngithub_username: &lt;your GitHub username&gt;\ngithub_url: &lt;url of GitHub repo you've just created&gt;\nproject_name: &lt;Name of GitHub repository&gt;\n</code></pre> You can use your own names below or use auto-generated values in the brackets by pressing enter. <pre><code>repo_name [...]:\npackage [...]:\nmodel_class [...]:\n</code></pre></p> <p>The template contains GitHub actions for building a Docker image from the package that will be used to run workflows in the distributed environment. If you plan to use the Stanford Container Registry provided by the code.stanford.edu effort, enter <code>2</code>. Otherwise, enter <code>1</code> for DockerHub. See (model_packaging.md#registries) </p> <p><pre><code>Select container_registry:\n1 - DockerHub\n2 - Stanford Container Registry\nChoose from 1, 2 [1]:\n</code></pre> Now, enter the username you use for the contaner registry:</p> <p><pre><code>container_username: &lt;your registry username&gt; \n</code></pre> And the full path to your variable file compiled in #2.  <pre><code>model_config_file: &lt;path/to/you/variable.yml&gt;\n</code></pre></p> <p>Now, navigate to the directory where you've created your repository: <pre><code>cd &lt;your project name&gt;\n</code></pre></p>"},{"location":"model_packaging/#3-configure-generated-repo-to-use-github-repo-as-the-origin","title":"3. Configure generated repo to use GitHub repo as the origin:","text":"<p>Replace username and brackets in the below command: <pre><code>git remote add origin git@github.com:&lt;your GitHub username&gt;/&lt;your project&gt;.git\ngit push --set-upstream -f origin main\n</code></pre></p> <p>Note: If your local git configuration defaults to creating a branch with name <code>master</code>, you'll have to rename the local branch to main using: <pre><code>git branch -m master main\n</code></pre></p>"},{"location":"model_packaging/#4-set-up-model","title":"4. Set up model","text":"<p>You can now update your model to fit your needs provided that the <code>evaluate</code> method accepts a dictionary mapping input variable name to LUME-model input variable and the method returns a dictionary mapping variable name to LUME-model output variable.</p> <p>Edit the environment.yml files in the repository to reflect the project dependencies. No additional dependencies will be downloaded for pip packages. This means all dependencies must be provided in the environment.yml of the project. </p>"},{"location":"model_packaging/#5-set-up-flow","title":"5. Set up flow","text":"<p>In order for our flow to run, we must edit the code in <code>&lt;your project&gt;/flow.py</code>. The comments in the <code>flow.py</code> should serve as a guide to setting up the flow.</p> <p>** Notes:</p> <p>Prefect's parameters must be json serializable Special datetime parameter uses pendulum to parse datetime strings Pass datetimes as strings by calling <code>my_datetime.isoformat()</code></p>"},{"location":"model_packaging/#6-create-development-environment","title":"6. Create development environment","text":"<p>Now, create an environment for working with your model package:</p> <pre><code>conda env create -f dev-environment.yml\nconda activate &lt;your-project&gt;-dev\n</code></pre> <p>Install your package into this environment: <pre><code>pip install -e .\n</code></pre></p>"},{"location":"model_packaging/#7-github-actions","title":"7. GitHub actions","text":"<p>The template configures a number of actions to run against the repository once checked in to the GitHub origin. Statuses of these workflows will be available in our GitHub repository at <code>https://github.com/&lt;your GitHub username&gt;/&lt;your project&gt;/actions</code>. The testing workflow configured in <code>.github/workflows/tests.yml</code> will run automatically on pushes to your main branch and you can monitor the success of these tests from the GitHub actions console for the repo. The workflow tests your package against a matrix of Python versions (3.7, 3.8, 3.9) on the latest Ubuntu build. You can expand this matrix if desired using GitHub actions matrix options.</p> <p>Versioned tags of the GitHub repository will correspond to deployments of a given model. The build action runs once a tag is created and uploads the model to your registry service of choice. </p>"},{"location":"model_packaging/#registries","title":"Registries","text":""},{"location":"model_packaging/#stanford-container-registry","title":"Stanford Container Registry","text":"<p>SLAC users can take advantage of the Stanford Container Registry to store their containers. The steps for configuring your project to use the registry are as follows:</p> <ol> <li>Create an API token at https://code.stanford.edu/-/profile/personal_access_tokens. For <code>Token name</code> enter <code>My Model</code>. Optionally choose an expiration date. This can be whatever you'd like, but the GitHub action secret defined in step 3. will need to be updated with a new value after this expiration. Record this API token for use in steps 9.3 and 9.4.</li> </ol> <p></p> <ol> <li>Create a project using Stanford Code. In the <code>Project name</code> field, write <code>My Model</code>. Select internal visibility level.</li> </ol> <p></p> <ol> <li>Add the token to your GitHub repository secrets. Navigate to your repository settings. In the left sidebar, click <code>Secrets</code>, <code>Actions</code>, then <code>New repository secret</code>. Type <code>SCR_PAT</code> into the name, and your generated API key into the value. Repeat this process to set a secret named <code>STANFORD_USERNAME</code> to your Stanford username.</li> </ol>"},{"location":"model_packaging/#dockerhub","title":"DockerHub","text":"<p>In order to use DockerHub workflow, you must first create a DockerHub account, then set the appropriate GitHub secrets on the repository.</p> <ol> <li>Navigate to the settings on your repository.  </li> </ol> <p></p> <ol> <li>In the left sidebar, click <code>Secrets</code>, <code>Actions</code>, then <code>New repository secret</code>. Type <code>DOCKER_USERNAME</code> into the name, and your DockerHub username into the value and click <code>Add secret</code> to save. Repeat this process to create a <code>DOCKER_PASSWORD</code> secret with your DockerHub password as the value.  </li> </ol>"},{"location":"workflows/","title":"Workflows","text":"<p>LUME-services allows us to build workflows using Prefect's Flow APIs. Flows can be conceptualized of as scoped units of work. Prefect uses a context-management pattern to define flow hierarchy. Parameters are units of data passed into the flow at runtime.</p>"},{"location":"workflows/#configuring-flows-for-use-with-lume-services","title":"Configuring flows for use with LUME-services","text":"<p>Inside a flow, you may use the <code>configure_lume_services</code> task to configure LUME-services from environment variables. This task must be set as upstream task to any tasks that then use those services <code>my_task.set_upstream(configure_task)</code> within the flow context.</p>"},{"location":"workflows/#flow-parameters","title":"Flow parameters","text":"<p>Flow Parameters have some constraints. To see why those constraints exist, see developer docs here. 1. Flow-level parameters must be json serializable, meaning they must be of types:</p> <p>|  Python          | JSON    | |----------------------------| | dict             | object  | | list, tuple      | array   | | str              | string  | | int, long, float | number  | | True             | true    | | False            | false   | | None             |         |</p> <ol> <li>In order to access the results of tasks outside a flow, the task-level results must also be JSON serializable. LUME-services packages some utilities for interacting with custom result types using JSON representations of the result that can be used to load at runtime. The method <code>lume_services.results.generic.get_db_dict()</code> creates a json serializable result that may then be used to fetch a full model run result form the results database.</li> </ol>"},{"location":"workflows/#common-tasks","title":"Common tasks","text":"<pre><code>from prefect import task, Flow, Parameter\nfrom prefect.storage import Module\nfrom lume_services.tasks import configure_lume_services, SaveFile\nfrom lume_services.files import TextFile\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nlogger.setLevel(logging.DEBUG)\n\n\n@task\ndef append_text(text1, text2):\n    return text1 + text2\n\nsave_file = SaveFile(name=\"save_text_file\")\n\nwith Flow(\"flow1\", storage=Module(__name__)) as flow1:\n    text1 = Parameter(\"text1\")\n    text2 = Parameter(\"text2\")\n    filename = Parameter(\"filename\")\n    filesystem_identifier = Parameter(\"filesystem_identifier\")\n    configure = configure_lume_services()\n    new_text = append_text(text1, text2)\n    file = save_file(\n        obj=new_text,\n        file_type=TextFile,\n        filename=filename,\n        filesystem_identifier=filesystem_identifier,\n    )\n    file.set_upstream(configure)\n</code></pre>"},{"location":"api/config/","title":"Config","text":""},{"location":"api/config/#lume_services.config.LUMEServicesSettings","title":"<code>LUMEServicesSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Settings describing configuration for default LUME-services provider objects.</p> Source code in <code>lume_services/config.py</code> <pre><code>class LUMEServicesSettings(BaseSettings):\n    \"\"\"Settings describing configuration for default LUME-services provider objects.\"\"\"\n\n    model_db: Optional[ModelDBConfig]\n    results_db: Optional[MongodbResultsDBConfig]\n    prefect: PrefectConfig\n    mounted_filesystem: Optional[MountedFilesystem]\n    backend: str = \"local\"\n    # something wrong with pydantic literal parsing?\n    # Literal[\"kubernetes\", \"local\", \"docker\"] = \"local\"\n\n    class Config:\n        # env_file = '.env'\n        # env_file_encoding = 'utf-8'\n        validate_assignment = True\n        env_prefix = \"LUME_\"\n        env_nested_delimiter = \"__\"\n</code></pre>"},{"location":"api/config/#lume_services.config.configure","title":"<code>configure(settings=None)</code>","text":"<p>Configure method with default methods for lume-services using ModelDB and MongodbResultsDB. Populates the global _settings object.</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>Optional[LUMEServicesSettings]</code> <p>LUMEServicesSettings object holding the runtime configuration for services used by LUME-services.</p> <code>None</code> Source code in <code>lume_services/config.py</code> <pre><code>def configure(settings: Optional[LUMEServicesSettings] = None):\n    \"\"\"Configure method with default methods for lume-services using ModelDB\n    and MongodbResultsDB. Populates the global _settings object.\n\n    Args:\n        settings (Optional[LUMEServicesSettings]): LUMEServicesSettings object holding\n            the runtime configuration for services used by LUME-services.\n\n    \"\"\"\n    logger.info(\"Configuring LUME-services environment...\")\n    if settings is None:\n        try:\n            settings = LUMEServicesSettings()\n\n        except ValidationError as e:\n            raise EnvironmentNotConfiguredError(\n                get_env_vars(LUMEServicesSettings), validation_error=e\n            )\n\n    # apply prefect config\n    if settings.prefect is not None:\n        settings.prefect.apply()\n\n    global context, _settings\n    model_db = None\n    if settings.model_db is not None:\n        model_db = ModelDB(settings.model_db)\n\n    results_db = None\n    if settings.results_db is not None:\n        results_db = MongodbResultsDB(settings.results_db)\n\n    # this could be moved to an enum\n    if settings.backend is not None:\n        if settings.backend == \"kubernetes\":\n            backend = KubernetesBackend(config=settings.prefect)\n\n        elif settings.backend == \"docker\":\n            backend = DockerBackend(config=settings.prefect)\n\n        elif settings.backend == \"local\":\n            backend = LocalBackend()\n\n        else:\n            raise ValueError(f\"Unsupported backend {settings.backend}\")\n\n    # default to local\n    else:\n        backend = LocalBackend()\n\n    filesystems = [\n        LocalFilesystem(),\n    ]\n\n    if settings.mounted_filesystem is not None:\n        filesystems.append(settings.mounted_filesystem)\n\n    context = Context(\n        model_db=model_db,\n        results_db=results_db,\n        filesystems=filesystems,\n        scheduling_backend=backend\n    )\n    _settings = settings\n    logger.info(\"Environment configured.\")\n    logger.debug(\"Environment configured using %s\", settings.dict())\n</code></pre>"},{"location":"api/errors/","title":"Errors","text":""},{"location":"api/errors/#lume_services.errors.DeploymentNotFoundError","title":"<code>DeploymentNotFoundError</code>","text":"<p>               Bases: <code>Exception</code></p> Source code in <code>lume_services/errors.py</code> <pre><code>class DeploymentNotFoundError(Exception):\n    def __init__(self, query: dict) -&gt; None:\n        \"\"\"\n        Args:\n            query (dict): Dictionary representation of mongodb query.\n\n        \"\"\"\n        self.query = query\n        self.message = \"Deployment not found for query: %s.\"\n        super().__init__(self.message, str(self.query))\n</code></pre>"},{"location":"api/errors/#lume_services.errors.DeploymentNotFoundError.__init__","title":"<code>__init__(query)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>query</code> <code>dict</code> <p>Dictionary representation of mongodb query.</p> required Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(self, query: dict) -&gt; None:\n    \"\"\"\n    Args:\n        query (dict): Dictionary representation of mongodb query.\n\n    \"\"\"\n    self.query = query\n    self.message = \"Deployment not found for query: %s.\"\n    super().__init__(self.message, str(self.query))\n</code></pre>"},{"location":"api/errors/#lume_services.errors.DeploymentNotRegisteredError","title":"<code>DeploymentNotRegisteredError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>DeploymentNotRegisteredError indicates that a deployment was not found for the given model. If no deployment_id is passed, assumes that the user is attempting to load the latest deployment for that model.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class DeploymentNotRegisteredError(Exception):\n    \"\"\"DeploymentNotRegisteredError indicates that a deployment was not found for the\n    given model. If no deployment_id is passed, assumes that the user is attempting to\n    load the latest deployment for that model.\n\n    \"\"\"\n\n    def __init__(self, model_id: int, deployment_id: Optional[int] = None) -&gt; None:\n        \"\"\"\n        Args:\n            model_id (int): ID of model.\n            deployment_id (Optional[int]): Deployment ID that was attempted to retrieve.\n        \"\"\"\n        self.model_id = model_id\n        self.deployment_id = deployment_id\n        if deployment_id is None:\n            self.message = \"No deployment registered for model_id=%s.\"\n            super().__init__(self.message, self.model_id)\n        else:\n            self.message = (\n                \"Deployment not found for model_id = %s with deploymend_id=%s\"\n            )\n            super().__init__(self.message, self.model_id, self.deployment_id)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.DeploymentNotRegisteredError.__init__","title":"<code>__init__(model_id, deployment_id=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>int</code> <p>ID of model.</p> required <code>deployment_id</code> <code>Optional[int]</code> <p>Deployment ID that was attempted to retrieve.</p> <code>None</code> Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(self, model_id: int, deployment_id: Optional[int] = None) -&gt; None:\n    \"\"\"\n    Args:\n        model_id (int): ID of model.\n        deployment_id (Optional[int]): Deployment ID that was attempted to retrieve.\n    \"\"\"\n    self.model_id = model_id\n    self.deployment_id = deployment_id\n    if deployment_id is None:\n        self.message = \"No deployment registered for model_id=%s.\"\n        super().__init__(self.message, self.model_id)\n    else:\n        self.message = (\n            \"Deployment not found for model_id = %s with deploymend_id=%s\"\n        )\n        super().__init__(self.message, self.model_id, self.deployment_id)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.EmptyResultError","title":"<code>EmptyResultError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error raised when a result is empty.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class EmptyResultError(Exception):\n    \"\"\"Error raised when a result is empty.\"\"\"\n\n    def __init__(\n        self, flow_id: str, flow_run_id: str, task_slug: Optional[str] = None\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            flow_id (str): ID of the flow.s\n            flow_run_id (str): ID of the flow run.\n            task_slug (Optional[str]): Prefect tasks are assigned slugs. The task\n                slug is the identifier for the task for which we're trying to load the\n                result.\n\n        \"\"\"\n        self.flow_id = flow_id\n        self.flow_run_id = flow_run_id\n        self.task_slug = task_slug\n        if not self.task_slug:\n            self.message = (\n                \"Task with slug: %s for flow run: %s of flow_id: %s has no result.\"\n            )\n            super().__init__(\n                self.message, self.task_slug, self.flow_run_id, self.flow_id\n            )\n\n        else:\n            self.message = \"Flow run: %s of flow_id: %s has no result.\"\n            super().__init__(self.message, self.flow_run_id, flow_id)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.EmptyResultError.__init__","title":"<code>__init__(flow_id, flow_run_id, task_slug=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>flow_id</code> <code>str</code> <p>ID of the flow.s</p> required <code>flow_run_id</code> <code>str</code> <p>ID of the flow run.</p> required <code>task_slug</code> <code>Optional[str]</code> <p>Prefect tasks are assigned slugs. The task slug is the identifier for the task for which we're trying to load the result.</p> <code>None</code> Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(\n    self, flow_id: str, flow_run_id: str, task_slug: Optional[str] = None\n) -&gt; None:\n    \"\"\"\n    Args:\n        flow_id (str): ID of the flow.s\n        flow_run_id (str): ID of the flow run.\n        task_slug (Optional[str]): Prefect tasks are assigned slugs. The task\n            slug is the identifier for the task for which we're trying to load the\n            result.\n\n    \"\"\"\n    self.flow_id = flow_id\n    self.flow_run_id = flow_run_id\n    self.task_slug = task_slug\n    if not self.task_slug:\n        self.message = (\n            \"Task with slug: %s for flow run: %s of flow_id: %s has no result.\"\n        )\n        super().__init__(\n            self.message, self.task_slug, self.flow_run_id, self.flow_id\n        )\n\n    else:\n        self.message = \"Flow run: %s of flow_id: %s has no result.\"\n        super().__init__(self.message, self.flow_run_id, flow_id)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.EnvironmentNotConfiguredError","title":"<code>EnvironmentNotConfiguredError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error used to mark an unconfigured environment.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class EnvironmentNotConfiguredError(Exception):\n    \"\"\"Error used to mark an unconfigured environment.\"\"\"\n\n    def __init__(\n        self, env_vars: Dict[str, str], validation_error: ValidationError = None\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            env_vars (Dict[str, str]): Dictionary mapping service to list of\n                environment variables used to configure that service.\n            validation_error (ValidationError): ValidationError raised by Pydantic\n                class during assignment.\n\n        \"\"\"\n\n        self.env = dict(os.environ)\n        self.env_vars = []\n\n        for service in env_vars:\n            self.env_vars += env_vars[service]\n\n        self.missing_vars = [var for var in self.env_vars if var not in self.env]\n\n        if validation_error is None:\n            self.message = \"Environment variables not defined: %s.\"\n            super().__init__(self.message, \", \".join(self.missing_vars))\n\n        else:\n            self.message = \"%s. Evironment variables not defined: %s\"\n            super().__init__(\n                self.message, str(validation_error), \", \".join(self.missing_vars)\n            )\n</code></pre>"},{"location":"api/errors/#lume_services.errors.EnvironmentNotConfiguredError.__init__","title":"<code>__init__(env_vars, validation_error=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>env_vars</code> <code>Dict[str, str]</code> <p>Dictionary mapping service to list of environment variables used to configure that service.</p> required <code>validation_error</code> <code>ValidationError</code> <p>ValidationError raised by Pydantic class during assignment.</p> <code>None</code> Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(\n    self, env_vars: Dict[str, str], validation_error: ValidationError = None\n) -&gt; None:\n    \"\"\"\n    Args:\n        env_vars (Dict[str, str]): Dictionary mapping service to list of\n            environment variables used to configure that service.\n        validation_error (ValidationError): ValidationError raised by Pydantic\n            class during assignment.\n\n    \"\"\"\n\n    self.env = dict(os.environ)\n    self.env_vars = []\n\n    for service in env_vars:\n        self.env_vars += env_vars[service]\n\n    self.missing_vars = [var for var in self.env_vars if var not in self.env]\n\n    if validation_error is None:\n        self.message = \"Environment variables not defined: %s.\"\n        super().__init__(self.message, \", \".join(self.missing_vars))\n\n    else:\n        self.message = \"%s. Evironment variables not defined: %s\"\n        super().__init__(\n            self.message, str(validation_error), \", \".join(self.missing_vars)\n        )\n</code></pre>"},{"location":"api/errors/#lume_services.errors.FlowFailedError","title":"<code>FlowFailedError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error raised when a flow fails to execute successfully.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class FlowFailedError(Exception):\n    \"\"\"Error raised when a flow fails to execute successfully.\"\"\"\n\n    def __init__(\n        self, flow_id: str, flow_run_id: str, exception_message: str = None\n    ) -&gt; None:\n        self.flow_id = flow_id\n        self.flow_run_id = flow_run_id\n        self.exception_message = exception_message\n        self.message = \"Flow run: %s failed for flow_id: %s. %s\"\n        super().__init__(\n            self.message, self.flow_run_id, self.flow_id, self.exception_message\n        )\n</code></pre>"},{"location":"api/errors/#lume_services.errors.FlowNotFoundError","title":"<code>FlowNotFoundError</code>","text":"<p>               Bases: <code>Exception</code></p> Source code in <code>lume_services/errors.py</code> <pre><code>class FlowNotFoundError(Exception):\n    def __init__(self, query: dict) -&gt; None:\n        \"\"\"\n        Args:\n            query (dict): Dictionary representation of mongodb query.\n\n        \"\"\"\n\n        self.query = query\n        self.message = \"Flow not found for query: %s.\"\n        super().__init__(self.message, self.query)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.FlowNotFoundError.__init__","title":"<code>__init__(query)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>query</code> <code>dict</code> <p>Dictionary representation of mongodb query.</p> required Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(self, query: dict) -&gt; None:\n    \"\"\"\n    Args:\n        query (dict): Dictionary representation of mongodb query.\n\n    \"\"\"\n\n    self.query = query\n    self.message = \"Flow not found for query: %s.\"\n    super().__init__(self.message, self.query)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.FlowOfFlowsNotFoundError","title":"<code>FlowOfFlowsNotFoundError</code>","text":"<p>               Bases: <code>Exception</code></p> Source code in <code>lume_services/errors.py</code> <pre><code>class FlowOfFlowsNotFoundError(Exception):\n    def __init__(self, query: dict) -&gt; None:\n        \"\"\"\n        Args:\n            query (dict): Dictionary representation of mongodb query.\n\n        \"\"\"\n        self.query = query\n        self.message = \"Flow not found for query: %s.\"\n        super().__init__(self.message, self.query)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.FlowOfFlowsNotFoundError.__init__","title":"<code>__init__(query)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>query</code> <code>dict</code> <p>Dictionary representation of mongodb query.</p> required Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(self, query: dict) -&gt; None:\n    \"\"\"\n    Args:\n        query (dict): Dictionary representation of mongodb query.\n\n    \"\"\"\n    self.query = query\n    self.message = \"Flow not found for query: %s.\"\n    super().__init__(self.message, self.query)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.LocalBackendError","title":"<code>LocalBackendError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>LocalBackendError indicates that a server-backend operation has been executed against the LocalBackend. Server-backend operations include flow registration and remote execution.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class LocalBackendError(Exception):\n    \"\"\"LocalBackendError indicates that a server-backend operation has been executed\n    against the LocalBackend. Server-backend operations include flow registration and\n    remote execution.\n\n    \"\"\"\n\n    def __init__(self):\n        self.message = \"Cannot run server-backend operation using LocalBackend.\"\n        super().__init__(self.message)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.MissingEnvironmentYamlError","title":"<code>MissingEnvironmentYamlError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error raised when a model package directory is missing an environment.yml spec.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class MissingEnvironmentYamlError(Exception):\n    \"\"\"Error raised when a model package directory is missing an environment.yml\n    spec.\n\n    \"\"\"\n\n    def __init__(self, directory: str):\n        \"\"\"\n        Args:\n            directory (str): Local directory holding the package source.\n\n        \"\"\"\n        self.directory = directory\n        self.message = \"Poorly formed package at %s. No Environment yaml provided.\"\n\n        super().__init__(self.message, self.directory)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.MissingEnvironmentYamlError.__init__","title":"<code>__init__(directory)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Local directory holding the package source.</p> required Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(self, directory: str):\n    \"\"\"\n    Args:\n        directory (str): Local directory holding the package source.\n\n    \"\"\"\n    self.directory = directory\n    self.message = \"Poorly formed package at %s. No Environment yaml provided.\"\n\n    super().__init__(self.message, self.directory)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.ModelNotFoundError","title":"<code>ModelNotFoundError</code>","text":"<p>               Bases: <code>Exception</code></p> Source code in <code>lume_services/errors.py</code> <pre><code>class ModelNotFoundError(Exception):\n    def __init__(self, query: dict) -&gt; None:\n        \"\"\"\n        Args:\n            query (dict): Dictionary representation of mongodb query.\n\n        \"\"\"\n        self.query = query\n        self.message = \"Model not found for query: %s.\"\n        super().__init__(self.message, self.query)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.ModelNotFoundError.__init__","title":"<code>__init__(query)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>query</code> <code>dict</code> <p>Dictionary representation of mongodb query.</p> required Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(self, query: dict) -&gt; None:\n    \"\"\"\n    Args:\n        query (dict): Dictionary representation of mongodb query.\n\n    \"\"\"\n    self.query = query\n    self.message = \"Model not found for query: %s.\"\n    super().__init__(self.message, self.query)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.NoCondaEnvironmentFoundError","title":"<code>NoCondaEnvironmentFoundError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error raised when CONDA_PREFIX is not defined.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class NoCondaEnvironmentFoundError(Exception):\n    \"\"\"Error raised when CONDA_PREFIX is not defined.\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(\"CONDA_PREFIX environment variabe is not set.\")\n</code></pre>"},{"location":"api/errors/#lume_services.errors.NoFlowFoundInPackageError","title":"<code>NoFlowFoundInPackageError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error raised when flow not found at a given entrypoint.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class NoFlowFoundInPackageError(Exception):\n    \"\"\"Error raised when flow not found at a given entrypoint.\"\"\"\n\n    def __init__(self, source_path: str):\n        \"\"\"\n        Args:\n            source_path (str): Import path of the flow provided to entrypoint.\n\n        \"\"\"\n        self.source_path = source_path\n        self.message = \"No flow entrypoint found for the distribution at %s\"\n        super().__init__(self.message, self.source_path)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.NoFlowFoundInPackageError.__init__","title":"<code>__init__(source_path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>Import path of the flow provided to entrypoint.</p> required Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(self, source_path: str):\n    \"\"\"\n    Args:\n        source_path (str): Import path of the flow provided to entrypoint.\n\n    \"\"\"\n    self.source_path = source_path\n    self.message = \"No flow entrypoint found for the distribution at %s\"\n    super().__init__(self.message, self.source_path)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.NoPackagesToInstallError","title":"<code>NoPackagesToInstallError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error indicates no packages were returned from environment resolution.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class NoPackagesToInstallError(Exception):\n    \"\"\"Error indicates no packages were returned from environment resolution.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.message = \"No packages were returned from environment resolution.\"\n        super().__init__(self.message)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.ParentFlowNotInFlowsError","title":"<code>ParentFlowNotInFlowsError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error raised when composing flow-of-flows when the parent flow is not found in the list of flows.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class ParentFlowNotInFlowsError(Exception):\n    \"\"\"Error raised when composing flow-of-flows when the parent flow is not found\n    in the list of flows.\n\n    \"\"\"\n\n    def __init__(self, flow_name: str, flows: List[str]):\n        \"\"\"\n        Args:\n            flow_name (str): Name of parent flow\n            flows (List[str]): List of flows provided\n\n        \"\"\"\n        self.flow_name = flow_name\n        self.flows = flows\n        self.message = \"Parent flow %s not in flows: %s\"\n        super().__init__(self.message, self.flow_name, \", \".join(self.flows))\n</code></pre>"},{"location":"api/errors/#lume_services.errors.ParentFlowNotInFlowsError.__init__","title":"<code>__init__(flow_name, flows)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>flow_name</code> <code>str</code> <p>Name of parent flow</p> required <code>flows</code> <code>List[str]</code> <p>List of flows provided</p> required Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(self, flow_name: str, flows: List[str]):\n    \"\"\"\n    Args:\n        flow_name (str): Name of parent flow\n        flows (List[str]): List of flows provided\n\n    \"\"\"\n    self.flow_name = flow_name\n    self.flows = flows\n    self.message = \"Parent flow %s not in flows: %s\"\n    super().__init__(self.message, self.flow_name, \", \".join(self.flows))\n</code></pre>"},{"location":"api/errors/#lume_services.errors.PathNotInMount","title":"<code>PathNotInMount</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error raised when the path provided does not exist in a mounted filesystem.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class PathNotInMount(Exception):\n    \"\"\"Error raised when the path provided does not exist in a mounted filesystem.\"\"\"\n\n    def __init__(\n        self, filesystem_identifier: str, path: str, mount_path: str, mount_alias: str\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            filesystem_identifier (str): Identifier of the filesystem to use.\n            path (str): Path that was not found.\n            mount_path (str): Original path of the mounted directory on host.\n            mount_alias (str): Alias used in mount the filesystem.\n        \"\"\"\n        self.filesystem_identifier = filesystem_identifier\n        self.path = (path,)\n        self.mount_path = mount_path\n        self.mount_alias = mount_alias\n        self.message = \"Path %s not in mount for mounted filesystem identifier: %s, \\\n            Mount path: %s, Mount alias: %s\"\n        super().__init__(\n            self.message,\n            self.path,\n            self.filesystem_identifier,\n            self.mount_path,\n            self.mount_alias,\n        )\n</code></pre>"},{"location":"api/errors/#lume_services.errors.PathNotInMount.__init__","title":"<code>__init__(filesystem_identifier, path, mount_path, mount_alias)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>filesystem_identifier</code> <code>str</code> <p>Identifier of the filesystem to use.</p> required <code>path</code> <code>str</code> <p>Path that was not found.</p> required <code>mount_path</code> <code>str</code> <p>Original path of the mounted directory on host.</p> required <code>mount_alias</code> <code>str</code> <p>Alias used in mount the filesystem.</p> required Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(\n    self, filesystem_identifier: str, path: str, mount_path: str, mount_alias: str\n) -&gt; None:\n    \"\"\"\n    Args:\n        filesystem_identifier (str): Identifier of the filesystem to use.\n        path (str): Path that was not found.\n        mount_path (str): Original path of the mounted directory on host.\n        mount_alias (str): Alias used in mount the filesystem.\n    \"\"\"\n    self.filesystem_identifier = filesystem_identifier\n    self.path = (path,)\n    self.mount_path = mount_path\n    self.mount_alias = mount_alias\n    self.message = \"Path %s not in mount for mounted filesystem identifier: %s, \\\n        Mount path: %s, Mount alias: %s\"\n    super().__init__(\n        self.message,\n        self.path,\n        self.filesystem_identifier,\n        self.mount_path,\n        self.mount_alias,\n    )\n</code></pre>"},{"location":"api/errors/#lume_services.errors.ProjectNotFoundError","title":"<code>ProjectNotFoundError</code>","text":"<p>               Bases: <code>Exception</code></p> Source code in <code>lume_services/errors.py</code> <pre><code>class ProjectNotFoundError(Exception):\n    def __init__(self, query: dict) -&gt; None:\n        \"\"\"\n        Args:\n            query (dict): Dictionary representation of mongodb query.\n\n\n        \"\"\"\n        self.query = query\n        self.message = \"Project not found for query: %s.\"\n        super().__init__(self.message, self.query)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.ProjectNotFoundError.__init__","title":"<code>__init__(query)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>query</code> <code>dict</code> <p>Dictionary representation of mongodb query.</p> required Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(self, query: dict) -&gt; None:\n    \"\"\"\n    Args:\n        query (dict): Dictionary representation of mongodb query.\n\n\n    \"\"\"\n    self.query = query\n    self.message = \"Project not found for query: %s.\"\n    super().__init__(self.message, self.query)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.TaskNotCompletedError","title":"<code>TaskNotCompletedError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error raised when a task fails to execute successfully.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class TaskNotCompletedError(Exception):\n    \"\"\"Error raised when a task fails to execute successfully.\"\"\"\n\n    def __init__(self, task_slug: str, flow_id: str, flow_run_id: str) -&gt; None:\n        \"\"\"\n        Args:\n            task_slug (str): Slug of the task that was not completed.\n            flow_id (str): ID of Prefect flow.\n            flow_run_id (str): ID of Prefect Flow run.\n        \"\"\"\n        self.flow_id = flow_id\n        self.flow_run_id = flow_run_id\n        self.task_slug = task_slug\n        self.message = (\n            \"Task with slug: %s not completed for flow_run_id: %s, flow_id: %s.\"\n        )\n        super().__init__(self.message, self.task_slug, self.flow_run_id, self.flow_id)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.TaskNotCompletedError.__init__","title":"<code>__init__(task_slug, flow_id, flow_run_id)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>task_slug</code> <code>str</code> <p>Slug of the task that was not completed.</p> required <code>flow_id</code> <code>str</code> <p>ID of Prefect flow.</p> required <code>flow_run_id</code> <code>str</code> <p>ID of Prefect Flow run.</p> required Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(self, task_slug: str, flow_id: str, flow_run_id: str) -&gt; None:\n    \"\"\"\n    Args:\n        task_slug (str): Slug of the task that was not completed.\n        flow_id (str): ID of Prefect flow.\n        flow_run_id (str): ID of Prefect Flow run.\n    \"\"\"\n    self.flow_id = flow_id\n    self.flow_run_id = flow_run_id\n    self.task_slug = task_slug\n    self.message = (\n        \"Task with slug: %s not completed for flow_run_id: %s, flow_id: %s.\"\n    )\n    super().__init__(self.message, self.task_slug, self.flow_run_id, self.flow_id)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.TaskNotInFlowError","title":"<code>TaskNotInFlowError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error raised to indicate that the given task is not in a flow.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class TaskNotInFlowError(Exception):\n    \"\"\"Error raised to indicate that the given task is not in a flow.\"\"\"\n\n    def __init__(self, flow_name: str, project_name: str, task_name: str) -&gt; None:\n        \"\"\"\n        Args:\n            flow_name (str): Name of flow\n            project_name (str): Name of project\n            task_name (str): Name of task\n\n        \"\"\"\n\n        self.flow_name = flow_name\n        self.task_name = task_name\n        self.project_name = project_name\n        self.message = \"Task %s not in flow %s.\"\n        super().__init__(\n            self.message, self.task_name, self.project_name, self.flow_name\n        )\n</code></pre>"},{"location":"api/errors/#lume_services.errors.TaskNotInFlowError.__init__","title":"<code>__init__(flow_name, project_name, task_name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>flow_name</code> <code>str</code> <p>Name of flow</p> required <code>project_name</code> <code>str</code> <p>Name of project</p> required <code>task_name</code> <code>str</code> <p>Name of task</p> required Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(self, flow_name: str, project_name: str, task_name: str) -&gt; None:\n    \"\"\"\n    Args:\n        flow_name (str): Name of flow\n        project_name (str): Name of project\n        task_name (str): Name of task\n\n    \"\"\"\n\n    self.flow_name = flow_name\n    self.task_name = task_name\n    self.project_name = project_name\n    self.message = \"Task %s not in flow %s.\"\n    super().__init__(\n        self.message, self.task_name, self.project_name, self.flow_name\n    )\n</code></pre>"},{"location":"api/errors/#lume_services.errors.UnableToIndexLocalChannelError","title":"<code>UnableToIndexLocalChannelError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error raised when unable to index the local channel during conda environment resolution to local channel.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class UnableToIndexLocalChannelError(Exception):\n    \"\"\"Error raised when unable to index the local channel during conda environment\n    resolution to local channel.\n\n    \"\"\"\n\n    def __init__(\n        self, local_channel_directory: str, return_code: int, output: str\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            local_channel_directory (str): Directory holding local channel.\n            return_code (int): Return code of the subprocess.\n            output (str): Output of the subprocess.\n\n\n        \"\"\"\n        self.local_channel_directory = local_channel_directory\n        self.return_code = return_code\n        self.output = output\n\n        self.message = \"Unable to index local channel at %s. Subprocess returned \\\n            code %s with output: %s\"\n        super().__init__(\n            self.message, self.local_channel_directory, self.return_code, self.output\n        )\n</code></pre>"},{"location":"api/errors/#lume_services.errors.UnableToIndexLocalChannelError.__init__","title":"<code>__init__(local_channel_directory, return_code, output)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>local_channel_directory</code> <code>str</code> <p>Directory holding local channel.</p> required <code>return_code</code> <code>int</code> <p>Return code of the subprocess.</p> required <code>output</code> <code>str</code> <p>Output of the subprocess.</p> required Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(\n    self, local_channel_directory: str, return_code: int, output: str\n) -&gt; None:\n    \"\"\"\n    Args:\n        local_channel_directory (str): Directory holding local channel.\n        return_code (int): Return code of the subprocess.\n        output (str): Output of the subprocess.\n\n\n    \"\"\"\n    self.local_channel_directory = local_channel_directory\n    self.return_code = return_code\n    self.output = output\n\n    self.message = \"Unable to index local channel at %s. Subprocess returned \\\n        code %s with output: %s\"\n    super().__init__(\n        self.message, self.local_channel_directory, self.return_code, self.output\n    )\n</code></pre>"},{"location":"api/errors/#lume_services.errors.UnableToInstallCondaDependenciesError","title":"<code>UnableToInstallCondaDependenciesError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error indicating that certain conda dependencies were not installed during resolution.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class UnableToInstallCondaDependenciesError(Exception):\n    \"\"\"Error indicating that certain conda dependencies were not installed during\n    resolution.\n\n    \"\"\"\n\n    def __init__(self, conda_dependencies: List[str]) -&gt; None:\n        \"\"\"\n        Args:\n            conda_dependencies (List[str]): List of conda dependencies that were not\n                installed.\n        \"\"\"\n        self.deps = conda_dependencies\n\n        self.message = \"Unable to install conda dependencies: %s\"\n        super().__init__(self.message, \", \".join(self.deps))\n</code></pre>"},{"location":"api/errors/#lume_services.errors.UnableToInstallCondaDependenciesError.__init__","title":"<code>__init__(conda_dependencies)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>conda_dependencies</code> <code>List[str]</code> <p>List of conda dependencies that were not installed.</p> required Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(self, conda_dependencies: List[str]) -&gt; None:\n    \"\"\"\n    Args:\n        conda_dependencies (List[str]): List of conda dependencies that were not\n            installed.\n    \"\"\"\n    self.deps = conda_dependencies\n\n    self.message = \"Unable to install conda dependencies: %s\"\n    super().__init__(self.message, \", \".join(self.deps))\n</code></pre>"},{"location":"api/errors/#lume_services.errors.UnableToInstallPipDependenciesError","title":"<code>UnableToInstallPipDependenciesError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error indicating failed pip installation.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class UnableToInstallPipDependenciesError(Exception):\n    \"\"\"Error indicating failed pip installation.\"\"\"\n\n    def __init__(\n        self,\n        pip_dependencies: List[str],\n        python_version: float,\n        platform: Literal[\"linux-64\", \"linux-32\", \"osx-64\", \"win-32\", \"win-64\"],\n        e: Exception,\n    ) -&gt; None:\n        \"\"\"\n\n        Args:\n            pip_dependencies (List[str]): List of dependencies to be installed with pip\n            python_version (float): Python version used for installation\n            platform (Literal[\"linux-64\", \"linux-32\", \"osx-64\", \"win-32\", \"win-64\"]):\n                Platform used for installation\n            e (Exception): Exception raised from installation subprocess\n\n\n        \"\"\"\n        self.deps = pip_dependencies\n        self.python_version = python_version\n        self.platform = platform\n\n        self.message = \"Unable to install pip dependencies: %s for python=%s on \\\n            platform=%s with error: %s\"\n        super().__init__(self.message, \", \".join(self.deps), self.python_version, e)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.UnableToInstallPipDependenciesError.__init__","title":"<code>__init__(pip_dependencies, python_version, platform, e)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pip_dependencies</code> <code>List[str]</code> <p>List of dependencies to be installed with pip</p> required <code>python_version</code> <code>float</code> <p>Python version used for installation</p> required <code>platform</code> <code>Literal['linux-64', 'linux-32', 'osx-64', 'win-32', 'win-64']</code> <p>Platform used for installation</p> required <code>e</code> <code>Exception</code> <p>Exception raised from installation subprocess</p> required Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(\n    self,\n    pip_dependencies: List[str],\n    python_version: float,\n    platform: Literal[\"linux-64\", \"linux-32\", \"osx-64\", \"win-32\", \"win-64\"],\n    e: Exception,\n) -&gt; None:\n    \"\"\"\n\n    Args:\n        pip_dependencies (List[str]): List of dependencies to be installed with pip\n        python_version (float): Python version used for installation\n        platform (Literal[\"linux-64\", \"linux-32\", \"osx-64\", \"win-32\", \"win-64\"]):\n            Platform used for installation\n        e (Exception): Exception raised from installation subprocess\n\n\n    \"\"\"\n    self.deps = pip_dependencies\n    self.python_version = python_version\n    self.platform = platform\n\n    self.message = \"Unable to install pip dependencies: %s for python=%s on \\\n        platform=%s with error: %s\"\n    super().__init__(self.message, \", \".join(self.deps), self.python_version, e)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.WritePermissionError","title":"<code>WritePermissionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error indicates missing write permission on a directory.</p> Source code in <code>lume_services/errors.py</code> <pre><code>class WritePermissionError(Exception):\n    \"\"\"Error indicates missing write permission on a directory.\"\"\"\n\n    def __init__(self, directory: str) -&gt; None:\n        \"\"\"\n        Args:\n            directory (str): Directory that is missing write permissions.\n\n        \"\"\"\n        self.directory = directory\n        self.user = os.getlogin()\n        self.message = \"User %s does not have write permissions for directory %s.\"\n\n        super().__init__(self.message, self.user, self.directory)\n</code></pre>"},{"location":"api/errors/#lume_services.errors.WritePermissionError.__init__","title":"<code>__init__(directory)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Directory that is missing write permissions.</p> required Source code in <code>lume_services/errors.py</code> <pre><code>def __init__(self, directory: str) -&gt; None:\n    \"\"\"\n    Args:\n        directory (str): Directory that is missing write permissions.\n\n    \"\"\"\n    self.directory = directory\n    self.user = os.getlogin()\n    self.message = \"User %s does not have write permissions for directory %s.\"\n\n    super().__init__(self.message, self.user, self.directory)\n</code></pre>"},{"location":"api/flows/","title":"Flows","text":""},{"location":"api/flows/#lume_services.flows.flow.FileMappedParameter","title":"<code>FileMappedParameter</code>","text":"<p>               Bases: <code>MappedParameter</code></p> <p>FileMappedParameters describe files passed between different flows. Files are saved as json representations describing file type (and serialization) and filesystem information.</p> Attr <p>parent_flow_name (str): Parent flow holding origin of mapped parameter. parent_task_name (str): Task whose result is mapped to the parameter. map_type (Literal[\"file\", \"db\", \"raw\"] = \"file\"): The \"file\" map type describes     the</p> Source code in <code>lume_services/flows/flow.py</code> <pre><code>class FileMappedParameter(MappedParameter):\n    \"\"\"FileMappedParameters describe files passed between different flows. Files are\n    saved as json representations describing file type (and serialization) and\n    filesystem information.\n\n    Attr:\n        parent_flow_name (str): Parent flow holding origin of mapped parameter.\n        parent_task_name (str): Task whose result is mapped to the parameter.\n        map_type (Literal[\"file\", \"db\", \"raw\"] = \"file\"): The \"file\" map type describes\n            the\n\n    \"\"\"\n\n    map_type: str = Field(\"file\", const=True)\n</code></pre>"},{"location":"api/flows/#lume_services.flows.flow.Flow","title":"<code>Flow</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Interface to a workflow object.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of flow</p> <code>flow_id</code> <code>Optional[str]</code> <p>ID of flow as registered with Prefect. If running locally, this will be null.</p> <code>project_name</code> <code>Optional[str]</code> <p>Name of Prefect project with which the flow is registered. If running locally this will be null.</p> <code>parameters</code> <code>Optional[Dict[str, Parameter]]</code> <p>Dictionary of Prefect parameters associated with the flow.</p> <code>mapped_parameters</code> <code>Optional[Dict[str, MappedParameter]]</code> <p>Parameters to be collected from results of other flows.</p> <code>task_slugs</code> <code>Optional[Dict[str, str]]</code> <p>Slug of tasks associated with the Prefect flow.</p> <code>labels</code> <code>List[str] = [\"lume-services\"]</code> <p>List of labels to assign to flow when registering with Prefect backend. This label is used to assign agents that will manage deployment.</p> <code>image</code> <code>str</code> <p>Image inside which to run flow if deploying to remote backend.</p> Source code in <code>lume_services/flows/flow.py</code> <pre><code>class Flow(BaseModel):\n    \"\"\"Interface to a workflow object.\n\n    Attributes:\n        name: Name of flow\n        flow_id (Optional[str]): ID of flow as registered with Prefect. If running\n            locally, this will be null.\n        project_name (Optional[str]): Name of Prefect project with which the flow is\n            registered. If running locally this will be null.\n        parameters (Optional[Dict[str, Parameter]]): Dictionary of Prefect parameters\n            associated with the flow.\n        mapped_parameters (Optional[Dict[str, MappedParameter]]): Parameters to be\n            collected from results of other flows.\n        task_slugs (Optional[Dict[str, str]]): Slug of tasks associated with the\n            Prefect flow.\n        labels (List[str] = [\"lume-services\"]): List of labels to assign to flow when\n            registering with Prefect backend. This label is used to assign agents that\n            will manage deployment.\n        image (str): Image inside which to run flow if deploying to remote backend.\n\n    \"\"\"\n\n    name: str\n    flow_id: Optional[str]\n    project_name: Optional[str]\n    prefect_flow: Optional[PrefectFlow]\n    parameters: Optional[Dict[str, Parameter]]\n    mapped_parameters: Optional[Dict[str, MappedParameter]]\n    task_slugs: Optional[Dict[str, str]]\n    labels: List[str] = [\"lume-services\"]\n    image: str\n\n    class Config:\n        arbitrary_types_allowed = True\n        validate_assignment = True\n\n    @validator(\"mapped_parameters\", pre=True)\n    def validate_mapped_parameters(cls, v):\n\n        if v is None:\n            return v\n\n        mapped_parameters = {}\n\n        for param_name, param in v.items():\n            # persist instantiated params\n            if isinstance(param, (MappedParameter,)):\n                mapped_parameters[param_name] = param\n\n            elif isinstance(param, (dict,)):\n                # default raw\n                if not param.get(\"map_type\"):\n                    mapped_parameters[param_name] = RawMappedParameter(**param)\n\n                else:\n                    mapped_param_type = _get_mapped_parameter_type(param[\"map_type\"])\n                    mapped_parameters[param_name] = mapped_param_type(**param)\n\n            else:\n                raise ValueError(\n                    \"Mapped parameters must be passed as instantiated \\\n                    MappedParameters or dictionary\"\n                )\n\n        return mapped_parameters\n\n    @inject\n    def load_flow(\n        self,\n        scheduling_service: SchedulingService = Provide[Context.scheduling_service],\n    ) -&gt; None:\n        \"\"\"Loads Prefect flow artifact from the backend.\n\n        Args:\n            scheduling_service (SchedulingService): Scheduling service. If not\n                provided, uses injected service.\n        \"\"\"\n        flow_dict = scheduling_service.load_flow(self.name, self.project_name)\n\n        flow = flow_dict[\"flow\"]\n\n        # assign attributes\n        self.prefect_flow = flow\n        self.task_slugs = {task.name: task.slug for task in flow.get_tasks()}\n        self.parameters = {parameter.name: parameter for parameter in flow.parameters()}\n        self.flow_id = flow_dict[\"flow_id\"]\n\n    @inject\n    def register(\n        self,\n        scheduling_service: SchedulingService = Provide[Context.scheduling_service],\n    ) -&gt; str:\n        \"\"\"Register flow with SchedulingService backend.\n\n        Args:\n            scheduling_service (SchedulingService): Scheduling service. If not\n                provided, uses injected service.\n\n        Returns:\n            flow_id (str): ID of registered flow.\n\n        \"\"\"\n\n        if self.prefect_flow is None:\n            # attempt loading\n            self.load_flow()\n\n        self.flow_id = scheduling_service.register_flow(\n            self.prefect_flow, self.project_name, labels=self.labels, image=self.image\n        )\n\n        self.parameters = {\n            parameter.name: parameter for parameter in self.prefect_flow.parameters()\n        }\n        self.task_slugs = {\n            task.name: task.slug for task in self.prefect_flow.get_tasks()\n        }\n\n        return self.flow_id\n\n    def run(\n        self,\n        parameters,\n        scheduling_service: SchedulingService = Provide[Context.scheduling_service],\n        **kwargs\n    ):\n        \"\"\"Run the flow.\n\n        Args:\n            kwargs: Arguments passed to run config construction\n\n        \"\"\"\n        if isinstance(scheduling_service.backend, (LocalBackend,)):\n            if self.prefect_flow is None:\n                self.load_flow()\n\n            scheduling_service.run(\n                parameters=parameters, flow=self.prefect_flow, **kwargs\n            )\n\n        elif isinstance(scheduling_service.backend, (ServerBackend,)):\n            scheduling_service.run(\n                parameters=parameters, flow_id=self.flow_id, image=self.image, **kwargs\n            )\n\n    def run_and_return(\n        self,\n        parameters,\n        task_name: Optional[str],\n        scheduling_service: SchedulingService = Provide[Context.scheduling_service],\n        **kwargs\n    ):\n        \"\"\"Run flow and return result. Result will reference either passed task name or\n        the result of all tasks.\n\n        Args:\n            kwargs: Arguments passed to run config construction\n\n\n        \"\"\"\n        if isinstance(scheduling_service.backend, (LocalBackend,)):\n            if self.prefect_flow is None:\n                self.load_flow()\n\n            return scheduling_service.run_and_return(\n                parameters=parameters,\n                flow=self.prefect_flow,\n                task_name=task_name,\n                image=self.image,\n                **kwargs\n            )\n\n        elif isinstance(scheduling_service.backend, (ServerBackend,)):\n            return scheduling_service.run_and_return(\n                parameters=parameters,\n                flow_id=self.flow_id,\n                task_name=task_name,\n                image=self.image,\n                **kwargs\n            )\n</code></pre>"},{"location":"api/flows/#lume_services.flows.flow.Flow.load_flow","title":"<code>load_flow(scheduling_service=Provide[Context.scheduling_service])</code>","text":"<p>Loads Prefect flow artifact from the backend.</p> <p>Parameters:</p> Name Type Description Default <code>scheduling_service</code> <code>SchedulingService</code> <p>Scheduling service. If not provided, uses injected service.</p> <code>Provide[scheduling_service]</code> Source code in <code>lume_services/flows/flow.py</code> <pre><code>@inject\ndef load_flow(\n    self,\n    scheduling_service: SchedulingService = Provide[Context.scheduling_service],\n) -&gt; None:\n    \"\"\"Loads Prefect flow artifact from the backend.\n\n    Args:\n        scheduling_service (SchedulingService): Scheduling service. If not\n            provided, uses injected service.\n    \"\"\"\n    flow_dict = scheduling_service.load_flow(self.name, self.project_name)\n\n    flow = flow_dict[\"flow\"]\n\n    # assign attributes\n    self.prefect_flow = flow\n    self.task_slugs = {task.name: task.slug for task in flow.get_tasks()}\n    self.parameters = {parameter.name: parameter for parameter in flow.parameters()}\n    self.flow_id = flow_dict[\"flow_id\"]\n</code></pre>"},{"location":"api/flows/#lume_services.flows.flow.Flow.register","title":"<code>register(scheduling_service=Provide[Context.scheduling_service])</code>","text":"<p>Register flow with SchedulingService backend.</p> <p>Parameters:</p> Name Type Description Default <code>scheduling_service</code> <code>SchedulingService</code> <p>Scheduling service. If not provided, uses injected service.</p> <code>Provide[scheduling_service]</code> <p>Returns:</p> Name Type Description <code>flow_id</code> <code>str</code> <p>ID of registered flow.</p> Source code in <code>lume_services/flows/flow.py</code> <pre><code>@inject\ndef register(\n    self,\n    scheduling_service: SchedulingService = Provide[Context.scheduling_service],\n) -&gt; str:\n    \"\"\"Register flow with SchedulingService backend.\n\n    Args:\n        scheduling_service (SchedulingService): Scheduling service. If not\n            provided, uses injected service.\n\n    Returns:\n        flow_id (str): ID of registered flow.\n\n    \"\"\"\n\n    if self.prefect_flow is None:\n        # attempt loading\n        self.load_flow()\n\n    self.flow_id = scheduling_service.register_flow(\n        self.prefect_flow, self.project_name, labels=self.labels, image=self.image\n    )\n\n    self.parameters = {\n        parameter.name: parameter for parameter in self.prefect_flow.parameters()\n    }\n    self.task_slugs = {\n        task.name: task.slug for task in self.prefect_flow.get_tasks()\n    }\n\n    return self.flow_id\n</code></pre>"},{"location":"api/flows/#lume_services.flows.flow.Flow.run","title":"<code>run(parameters, scheduling_service=Provide[Context.scheduling_service], **kwargs)</code>","text":"<p>Run the flow.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Arguments passed to run config construction</p> <code>{}</code> Source code in <code>lume_services/flows/flow.py</code> <pre><code>def run(\n    self,\n    parameters,\n    scheduling_service: SchedulingService = Provide[Context.scheduling_service],\n    **kwargs\n):\n    \"\"\"Run the flow.\n\n    Args:\n        kwargs: Arguments passed to run config construction\n\n    \"\"\"\n    if isinstance(scheduling_service.backend, (LocalBackend,)):\n        if self.prefect_flow is None:\n            self.load_flow()\n\n        scheduling_service.run(\n            parameters=parameters, flow=self.prefect_flow, **kwargs\n        )\n\n    elif isinstance(scheduling_service.backend, (ServerBackend,)):\n        scheduling_service.run(\n            parameters=parameters, flow_id=self.flow_id, image=self.image, **kwargs\n        )\n</code></pre>"},{"location":"api/flows/#lume_services.flows.flow.Flow.run_and_return","title":"<code>run_and_return(parameters, task_name, scheduling_service=Provide[Context.scheduling_service], **kwargs)</code>","text":"<p>Run flow and return result. Result will reference either passed task name or the result of all tasks.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Arguments passed to run config construction</p> <code>{}</code> Source code in <code>lume_services/flows/flow.py</code> <pre><code>def run_and_return(\n    self,\n    parameters,\n    task_name: Optional[str],\n    scheduling_service: SchedulingService = Provide[Context.scheduling_service],\n    **kwargs\n):\n    \"\"\"Run flow and return result. Result will reference either passed task name or\n    the result of all tasks.\n\n    Args:\n        kwargs: Arguments passed to run config construction\n\n\n    \"\"\"\n    if isinstance(scheduling_service.backend, (LocalBackend,)):\n        if self.prefect_flow is None:\n            self.load_flow()\n\n        return scheduling_service.run_and_return(\n            parameters=parameters,\n            flow=self.prefect_flow,\n            task_name=task_name,\n            image=self.image,\n            **kwargs\n        )\n\n    elif isinstance(scheduling_service.backend, (ServerBackend,)):\n        return scheduling_service.run_and_return(\n            parameters=parameters,\n            flow_id=self.flow_id,\n            task_name=task_name,\n            image=self.image,\n            **kwargs\n        )\n</code></pre>"},{"location":"api/flows/#lume_services.flows.flow.MappedParameter","title":"<code>MappedParameter</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>There are three types of mapped parameters: file, db, and raw.</p> <p>file: File parameters are file outputs that will be loaded in downstream flows. Downstream loading must use the packaged <code>load_file</code> task in <code>lume_services.tasks.file</code>.</p> <p>db: Database results ...</p> <p>raw: Raw values are passed from task output to parameter input.</p> Attr <p>parent_flow_name (str): Parent flow holding origin of mapped parameter. parent_task_name (str): Task whose result is mapped to the parameter. map_type (Literal[\"file\", \"db\", \"raw\"]): Type of mapping describing the     parameters.</p> Source code in <code>lume_services/flows/flow.py</code> <pre><code>class MappedParameter(BaseModel):\n    \"\"\"There are three types of mapped parameters: file, db, and raw.\n\n    file: File parameters are file outputs that will be loaded in downstream flows.\n    Downstream loading must use the packaged `load_file` task in\n    `lume_services.tasks.file`.\n\n    db: Database results ...\n\n    raw: Raw values are passed from task output to parameter input.\n\n    Attr:\n        parent_flow_name (str): Parent flow holding origin of mapped parameter.\n        parent_task_name (str): Task whose result is mapped to the parameter.\n        map_type (Literal[\"file\", \"db\", \"raw\"]): Type of mapping describing the\n            parameters.\n\n    \"\"\"\n\n    parent_flow_name: str\n    parent_task_name: str\n    map_type: Literal[\"file\", \"db\", \"raw\"] = \"raw\"\n</code></pre>"},{"location":"api/flows/#lume_services.flows.flow.RawMappedParameter","title":"<code>RawMappedParameter</code>","text":"<p>               Bases: <code>MappedParameter</code></p> <p>RawMappedParameters describe parameter mappings where the result of a task is used as the input to a parameter.</p> Attr <p>parent_flow_name (str): Parent flow holding origin of mapped parameter. parent_task_name (str): Task whose result is mapped to the parameter. map_type (Literal[\"file\", \"db\", \"raw\"] = \"raw\"): The \"raw\" map type describes     the one-to-one result to parameter map.</p> Source code in <code>lume_services/flows/flow.py</code> <pre><code>class RawMappedParameter(MappedParameter):\n    \"\"\"RawMappedParameters describe parameter mappings where the result of a task is\n    used as the input to a parameter.\n\n    Attr:\n        parent_flow_name (str): Parent flow holding origin of mapped parameter.\n        parent_task_name (str): Task whose result is mapped to the parameter.\n        map_type (Literal[\"file\", \"db\", \"raw\"] = \"raw\"): The \"raw\" map type describes\n            the one-to-one result to parameter map.\n\n    \"\"\"\n\n    map_type: str = Field(\"raw\", const=True)\n</code></pre>"},{"location":"api/flows/#lume_services.flows.flow_of_flows.FlowOfFlows","title":"<code>FlowOfFlows</code>","text":"<p>               Bases: <code>Flow</code></p> Source code in <code>lume_services/flows/flow_of_flows.py</code> <pre><code>class FlowOfFlows(Flow):\n    composing_flows: dict\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    @root_validator(pre=True)\n    def validate(cls, values: dict):\n        \"\"\"Validate composing flow data against Prefect server.\"\"\"\n        flows = {}\n\n        scheduling_service = None\n        if \"scheduling_service\" in values:\n            scheduling_service = values.pop(\"scheduling_service\")\n\n        # validate composing flow existence\n        composing_flows = values.get(\"composing_flows\")\n\n        if isinstance(composing_flows, (dict,)):\n            pass\n\n        # iterate to create dict\n        elif isinstance(composing_flows, (list,)):\n            for flow in values[\"composing_flows\"]:\n\n                # compose flow objects\n                flow_obj = Flow(\n                    name=flow[\"name\"],\n                    project_name=flow[\"project_name\"],\n                    mapped_parameters=flow.get(\"mapped_parameters\"),\n                )\n\n                # load Prefect parameters\n                if scheduling_service is not None:\n                    flow_obj.load_flow(scheduling_service=scheduling_service)\n                else:\n                    flow_obj.load_flow()\n\n                flows[flow[\"name\"]] = flow_obj\n\n        # validate flow parameters\n        for flow_name, flow in flows.items():\n            if flow.mapped_parameters is not None:\n                for parameter_name, parameter in flow.mapped_parameters.items():\n\n                    # validate parameter is in flow spec\n                    parameter_obj = flow.parameters.get(parameter_name)\n                    if parameter_obj is None:\n                        raise ParameterNotInFlowError(parameter_name, flow_name)\n\n                    # validate parent flow is included in listed flows\n                    parent_flow = flows.get(parameter.parent_flow_name)\n                    if parent_flow is None:\n                        raise ParentFlowNotInFlowsError(\n                            parameter.parent_flow_name, list(flows.keys())\n                        )\n\n                    # validate task is in the parent flow\n                    task = parent_flow.task_slugs.get(parameter.parent_task_name)\n\n                    if task is None:\n                        raise TaskNotInFlowError(\n                            parameter.parent_flow_name, parameter.parent_task_name\n                        )\n\n        values[\"composing_flows\"] = flows\n\n        return values\n\n    def compose(\n        self,\n        image_name: str,\n        image_tag: str = \"latest\",\n        local: bool = False,\n        scheduling_service: SchedulingService = Provide[Context.scheduling_service],\n    ) -&gt; PrefectFlow:\n        \"\"\"Compose Prefect flow from FlowOfFlows object. Uses base image assigned to\n        the FlowOfFlows Object and builds a new Docker image containing the composite\n        flow.\n\n\n        Args:\n            image_name (str): Name of generated image.\n            image_tag (str): Tag of generated image.\n            local (bool=False): Whether to use local images for the base image.\n\n\n        Returns:\n            PrefectFlow\n\n        \"\"\"\n\n        # compose flow of flows\n        with PrefectFlow(\n            self.name,\n            storage=Docker(\n                base_image=self.image,\n                image_name=image_name,\n                image_tag=image_tag,\n                local_image=local,\n            ),\n        ) as composed_flow:\n\n            flow_runs = {}\n            flow_waits = {}\n            params = {}\n\n            for i, (flow_name, flow) in enumerate(self.composing_flows.items()):\n\n                # begin by creating parameters for all flow parameters\n                flow_params = {}\n                for param_name, param in flow.parameters.items():\n\n                    # update name and slug\n                    param.name = f\"{flow_name}-{param_name}\"\n                    param.slug = f\"{flow_name}-{param_name}\"\n                    params[param.name] = param\n\n                    # use original param name for flow config\n                    flow_params[param_name] = param\n\n                # set up entry task\n                if i == 0:\n                    flow_run = create_flow_run(\n                        flow_id=flow.flow_id,\n                        parameters=flow_params,\n                        labels=flow.labels,\n                    )\n\n                # setup other tasks\n                elif i &gt; 0:\n\n                    # create references to parameters\n                    upstream_flows = set()\n                    if flow.mapped_parameters is not None:\n\n                        # update flow_params with mapping\n                        for param_name, mapped_param in flow.mapped_parameters.items():\n                            task_slug = self.composing_flows[\n                                mapped_param.parent_flow_name\n                            ].task_slugs[mapped_param.parent_task_name]\n\n                            task_run_result = get_task_run_result(\n                                flow_runs[mapped_param.parent_flow_name], task_slug\n                            )\n\n                            # raw results and file results use their values directly\n                            if mapped_param.map_type in [\"raw\", \"file\"]:\n                                flow.prefect_flow.replace(\n                                    flow_params.pop(param_name), task_run_result\n                                )\n\n                            # handle database results\n                            elif mapped_param.map_type == \"db\":\n                                load_db_result = LoadDBResult()\n                                db_result = load_db_result(\n                                    task_run_result,\n                                    attribute_index=mapped_param.attribute_index,\n                                )\n                                flow.prefect_flow.replace(\n                                    flow_params.pop(param_name), db_result\n                                )\n\n                                # add db result parameters to the task and create edge\n                                for param in load_db_result.parameters.values():\n                                    flow.prefect_flow.add_task(param)\n                                    flow.prefect_flow.add_edge(\n                                        param, load_db_result, mapped=True\n                                    )\n\n                            else:\n                                # should never reach if instantiating MappedParameter\n                                mapped_param_types = get_args(\n                                    MappedParameter.__fields__[\"map_type\"].type_\n                                )\n                                raise ValueError(\n                                    f\"Task type {mapped_param.map_type} not in task. \\\n                                        Allowed types: {mapped_param_types}.\"\n                                )\n\n                            # add flow to upstream\n                            upstream_flows.add(mapped_param.parent_flow_name)\n\n                        # add creation of flow run to flow\n                        flow_run = create_flow_run(\n                            flow_id=flow.flow_id,\n                            parameters=flow_params,\n                            labels=flow.labels,\n                        )\n\n                    # configure upstreams if any\n                    for upstream in upstream_flows:\n                        flow_run.set_upstream(flow_waits[upstream])\n\n                flow_wait = wait_for_flow_run(flow_run, raise_final_state=True)\n                flow_runs[flow_name] = flow_run\n                flow_waits[flow_name] = flow_wait\n\n        # validate flow of flows\n        composed_flow.validate()\n\n        # assign to obj\n        self.prefect_flow = composed_flow\n        self.image = f\"{image_name}:{image_tag}\"\n\n        return composed_flow\n\n    def compose_and_register(self) -&gt; str:\n        \"\"\"Compose flow and register with project.\n\n        Returns:\n            str: Registered flow id\n\n        \"\"\"\n\n        flow = self.compose()\n        self.prefect_flow = flow\n        return self.register(self.project_name)\n\n    @classmethod\n    @inject\n    def from_yaml(\n        cls,\n        yaml_obj,\n        scheduling_service: SchedulingService = Provide[Context.scheduling_service],\n    ):\n        if os.path.exists(yaml_obj):\n            flow_of_flow_config = yaml.safe_load(open(yaml_obj))\n\n        else:\n            flow_of_flow_config = yaml_obj\n\n        # now validate\n        return cls(**flow_of_flow_config, scheduling_service=scheduling_service)\n\n    def _compose_local(self):\n        \"\"\"\n\n        Note:\n            Prefect 1.0 does not allow subflow run without previous registration with\n            the server. This function is a workaround, but will be massively simplified\n            once moved to Prefect 2.0, which does support direct subflow run.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/flows/#lume_services.flows.flow_of_flows.FlowOfFlows.compose","title":"<code>compose(image_name, image_tag='latest', local=False, scheduling_service=Provide[Context.scheduling_service])</code>","text":"<p>Compose Prefect flow from FlowOfFlows object. Uses base image assigned to the FlowOfFlows Object and builds a new Docker image containing the composite flow.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>Name of generated image.</p> required <code>image_tag</code> <code>str</code> <p>Tag of generated image.</p> <code>'latest'</code> <code>local</code> <code>bool=False</code> <p>Whether to use local images for the base image.</p> <code>False</code> <p>Returns:</p> Type Description <code>Flow</code> <p>PrefectFlow</p> Source code in <code>lume_services/flows/flow_of_flows.py</code> <pre><code>def compose(\n    self,\n    image_name: str,\n    image_tag: str = \"latest\",\n    local: bool = False,\n    scheduling_service: SchedulingService = Provide[Context.scheduling_service],\n) -&gt; PrefectFlow:\n    \"\"\"Compose Prefect flow from FlowOfFlows object. Uses base image assigned to\n    the FlowOfFlows Object and builds a new Docker image containing the composite\n    flow.\n\n\n    Args:\n        image_name (str): Name of generated image.\n        image_tag (str): Tag of generated image.\n        local (bool=False): Whether to use local images for the base image.\n\n\n    Returns:\n        PrefectFlow\n\n    \"\"\"\n\n    # compose flow of flows\n    with PrefectFlow(\n        self.name,\n        storage=Docker(\n            base_image=self.image,\n            image_name=image_name,\n            image_tag=image_tag,\n            local_image=local,\n        ),\n    ) as composed_flow:\n\n        flow_runs = {}\n        flow_waits = {}\n        params = {}\n\n        for i, (flow_name, flow) in enumerate(self.composing_flows.items()):\n\n            # begin by creating parameters for all flow parameters\n            flow_params = {}\n            for param_name, param in flow.parameters.items():\n\n                # update name and slug\n                param.name = f\"{flow_name}-{param_name}\"\n                param.slug = f\"{flow_name}-{param_name}\"\n                params[param.name] = param\n\n                # use original param name for flow config\n                flow_params[param_name] = param\n\n            # set up entry task\n            if i == 0:\n                flow_run = create_flow_run(\n                    flow_id=flow.flow_id,\n                    parameters=flow_params,\n                    labels=flow.labels,\n                )\n\n            # setup other tasks\n            elif i &gt; 0:\n\n                # create references to parameters\n                upstream_flows = set()\n                if flow.mapped_parameters is not None:\n\n                    # update flow_params with mapping\n                    for param_name, mapped_param in flow.mapped_parameters.items():\n                        task_slug = self.composing_flows[\n                            mapped_param.parent_flow_name\n                        ].task_slugs[mapped_param.parent_task_name]\n\n                        task_run_result = get_task_run_result(\n                            flow_runs[mapped_param.parent_flow_name], task_slug\n                        )\n\n                        # raw results and file results use their values directly\n                        if mapped_param.map_type in [\"raw\", \"file\"]:\n                            flow.prefect_flow.replace(\n                                flow_params.pop(param_name), task_run_result\n                            )\n\n                        # handle database results\n                        elif mapped_param.map_type == \"db\":\n                            load_db_result = LoadDBResult()\n                            db_result = load_db_result(\n                                task_run_result,\n                                attribute_index=mapped_param.attribute_index,\n                            )\n                            flow.prefect_flow.replace(\n                                flow_params.pop(param_name), db_result\n                            )\n\n                            # add db result parameters to the task and create edge\n                            for param in load_db_result.parameters.values():\n                                flow.prefect_flow.add_task(param)\n                                flow.prefect_flow.add_edge(\n                                    param, load_db_result, mapped=True\n                                )\n\n                        else:\n                            # should never reach if instantiating MappedParameter\n                            mapped_param_types = get_args(\n                                MappedParameter.__fields__[\"map_type\"].type_\n                            )\n                            raise ValueError(\n                                f\"Task type {mapped_param.map_type} not in task. \\\n                                    Allowed types: {mapped_param_types}.\"\n                            )\n\n                        # add flow to upstream\n                        upstream_flows.add(mapped_param.parent_flow_name)\n\n                    # add creation of flow run to flow\n                    flow_run = create_flow_run(\n                        flow_id=flow.flow_id,\n                        parameters=flow_params,\n                        labels=flow.labels,\n                    )\n\n                # configure upstreams if any\n                for upstream in upstream_flows:\n                    flow_run.set_upstream(flow_waits[upstream])\n\n            flow_wait = wait_for_flow_run(flow_run, raise_final_state=True)\n            flow_runs[flow_name] = flow_run\n            flow_waits[flow_name] = flow_wait\n\n    # validate flow of flows\n    composed_flow.validate()\n\n    # assign to obj\n    self.prefect_flow = composed_flow\n    self.image = f\"{image_name}:{image_tag}\"\n\n    return composed_flow\n</code></pre>"},{"location":"api/flows/#lume_services.flows.flow_of_flows.FlowOfFlows.compose_and_register","title":"<code>compose_and_register()</code>","text":"<p>Compose flow and register with project.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Registered flow id</p> Source code in <code>lume_services/flows/flow_of_flows.py</code> <pre><code>def compose_and_register(self) -&gt; str:\n    \"\"\"Compose flow and register with project.\n\n    Returns:\n        str: Registered flow id\n\n    \"\"\"\n\n    flow = self.compose()\n    self.prefect_flow = flow\n    return self.register(self.project_name)\n</code></pre>"},{"location":"api/flows/#lume_services.flows.flow_of_flows.FlowOfFlows.validate","title":"<code>validate(values)</code>","text":"<p>Validate composing flow data against Prefect server.</p> Source code in <code>lume_services/flows/flow_of_flows.py</code> <pre><code>@root_validator(pre=True)\ndef validate(cls, values: dict):\n    \"\"\"Validate composing flow data against Prefect server.\"\"\"\n    flows = {}\n\n    scheduling_service = None\n    if \"scheduling_service\" in values:\n        scheduling_service = values.pop(\"scheduling_service\")\n\n    # validate composing flow existence\n    composing_flows = values.get(\"composing_flows\")\n\n    if isinstance(composing_flows, (dict,)):\n        pass\n\n    # iterate to create dict\n    elif isinstance(composing_flows, (list,)):\n        for flow in values[\"composing_flows\"]:\n\n            # compose flow objects\n            flow_obj = Flow(\n                name=flow[\"name\"],\n                project_name=flow[\"project_name\"],\n                mapped_parameters=flow.get(\"mapped_parameters\"),\n            )\n\n            # load Prefect parameters\n            if scheduling_service is not None:\n                flow_obj.load_flow(scheduling_service=scheduling_service)\n            else:\n                flow_obj.load_flow()\n\n            flows[flow[\"name\"]] = flow_obj\n\n    # validate flow parameters\n    for flow_name, flow in flows.items():\n        if flow.mapped_parameters is not None:\n            for parameter_name, parameter in flow.mapped_parameters.items():\n\n                # validate parameter is in flow spec\n                parameter_obj = flow.parameters.get(parameter_name)\n                if parameter_obj is None:\n                    raise ParameterNotInFlowError(parameter_name, flow_name)\n\n                # validate parent flow is included in listed flows\n                parent_flow = flows.get(parameter.parent_flow_name)\n                if parent_flow is None:\n                    raise ParentFlowNotInFlowsError(\n                        parameter.parent_flow_name, list(flows.keys())\n                    )\n\n                # validate task is in the parent flow\n                task = parent_flow.task_slugs.get(parameter.parent_task_name)\n\n                if task is None:\n                    raise TaskNotInFlowError(\n                        parameter.parent_flow_name, parameter.parent_task_name\n                    )\n\n    values[\"composing_flows\"] = flows\n\n    return values\n</code></pre>"},{"location":"api/results/","title":"Results","text":""},{"location":"api/results/#lume_services.results.generic.Result","title":"<code>Result</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Creates a data model for a result and generates a unique result hash.</p> Source code in <code>lume_services/results/generic.py</code> <pre><code>class Result(BaseModel):\n    \"\"\"Creates a data model for a result and generates a unique result hash.\"\"\"\n\n    project_name: str = Field(\n        \"local\", alias=\"collection\"\n    )  # this will be the project_name for the scheduled flow\n\n    # database id\n    id: Optional[str] = Field(alias=\"_id\", exclude=True)\n\n    # db fields\n    flow_id: str\n    inputs: Dict[str, Union[float, str, np.ndarray, list, pd.DataFrame]]\n    outputs: Dict[str, Union[float, str, np.ndarray, list, pd.DataFrame]]\n    date_modified: datetime = datetime.utcnow()\n\n    # set of establishes uniqueness\n    unique_on: List[str] = Field(\n        [\"inputs\", \"outputs\", \"flow_id\"], alias=\"index\", exclude=True\n    )\n\n    # establishes uniqueness\n    unique_hash: str\n\n    # store result type\n    result_type_string: str\n\n    class Config:\n        arbitrary_types_allowed = True\n        json_encoders = JSON_ENCODERS\n        allow_population_by_field_name = True\n        extra = Extra.forbid\n\n    _round_datetime_to_milliseconds = validator(\n        \"date_modified\", allow_reuse=True, always=True, pre=True\n    )(round_datetime_to_milliseconds)\n\n    @validator(\"inputs\", pre=True)\n    def validate_inputs(cls, v):\n        return load_db_dict(v)\n\n    @validator(\"outputs\", pre=True)\n    def validate_outputs(cls, v):\n        return load_db_dict(v)\n\n    @root_validator(pre=True)\n    def validate_all(cls, values):\n        unique_fields = cls.__fields__[\"unique_on\"].default\n\n        # If flow_id is not passed, check prefect context\n        if not values.get(\"flow_id\"):\n            if not hasattr(prefect_context, \"flow_id\"):\n                raise ValueError(\"No flow_id passed to result\")\n\n            values[\"flow_id\"] = prefect_context.flow_id\n\n        if not values.get(\"collection\") and not values.get(\"project_name\"):\n            if not hasattr(prefect_context, \"project_name\"):\n                logger.warning(\"No project_name passed to result\")\n\n            else:\n                values[\"project_name\"] = prefect_context.project_name\n\n        # create index hash\n        if not values.get(\"unique_hash\"):\n\n            for field in unique_fields:\n                if not values.get(field):\n                    raise ValueError(\"%s not provided.\", field)\n\n            values[\"unique_hash\"] = fingerprint_dict(\n                {index: values[index] for index in unique_fields}\n            )\n\n        if values.get(\"_id\"):\n            _id = values[\"_id\"]\n            if isinstance(_id, (ObjectId,)):\n                values[\"_id\"] = str(values[\"_id\"])\n\n        values[\"result_type_string\"] = f\"{cls.__module__}.{cls.__name__}\"\n\n        return values\n\n    def get_unique_result_index(self) -&gt; dict:\n        return {field: getattr(self, field) for field in self.unique_on}\n\n    @inject\n    def insert(\n        self, results_db_service: ResultsDB = Provide[Context.results_db_service]\n    ):\n\n        # must convert to jsonable dict\n        rep = self.get_db_dict()\n        return results_db_service.insert_one(rep)\n\n    @classmethod\n    @inject\n    def load_from_query(\n        cls,\n        project_name: str,\n        query: dict,\n        results_db_service: ResultsDB = Provide[Context.results_db_service],\n    ):\n        query = get_bson_dict(query)\n        res = results_db_service.find(collection=project_name, query=query)\n\n        if len(res) == 0:\n            raise ValueError(\"Provided query returned no results. %s\", query)\n\n        elif len(res) &gt; 1:\n            raise ValueError(\"Provided query returned multiple results. %s\", query)\n\n        values = load_db_dict(res[0])\n        return cls(project_name=project_name, **values)\n\n    def unique_rep(self) -&gt; dict:\n        \"\"\"Get minimal representation needed to load result object from database.\"\"\"\n\n        return {\n            \"project_name\": self.project_name,\n            \"result_type_string\": self.result_type_string,\n            \"query\": {\"unique_hash\": self.unique_hash},\n        }\n\n    def get_db_dict(self) -&gt; dict:\n        rep = self.dict(by_alias=True)\n        return get_bson_dict(rep)\n</code></pre>"},{"location":"api/results/#lume_services.results.generic.Result.unique_rep","title":"<code>unique_rep()</code>","text":"<p>Get minimal representation needed to load result object from database.</p> Source code in <code>lume_services/results/generic.py</code> <pre><code>def unique_rep(self) -&gt; dict:\n    \"\"\"Get minimal representation needed to load result object from database.\"\"\"\n\n    return {\n        \"project_name\": self.project_name,\n        \"result_type_string\": self.result_type_string,\n        \"query\": {\"unique_hash\": self.unique_hash},\n    }\n</code></pre>"},{"location":"api/results/#lume_services.results.generic.get_bson_dict","title":"<code>get_bson_dict(dictionary)</code>","text":"<p>Recursively converts numpy arrays inside a dictionary to bson encoded items and pandas dataframes to json reps.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>Dictionary to load.</p> required <p>Returns     dict</p> Source code in <code>lume_services/results/generic.py</code> <pre><code>def get_bson_dict(dictionary: dict) -&gt; dict:\n    \"\"\"Recursively converts numpy arrays inside a dictionary to bson encoded items and\n    pandas dataframes to json reps.\n\n    Args:\n        dictionary (dict): Dictionary to load.\n\n    Returns\n        dict\n    \"\"\"\n\n    def convert_values(dictionary):\n        \"\"\"Convert values to list so the dictionary can be inserted and loaded.\"\"\"\n\n        # convert numpy arrays to binary format\n        dictionary = {\n            key: Binary(pickle.dumps(value, protocol=2))\n            if isinstance(value, (np.ndarray,))\n            else value\n            for key, value in dictionary.items()\n        }\n\n        # convert pandas array to json\n        dictionary = {\n            key: value.to_json() if isinstance(value, (pd.DataFrame,)) else value\n            for key, value in dictionary.items()\n        }\n\n        # create file rep\n        dictionary = {\n            key: value.jsonable_dict() if isinstance(value, (File,)) else value\n            for key, value in dictionary.items()\n        }\n\n        dictionary = {\n            key: convert_values(value) if isinstance(value, (dict,)) else value\n            for key, value in dictionary.items()\n        }\n        return dictionary\n\n    return convert_values(dictionary)\n</code></pre>"},{"location":"api/results/#lume_services.results.generic.load_db_dict","title":"<code>load_db_dict(dictionary)</code>","text":"<p>Loads representation of mongodb dictionary with appropriate python classes. Numpy arrays are loaded from binary objects and pandas dataframes from json blobs.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>Dictionary to load.</p> required Source code in <code>lume_services/results/generic.py</code> <pre><code>def load_db_dict(dictionary: dict):\n    \"\"\"Loads representation of mongodb dictionary with appropriate python classes.\n    Numpy arrays are loaded from binary objects and pandas dataframes from json blobs.\n\n    Args:\n        dictionary (dict): Dictionary to load.\n\n    \"\"\"\n\n    def check_and_convert_json_str(string: str):\n        try:\n\n            loaded_ = json.loads(string)\n            return pd.DataFrame(loaded_)\n\n        except json.JSONDecodeError:\n            return string\n\n    def convert_values(dictionary):\n        if \"file_type_string\" in dictionary:\n            file_type = get_file_from_serializer_string(dictionary[\"file_type_string\"])\n            return file_type(**dictionary)\n\n        # convert numpy arrays from binary format\n        dictionary = {\n            key: pickle.loads(value) if isinstance(value, (bytes,)) else value\n            for key, value in dictionary.items()\n        }\n\n        # convert pandas array to json\n        dictionary = {\n            key: check_and_convert_json_str(value)\n            if isinstance(value, (str,))\n            else value\n            for key, value in dictionary.items()\n        }\n\n        dictionary = {\n            key: convert_values(value) if isinstance(value, (dict,)) else value\n            for key, value in dictionary.items()\n        }\n\n        return dictionary\n\n    return convert_values(dictionary)\n</code></pre>"},{"location":"api/results/#lume_services.results.generic.round_datetime_to_milliseconds","title":"<code>round_datetime_to_milliseconds(time)</code>","text":"<p>Mongodb rounds datetime to milliseconds so round on assignment for consistency.</p> Source code in <code>lume_services/results/generic.py</code> <pre><code>def round_datetime_to_milliseconds(time: Union[datetime, str]) -&gt; datetime:\n    \"\"\"Mongodb rounds datetime to milliseconds so round on assignment for\n    consistency.\n\n    \"\"\"\n    if isinstance(time, datetime):\n        time = time.isoformat(timespec=\"milliseconds\")\n    return time\n</code></pre>"},{"location":"api/results/#lume_services.results.impact.ImpactResult","title":"<code>ImpactResult</code>","text":"<p>               Bases: <code>Result</code></p> <p>Extends Result base and implements Impact specific attributes</p> Source code in <code>lume_services/results/impact.py</code> <pre><code>class ImpactResult(Result):\n    \"\"\"Extends Result base and implements Impact specific attributes\"\"\"\n\n    plot_file: Optional[ImageFile]\n    archive: HDF5File\n    pv_collection_isotime: datetime\n    config: dict\n\n    _round_datetime_to_milliseconds = validator(\n        \"pv_collection_isotime\", allow_reuse=True, always=True, pre=True\n    )(round_datetime_to_milliseconds)\n\n    @validator(\"plot_file\", pre=True)\n    def validate_plot_file(cls, v):\n\n        # if the plot file isinstance of dictionary, load file type\n        if isinstance(v, dict):\n            return ImageFile(**v)\n\n        return v\n\n    @validator(\"archive\", pre=True)\n    def validate_archive_file(cls, v):\n\n        # if the plot file isinstance of dictionary, load file type\n        if isinstance(v, dict):\n            return HDF5File(**v)\n\n        return v\n</code></pre>"},{"location":"api/tasks/","title":"Tasks","text":""},{"location":"api/tasks/#lume_services.tasks.configure.configure_lume_services","title":"<code>configure_lume_services()</code>","text":"<p>Configure LUME-services using environment variables. This task must be included in any workflow using common database services.</p> Source code in <code>lume_services/tasks/configure.py</code> <pre><code>@task(name=\"configure_lume_services\")\ndef configure_lume_services():\n    \"\"\"Configure LUME-services using environment variables. This task must be included\n    in any workflow using common database services.\n\n    \"\"\"\n    logger.debug(\"Configuring environment using %s\", json.dumps(dict(os.environ)))\n    config.configure()\n</code></pre>"},{"location":"api/tasks/#lume_services.tasks.configure.prepare_lume_model_variables","title":"<code>prepare_lume_model_variables(value_map, variables)</code>","text":"<p>Utility task for translating parameter values to LUME-model variables.</p> <p>Parameters:</p> Name Type Description Default <code>value_map</code> <code>Dict[str, Any]</code> <p>Dictionary mapping variable name to value.</p> required <code>variables</code> <code>Dict[str, Variable]</code> <p>Dictionary mapping variable name to LUME-model variable object.</p> required <p>Returns:</p> Name Type Description <code>variables</code> <code>Dict[str, Variable]</code> <p>Formatted LUME-model variables.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Variable name passed to value_map is not found in model variables.</p> Source code in <code>lume_services/tasks/configure.py</code> <pre><code>@task(name=\"prepare_lume_model_variables\")\ndef prepare_lume_model_variables(\n    value_map: Dict[str, Any], variables: Dict[str, Variable]\n) -&gt; Dict[str, Variable]:\n    \"\"\"Utility task for translating parameter values to LUME-model variables.\n\n    Args:\n        value_map (Dict[str, Any]): Dictionary mapping variable name to value.\n        variables (Dict[str, Variable]): Dictionary mapping variable name to LUME-model\n            variable object.\n\n    Returns:\n        variables (Dict[str, Variable]): Formatted LUME-model variables.\n\n    Raises:\n        ValueError: Variable name passed to value_map is not found in model variables.\n\n    \"\"\"\n    # Use deepcopy because don't want to transform global vars\n    variables = copy.deepcopy(variables)\n\n    for var_name in value_map:\n        if var_name not in variables:\n            raise ValueError(\n                \"Variable name passed to value_map %s not found in model variables. \\\n                Model variables are %s\",\n                var_name,\n                \",\".join(list(variables.keys())),\n            )\n        variables[var_name].value = value_map[var_name]\n\n    missing_values = [\n        var_name for var_name in variables.keys() if var_name not in value_map\n    ]\n    if len(missing_values):\n        logger.warning(\n            \"No value provided for: %s. Will assign variable default to value.\",\n            \", \".join(missing_values),\n        )\n    for var_name in missing_values:\n        variables[var_name].value = variables[var_name].default\n\n    return variables\n</code></pre>"},{"location":"api/tasks/#lume_services.tasks.db.LoadDBResult","title":"<code>LoadDBResult</code>","text":"<p>               Bases: <code>Task</code></p> <p>Load a result from the results database. All database results generate a minimally representative identifier that can be used to query the database and load the result. This idenifier is jsonable and therefore accessable outside of the workflow's scope. This task uses the identifier to load the database query and performs data selection via the <code>attribute_index</code> parameters.</p> <p>The <code>attribute_index</code> parameter provides selection instructions from query. For example, selecting the first <code>toyota</code> from a dictionary of form: <code>{\"outputs\": {\"vehicle\": {\"car\":  [\"toyota\", \"mini\"], \"boat\": [\"sail\", \"motor\"]}}}</code>would be accomplished by passing <code>attribute_index=[\"outputs\", \"vehicle\", \"car\", 0]</code>.</p> <p>This task is defined as a subclass of the Prefect Task object and accepts all Task arguments during initialization.</p> <p>Examples:</p> <pre><code>from prefect import Flow, task\nfrom lume_services.tasks import configure_lume_services, LoadDBResult\n\n\nload_db_result_task = LoadDBResult(timeout=20)\n\n@task\ndef print_result(value):\n    print(value)\n\n\nwith Flow(\n    \"my_flow\"\n) as flow:\n    # must first configure services because using injected results\n    # database service\n    configure_lume_services()\n\n    # assume some other flow has saved a Result object to the database with the\n    # resulting unique representation (Result.unique_rep):\n    unique_rep = {\n        \"result_type_string\": \"lume_services.results.generic:Result\",\n        \"query\": {\n               \"inputs\": {\n                   \"input1\": 1.0,\n                   \"input2\": 2.0\n                },\n                \"outputs\": {\n                    \"output1\": 1.0,\n                    \"output2\": 2.0\n                },\n               \"flow_id\": 1\n        }\n    }\n\n    my_result = load_db_result_task(\n        result_rep = unique_rep,\n        attribute_index=[\"outputs\", \"output1\"]\n    )\n\n    print_result(my_result) # Will print 1.0\n</code></pre> Source code in <code>lume_services/tasks/db.py</code> <pre><code>class LoadDBResult(Task):\n    \"\"\"Load a result from the results database. All database results generate a\n    [minimally representative identifier][lume_services.results.generic.Result] that can\n    be used to query the database and load the result. This idenifier is jsonable and\n    therefore accessable outside of the workflow's scope. This task uses the identifier\n    to load the database query and performs data selection via the `attribute_index`\n    parameters.\n\n    The `attribute_index` parameter provides selection instructions from query. For\n    example, selecting the first `toyota` from a dictionary of form: `{\"outputs\":\n    {\"vehicle\": {\"car\":  [\"toyota\", \"mini\"], \"boat\": [\"sail\", \"motor\"]}}}`would be\n    accomplished by passing `attribute_index=[\"outputs\", \"vehicle\", \"car\", 0]`.\n\n    This task is defined as a subclass of the Prefect [Task](https://docs-v1.prefect.io/api/latest/core/task.html#task-2)\n    object and accepts all Task arguments during initialization.\n\n    Examples:\n        ```python\n        from prefect import Flow, task\n        from lume_services.tasks import configure_lume_services, LoadDBResult\n\n\n        load_db_result_task = LoadDBResult(timeout=20)\n\n        @task\n        def print_result(value):\n            print(value)\n\n\n        with Flow(\n            \"my_flow\"\n        ) as flow:\n            # must first configure services because using injected results\n            # database service\n            configure_lume_services()\n\n            # assume some other flow has saved a Result object to the database with the\n            # resulting unique representation (Result.unique_rep):\n            unique_rep = {\n                \"result_type_string\": \"lume_services.results.generic:Result\",\n                \"query\": {\n                       \"inputs\": {\n                           \"input1\": 1.0,\n                           \"input2\": 2.0\n                        },\n                        \"outputs\": {\n                            \"output1\": 1.0,\n                            \"output2\": 2.0\n                        },\n                       \"flow_id\": 1\n                }\n            }\n\n            my_result = load_db_result_task(\n                result_rep = unique_rep,\n                attribute_index=[\"outputs\", \"output1\"]\n            )\n\n            print_result(my_result) # Will print 1.0\n        ```\n\n    \"\"\"  # noqa\n\n    parameters = {\n        \"attribute_index\": Parameter(\"attribute_index\"),\n        \"result_rep\": Parameter(\"result_rep\"),\n    }\n\n    def __init__(self, **kwargs):\n        \"\"\"This task is defined as a subclass of the Prefect [Task](https://docs-v1.prefect.io/api/latest/core/task.html#task-2)\n        object and accepts all Task arguments during initialization.\n\n        Args:\n            name (Optional[str]): The name of this task.\n            slug (Optional[str]):  The slug for this task. Slugs provide a stable ID\n                for tasks so that the Prefect API can identify task run states. If a\n                slug is not provided, one will be generated automatically once the\n                task is added to a Flow.\n            tags (Optional[List[str]]): A list of tags for this task.\n            max_retries (Optional[int]): The maximum amount of times this task can be\n                retried\n            retry_delay (Optional[datetime.timedelta]): The amount of time to wait\n                until task is retried\n            retry_on (Optional[Union[Exception, Iterable[Type[Exception]]]]): Exception\n                types that will allow retry behavior to occur. If not set, all\n                exceptions will allow retries. If set, retries will only occur if the\n                exception is a subtype of the exception types provided.\n            timeout (Optional[Union[int, timedelta]]): The amount of time (in seconds)\n                to wait while running this task before a timeout occurs; note that\n                sub-second resolution is not supported, even when passing in a\n                timedelta.\n            trigger (Optional[callable]):  a function that determines whether the task\n                should run, based on the states of any upstream tasks.\n            skip_on_upstream_skip (Optional[bool]): if True, if any immediately upstream\n                tasks are skipped, this task will automatically be skipped as well,\n                regardless of trigger. By default, this prevents tasks from attempting\n                to use either state or data from tasks that didn't run. If False, the\n                task's trigger will be called as normal, with skips considered\n                successes. Defaults to True.\n            cache_for (Optional[timedelta]): The amount of time to maintain a cache\n            of the outputs of this task.  Useful for situations where the containing\n            Flow will be rerun multiple times, but this task doesn't need to be.\n            cache_validator (Optional[Callable]): Validator that will determine\n                whether the cache for this task is still valid (only required if\n                `cache_for` is provided; defaults to\n                `prefect.engine.cache_validators.duration_only`)\n            cache_key (Optional[str]): if provided, a `cache_key`\n                serves as a unique identifier for this Task's cache, and can be shared\n                across both Tasks _and_ Flows; if not provided, the Task's _name_ will\n                be used if running locally, or the Task's database ID if running in\n                Cloud\n            checkpoint (Optional[bool]): if this Task is successful, whether to\n                store its result using the configured result available during the run;\n                Also note that checkpointing will only occur locally if\n                `prefect.config.flows.checkpointing` is set to `True`\n            result (Optional[Result]): the result instance used to retrieve and\n                store task results during execution\n            target (Optional[Union[str, Callable]]): location to check for task Result.\n                If a result exists at that location then the task run will enter a\n                cached state. `target` strings can be templated formatting strings\n                which will be formatted at runtime with values from `prefect.context`.\n                If a callable function is provided, it should have signature\n                `callable(**kwargs) -&gt; str` and at write time all formatting kwargs\n                will be passed and a fully formatted location is expected as the return\n                value. The callable can be used for string formatting logic that\n                `.format(**kwargs)` doesn't support.\n            state_handlers (Optional[Iterable[Callable]]): A list of state change\n                handlers that will be called whenever the task changes state,\n                providing an opportunity to inspect or modify the new state. The\n                handler will be passed the task instance, the old (prior) state,\n                and the new\n                (current) state, with the following signature:\n                    `state_handler(task: Task, old_state: State, new_state: State) -&gt;\n                    Optional[State]`\n                If multiple functions are passed, then the `new_state` argument will\n                be the result of the previous handler.\n            on_failure (Optional[Callable]): A function with signature\n                `fn(task: Task, state: State) -&gt; None` that will be called anytime this\n                Task enters a failure state\n            log_stdout (Optional[bool]): Toggle whether or not to send stdout messages\n                to the Prefect logger. Defaults to `False`.\n            task_run_name (Optional[Union[str, Callable]]): a name to set for this task\n                at runtime. `task_run_name` strings can be templated formatting strings\n                which will be formatted at runtime with values from task arguments,\n                `prefect.context`, and flow parameters (in the case of a name conflict\n                between these, earlier values take precedence). If a callable function\n                is provided, it should have signature `callable(**kwargs) -&gt; str` and\n                at write time all formatting kwargs will be passed and a fully\n                formatted location is expected as the return value. The callable can\n                be used for string formatting logic that `.format(**kwargs)` doesn't\n                support. **Note**: this only works for tasks running against a\n                backend API.\n            nout (Optional[int]): for tasks that return multiple results, the number of\n                outputs to expect. If not provided, will be inferred from the task\n                return annotation, if possible.  Note that `nout=1` implies the task\n                returns a tuple of one value (leave as `None` for non-tuple return\n                types).\n\n        \"\"\"  # noqa\n\n        # apply some defaults but allow overrides\n        log_stdout = kwargs.get(\"log_stdout\")\n        if not kwargs.get(\"log_stdout\"):\n            log_stdout = True\n        else:\n            log_stdout = kwargs.pop(\"log_stdout\")\n\n        if not kwargs.get(\"name\"):\n            name = \"load_db_result\"\n        else:\n            name = kwargs.pop(\"name\")\n\n        super().__init__(log_stdout=log_stdout, name=name, **kwargs)\n\n    def run(\n        self,\n        result_rep: dict,\n        attribute_index: Optional[list],\n        results_db_service: ResultsDB = Provide[Context.results_db_service],\n    ) -&gt; Any:\n        \"\"\"Load a result from the database using a lume_services.Result represention.\n\n        Args:\n            result_rep (Union[dict, str]): Result representation containing\n                result_type_string and query for selection. If string passed,\n                will perform json loads to get dictionary.\n            attribute_index (Optional[list]): Selection instructions from query.\n                For example, selecting the first `toyota` from a dictionary of form:\n                `{\"vehicle\": {\"car\":  [\"toyota\", \"mini\"], \"boat\": [\"sail\", \"motor\"]}}`\n                would be accomplished by passing `attribute_index=[\"car\", 0].\n            results_db_service (ResultsDB): Results database service. This is injected\n                when using the LUME-service configuration toolset.\n\n        Returns:\n            Any: Returns selection of value from result if attibute_index is passed,\n                otherwise returns Result object.\n\n        \"\"\"\n        result_type = get_result_from_string(result_rep[\"result_type_string\"])\n        result = result_type.load_from_query(\n            result_rep[\"project_name\"],\n            result_rep[\"query\"],\n            results_db_service=results_db_service,\n        )\n\n        # select first attribute\n        attr_value = getattr(result, attribute_index[0], None)\n        if attr_value is None:\n            return result\n\n        else:\n            for index in attribute_index[1:]:\n                attr_value = attr_value[index]\n\n            return attr_value\n</code></pre>"},{"location":"api/tasks/#lume_services.tasks.db.LoadDBResult.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>This task is defined as a subclass of the Prefect Task object and accepts all Task arguments during initialization.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of this task.</p> required <code>slug</code> <code>Optional[str]</code> <p>The slug for this task. Slugs provide a stable ID for tasks so that the Prefect API can identify task run states. If a slug is not provided, one will be generated automatically once the task is added to a Flow.</p> required <code>tags</code> <code>Optional[List[str]]</code> <p>A list of tags for this task.</p> required <code>max_retries</code> <code>Optional[int]</code> <p>The maximum amount of times this task can be retried</p> required <code>retry_delay</code> <code>Optional[timedelta]</code> <p>The amount of time to wait until task is retried</p> required <code>retry_on</code> <code>Optional[Union[Exception, Iterable[Type[Exception]]]]</code> <p>Exception types that will allow retry behavior to occur. If not set, all exceptions will allow retries. If set, retries will only occur if the exception is a subtype of the exception types provided.</p> required <code>timeout</code> <code>Optional[Union[int, timedelta]]</code> <p>The amount of time (in seconds) to wait while running this task before a timeout occurs; note that sub-second resolution is not supported, even when passing in a timedelta.</p> required <code>trigger</code> <code>Optional[callable]</code> <p>a function that determines whether the task should run, based on the states of any upstream tasks.</p> required <code>skip_on_upstream_skip</code> <code>Optional[bool]</code> <p>if True, if any immediately upstream tasks are skipped, this task will automatically be skipped as well, regardless of trigger. By default, this prevents tasks from attempting to use either state or data from tasks that didn't run. If False, the task's trigger will be called as normal, with skips considered successes. Defaults to True.</p> required <code>cache_for</code> <code>Optional[timedelta]</code> <p>The amount of time to maintain a cache</p> required <code>cache_validator</code> <code>Optional[Callable]</code> <p>Validator that will determine whether the cache for this task is still valid (only required if <code>cache_for</code> is provided; defaults to <code>prefect.engine.cache_validators.duration_only</code>)</p> required <code>cache_key</code> <code>Optional[str]</code> <p>if provided, a <code>cache_key</code> serves as a unique identifier for this Task's cache, and can be shared across both Tasks and Flows; if not provided, the Task's name will be used if running locally, or the Task's database ID if running in Cloud</p> required <code>checkpoint</code> <code>Optional[bool]</code> <p>if this Task is successful, whether to store its result using the configured result available during the run; Also note that checkpointing will only occur locally if <code>prefect.config.flows.checkpointing</code> is set to <code>True</code></p> required <code>result</code> <code>Optional[Result]</code> <p>the result instance used to retrieve and store task results during execution</p> required <code>target</code> <code>Optional[Union[str, Callable]]</code> <p>location to check for task Result. If a result exists at that location then the task run will enter a cached state. <code>target</code> strings can be templated formatting strings which will be formatted at runtime with values from <code>prefect.context</code>. If a callable function is provided, it should have signature <code>callable(**kwargs) -&gt; str</code> and at write time all formatting kwargs will be passed and a fully formatted location is expected as the return value. The callable can be used for string formatting logic that <code>.format(**kwargs)</code> doesn't support.</p> required <code>state_handlers</code> <code>Optional[Iterable[Callable]]</code> <p>A list of state change handlers that will be called whenever the task changes state, providing an opportunity to inspect or modify the new state. The handler will be passed the task instance, the old (prior) state, and the new (current) state, with the following signature:     <code>state_handler(task: Task, old_state: State, new_state: State) -&gt;     Optional[State]</code> If multiple functions are passed, then the <code>new_state</code> argument will be the result of the previous handler.</p> required <code>on_failure</code> <code>Optional[Callable]</code> <p>A function with signature <code>fn(task: Task, state: State) -&gt; None</code> that will be called anytime this Task enters a failure state</p> required <code>log_stdout</code> <code>Optional[bool]</code> <p>Toggle whether or not to send stdout messages to the Prefect logger. Defaults to <code>False</code>.</p> required <code>task_run_name</code> <code>Optional[Union[str, Callable]]</code> <p>a name to set for this task at runtime. <code>task_run_name</code> strings can be templated formatting strings which will be formatted at runtime with values from task arguments, <code>prefect.context</code>, and flow parameters (in the case of a name conflict between these, earlier values take precedence). If a callable function is provided, it should have signature <code>callable(**kwargs) -&gt; str</code> and at write time all formatting kwargs will be passed and a fully formatted location is expected as the return value. The callable can be used for string formatting logic that <code>.format(**kwargs)</code> doesn't support. Note: this only works for tasks running against a backend API.</p> required <code>nout</code> <code>Optional[int]</code> <p>for tasks that return multiple results, the number of outputs to expect. If not provided, will be inferred from the task return annotation, if possible.  Note that <code>nout=1</code> implies the task returns a tuple of one value (leave as <code>None</code> for non-tuple return types).</p> required Source code in <code>lume_services/tasks/db.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"This task is defined as a subclass of the Prefect [Task](https://docs-v1.prefect.io/api/latest/core/task.html#task-2)\n    object and accepts all Task arguments during initialization.\n\n    Args:\n        name (Optional[str]): The name of this task.\n        slug (Optional[str]):  The slug for this task. Slugs provide a stable ID\n            for tasks so that the Prefect API can identify task run states. If a\n            slug is not provided, one will be generated automatically once the\n            task is added to a Flow.\n        tags (Optional[List[str]]): A list of tags for this task.\n        max_retries (Optional[int]): The maximum amount of times this task can be\n            retried\n        retry_delay (Optional[datetime.timedelta]): The amount of time to wait\n            until task is retried\n        retry_on (Optional[Union[Exception, Iterable[Type[Exception]]]]): Exception\n            types that will allow retry behavior to occur. If not set, all\n            exceptions will allow retries. If set, retries will only occur if the\n            exception is a subtype of the exception types provided.\n        timeout (Optional[Union[int, timedelta]]): The amount of time (in seconds)\n            to wait while running this task before a timeout occurs; note that\n            sub-second resolution is not supported, even when passing in a\n            timedelta.\n        trigger (Optional[callable]):  a function that determines whether the task\n            should run, based on the states of any upstream tasks.\n        skip_on_upstream_skip (Optional[bool]): if True, if any immediately upstream\n            tasks are skipped, this task will automatically be skipped as well,\n            regardless of trigger. By default, this prevents tasks from attempting\n            to use either state or data from tasks that didn't run. If False, the\n            task's trigger will be called as normal, with skips considered\n            successes. Defaults to True.\n        cache_for (Optional[timedelta]): The amount of time to maintain a cache\n        of the outputs of this task.  Useful for situations where the containing\n        Flow will be rerun multiple times, but this task doesn't need to be.\n        cache_validator (Optional[Callable]): Validator that will determine\n            whether the cache for this task is still valid (only required if\n            `cache_for` is provided; defaults to\n            `prefect.engine.cache_validators.duration_only`)\n        cache_key (Optional[str]): if provided, a `cache_key`\n            serves as a unique identifier for this Task's cache, and can be shared\n            across both Tasks _and_ Flows; if not provided, the Task's _name_ will\n            be used if running locally, or the Task's database ID if running in\n            Cloud\n        checkpoint (Optional[bool]): if this Task is successful, whether to\n            store its result using the configured result available during the run;\n            Also note that checkpointing will only occur locally if\n            `prefect.config.flows.checkpointing` is set to `True`\n        result (Optional[Result]): the result instance used to retrieve and\n            store task results during execution\n        target (Optional[Union[str, Callable]]): location to check for task Result.\n            If a result exists at that location then the task run will enter a\n            cached state. `target` strings can be templated formatting strings\n            which will be formatted at runtime with values from `prefect.context`.\n            If a callable function is provided, it should have signature\n            `callable(**kwargs) -&gt; str` and at write time all formatting kwargs\n            will be passed and a fully formatted location is expected as the return\n            value. The callable can be used for string formatting logic that\n            `.format(**kwargs)` doesn't support.\n        state_handlers (Optional[Iterable[Callable]]): A list of state change\n            handlers that will be called whenever the task changes state,\n            providing an opportunity to inspect or modify the new state. The\n            handler will be passed the task instance, the old (prior) state,\n            and the new\n            (current) state, with the following signature:\n                `state_handler(task: Task, old_state: State, new_state: State) -&gt;\n                Optional[State]`\n            If multiple functions are passed, then the `new_state` argument will\n            be the result of the previous handler.\n        on_failure (Optional[Callable]): A function with signature\n            `fn(task: Task, state: State) -&gt; None` that will be called anytime this\n            Task enters a failure state\n        log_stdout (Optional[bool]): Toggle whether or not to send stdout messages\n            to the Prefect logger. Defaults to `False`.\n        task_run_name (Optional[Union[str, Callable]]): a name to set for this task\n            at runtime. `task_run_name` strings can be templated formatting strings\n            which will be formatted at runtime with values from task arguments,\n            `prefect.context`, and flow parameters (in the case of a name conflict\n            between these, earlier values take precedence). If a callable function\n            is provided, it should have signature `callable(**kwargs) -&gt; str` and\n            at write time all formatting kwargs will be passed and a fully\n            formatted location is expected as the return value. The callable can\n            be used for string formatting logic that `.format(**kwargs)` doesn't\n            support. **Note**: this only works for tasks running against a\n            backend API.\n        nout (Optional[int]): for tasks that return multiple results, the number of\n            outputs to expect. If not provided, will be inferred from the task\n            return annotation, if possible.  Note that `nout=1` implies the task\n            returns a tuple of one value (leave as `None` for non-tuple return\n            types).\n\n    \"\"\"  # noqa\n\n    # apply some defaults but allow overrides\n    log_stdout = kwargs.get(\"log_stdout\")\n    if not kwargs.get(\"log_stdout\"):\n        log_stdout = True\n    else:\n        log_stdout = kwargs.pop(\"log_stdout\")\n\n    if not kwargs.get(\"name\"):\n        name = \"load_db_result\"\n    else:\n        name = kwargs.pop(\"name\")\n\n    super().__init__(log_stdout=log_stdout, name=name, **kwargs)\n</code></pre>"},{"location":"api/tasks/#lume_services.tasks.db.LoadDBResult.run","title":"<code>run(result_rep, attribute_index, results_db_service=Provide[Context.results_db_service])</code>","text":"<p>Load a result from the database using a lume_services.Result represention.</p> <p>Parameters:</p> Name Type Description Default <code>result_rep</code> <code>Union[dict, str]</code> <p>Result representation containing result_type_string and query for selection. If string passed, will perform json loads to get dictionary.</p> required <code>attribute_index</code> <code>Optional[list]</code> <p>Selection instructions from query. For example, selecting the first <code>toyota</code> from a dictionary of form: <code>{\"vehicle\": {\"car\":  [\"toyota\", \"mini\"], \"boat\": [\"sail\", \"motor\"]}}</code> would be accomplished by passing `attribute_index=[\"car\", 0].</p> required <code>results_db_service</code> <code>ResultsDB</code> <p>Results database service. This is injected when using the LUME-service configuration toolset.</p> <code>Provide[results_db_service]</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Returns selection of value from result if attibute_index is passed, otherwise returns Result object.</p> Source code in <code>lume_services/tasks/db.py</code> <pre><code>def run(\n    self,\n    result_rep: dict,\n    attribute_index: Optional[list],\n    results_db_service: ResultsDB = Provide[Context.results_db_service],\n) -&gt; Any:\n    \"\"\"Load a result from the database using a lume_services.Result represention.\n\n    Args:\n        result_rep (Union[dict, str]): Result representation containing\n            result_type_string and query for selection. If string passed,\n            will perform json loads to get dictionary.\n        attribute_index (Optional[list]): Selection instructions from query.\n            For example, selecting the first `toyota` from a dictionary of form:\n            `{\"vehicle\": {\"car\":  [\"toyota\", \"mini\"], \"boat\": [\"sail\", \"motor\"]}}`\n            would be accomplished by passing `attribute_index=[\"car\", 0].\n        results_db_service (ResultsDB): Results database service. This is injected\n            when using the LUME-service configuration toolset.\n\n    Returns:\n        Any: Returns selection of value from result if attibute_index is passed,\n            otherwise returns Result object.\n\n    \"\"\"\n    result_type = get_result_from_string(result_rep[\"result_type_string\"])\n    result = result_type.load_from_query(\n        result_rep[\"project_name\"],\n        result_rep[\"query\"],\n        results_db_service=results_db_service,\n    )\n\n    # select first attribute\n    attr_value = getattr(result, attribute_index[0], None)\n    if attr_value is None:\n        return result\n\n    else:\n        for index in attribute_index[1:]:\n            attr_value = attr_value[index]\n\n        return attr_value\n</code></pre>"},{"location":"api/tasks/#lume_services.tasks.db.SaveDBResult","title":"<code>SaveDBResult</code>","text":"<p>               Bases: <code>Task</code></p> <p>Save a result from the results database. All database results generate a minimally representative identifier that can be used to query the database and load the result. This idenifier is jsonable and therefore accessable outside of the workflow's scope. This task uses either a passed or injected results database service to save the unique representation of the result to the database. Custom result sublasses may impose additional uniqueness constraints. In order to use this task with the backend, your flow must be registered with the backend as the result's <code>flow_id</code> is inferred from the Prefect Context. Alternatively, for development purposes, <code>flow_id</code> can be passed directly.</p> <p>This task is defined as a subclass of the Prefect Task object and accepts all Task arguments during initialization.</p> <p>Examples:</p> <pre><code>from prefect import Flow, task\nfrom lume_services.results import Result\nfrom lume_services.tasks import configure_lume_services, SaveDBResult\n\n# construct_task\nsave_db_result_task = SaveDBResult(timeout=20)\n\n@task\ndef format_result_entry():\n    inputs = {\n        \"input1\": 1.0,\n        \"input2\": 2.0,\n    }\n\n    outputs = {\n        \"output1\" : 1.0,\n        \"output2\": 2.0\n    }\n\n    return Result(\n        inputs=inputs,\n        outputs=outputs,\n        flow_id=\"local-test\"\n    )\n\nwith Flow(\n    \"my_flow\"\n) as flow:\n    # must first configure services because using injected results\n    # database service\n    configure_lume_services()\n\n    result = format_result_entry()\n\n    my_result = save_db_result_task(\n        result\n    )\n</code></pre> Source code in <code>lume_services/tasks/db.py</code> <pre><code>class SaveDBResult(Task):\n    \"\"\"Save a result from the results database. All database results generate a\n    [minimally representative identifier][lume_services.results.generic.Result] that can\n    be used to query the database and load the result. This idenifier is jsonable and\n    therefore accessable outside of the workflow's scope. This task uses either a\n    passed or injected results database service to save the unique representation\n    of the result to the database. Custom result sublasses may impose additional\n    uniqueness constraints. In order to use this task with the backend, your\n    flow must be registered with the backend as the result's `flow_id` is inferred from\n    the Prefect Context. Alternatively, for development purposes, `flow_id` can be\n    passed directly.\n\n    This task is defined as a subclass of the Prefect [Task](https://docs-v1.prefect.io/api/latest/core/task.html#task-2)\n    object and accepts all Task arguments during initialization.\n\n    Examples:\n        ```python\n        from prefect import Flow, task\n        from lume_services.results import Result\n        from lume_services.tasks import configure_lume_services, SaveDBResult\n\n        # construct_task\n        save_db_result_task = SaveDBResult(timeout=20)\n\n        @task\n        def format_result_entry():\n            inputs = {\n                \"input1\": 1.0,\n                \"input2\": 2.0,\n            }\n\n            outputs = {\n                \"output1\" : 1.0,\n                \"output2\": 2.0\n            }\n\n            return Result(\n                inputs=inputs,\n                outputs=outputs,\n                flow_id=\"local-test\"\n            )\n\n        with Flow(\n            \"my_flow\"\n        ) as flow:\n            # must first configure services because using injected results\n            # database service\n            configure_lume_services()\n\n            result = format_result_entry()\n\n            my_result = save_db_result_task(\n                result\n            )\n\n        ```\n\n    \"\"\"  # noqa\n\n    def __init__(self, **kwargs):\n        \"\"\"This task is defined as a subclass of the Prefect [Task](https://docs-v1.prefect.io/api/latest/core/task.html#task-2)\n        object and accepts all Task arguments during initialization.\n\n        Args:\n            name (Optional[str]): The name of this task.\n            slug (Optional[str]):  The slug for this task. Slugs provide a stable ID\n                for tasks so that the Prefect API can identify task run states. If a\n                slug is not provided, one will be generated automatically once the\n                task is added to a Flow.\n            tags (Optional[List[str]]): A list of tags for this task.\n            max_retries (Optional[int]): The maximum amount of times this task can be\n                retried\n            retry_delay (Optional[datetime.timedelta]): The amount of time to wait\n                until task is retried\n            retry_on (Optional[Union[Exception, Iterable[Type[Exception]]]]): Exception\n                types that will allow retry behavior to occur. If not set, all\n                exceptions will allow retries. If set, retries will only occur if the\n                exception is a subtype of the exception types provided.\n            timeout (Optional[Union[int, timedelta]]): The amount of time (in seconds)\n                to wait while running this task before a timeout occurs; note that\n                sub-second resolution is not supported, even when passing in a\n                timedelta.\n            trigger (Optional[callable]):  a function that determines whether the task\n                should run, based on the states of any upstream tasks.\n            skip_on_upstream_skip (Optional[bool]): if True, if any immediately upstream\n                tasks are skipped, this task will automatically be skipped as well,\n                regardless of trigger. By default, this prevents tasks from attempting\n                to use either state or data from tasks that didn't run. If False, the\n                task's trigger will be called as normal, with skips considered\n                successes. Defaults to True.\n            cache_for (Optional[timedelta]): The amount of time to maintain a cache\n            of the outputs of this task.  Useful for situations where the containing\n            Flow will be rerun multiple times, but this task doesn't need to be.\n            cache_validator (Optional[Callable]): Validator that will determine\n                whether the cache for this task is still valid (only required if\n                `cache_for` is provided; defaults to\n                `prefect.engine.cache_validators.duration_only`)\n            cache_key (Optional[str]): if provided, a `cache_key`\n                serves as a unique identifier for this Task's cache, and can be shared\n                across both Tasks _and_ Flows; if not provided, the Task's _name_ will\n                be used if running locally, or the Task's database ID if running in\n                Cloud\n            checkpoint (Optional[bool]): if this Task is successful, whether to\n                store its result using the configured result available during the run;\n                Also note that checkpointing will only occur locally if\n                `prefect.config.flows.checkpointing` is set to `True`\n            result (Optional[Result]): the result instance used to retrieve and\n                store task results during execution\n            target (Optional[Union[str, Callable]]): location to check for task Result.\n                If a result exists at that location then the task run will enter a\n                cached state. `target` strings can be templated formatting strings\n                which will be formatted at runtime with values from `prefect.context`.\n                If a callable function is provided, it should have signature\n                `callable(**kwargs) -&gt; str` and at write time all formatting kwargs\n                will be passed and a fully formatted location is expected as the return\n                value. The callable can be used for string formatting logic that\n                `.format(**kwargs)` doesn't support.\n            state_handlers (Optional[Iterable[Callable]]): A list of state change\n                handlers that will be called whenever the task changes state,\n                providing an opportunity to inspect or modify the new state. The\n                handler will be passed the task instance, the old (prior) state,\n                and the new\n                (current) state, with the following signature:\n                    `state_handler(task: Task, old_state: State, new_state: State) -&gt;\n                    Optional[State]`\n                If multiple functions are passed, then the `new_state` argument will\n                be the result of the previous handler.\n            on_failure (Optional[Callable]): A function with signature\n                `fn(task: Task, state: State) -&gt; None` that will be called anytime this\n                Task enters a failure state\n            log_stdout (Optional[bool]): Toggle whether or not to send stdout messages\n                to the Prefect logger. Defaults to `False`.\n            task_run_name (Optional[Union[str, Callable]]): a name to set for this task\n                at runtime. `task_run_name` strings can be templated formatting strings\n                which will be formatted at runtime with values from task arguments,\n                `prefect.context`, and flow parameters (in the case of a name conflict\n                between these, earlier values take precedence). If a callable function\n                is provided, it should have signature `callable(**kwargs) -&gt; str` and\n                at write time all formatting kwargs will be passed and a fully\n                formatted location is expected as the return value. The callable can\n                be used for string formatting logic that `.format(**kwargs)` doesn't\n                support. **Note**: this only works for tasks running against a\n                backend API.\n            nout (Optional[int]): for tasks that return multiple results, the number of\n                outputs to expect. If not provided, will be inferred from the task\n                return annotation, if possible.  Note that `nout=1` implies the task\n                returns a tuple of one value (leave as `None` for non-tuple return\n                types).\n\n        \"\"\"  # noqa\n\n        # apply some defaults but allow overrides\n        log_stdout = kwargs.get(\"log_stdout\")\n        if not kwargs.get(\"log_stdout\"):\n            log_stdout = True\n        else:\n            log_stdout = kwargs.pop(\"log_stdout\")\n\n        if not kwargs.get(\"name\"):\n            name = \"save_db_result\"\n        else:\n            name = kwargs.pop(\"name\")\n\n        if not kwargs.get(\"result\"):\n            result = PrefectResult(location=_unique_db_location)\n        else:\n            result = kwargs.pop(\"result\")\n\n        super().__init__(log_stdout=log_stdout, name=name, result=result, **kwargs)\n\n    @inject\n    def run(\n        self,\n        result,\n        results_db_service: ResultsDB = Provide[Context.results_db_service],\n    ) -&gt; dict:\n        \"\"\"Insert result into the results database service. Creates a PrefectResult that\n        contains minimal representative information for reconstruction.\n\n        Args:\n            result (Result): Result object to save\n            results_db_service (ResultsDB): Results database service\n\n\n        Returns:\n            dict: Unique representation for collecting results.\n\n        \"\"\"\n\n        result.insert(results_db_service=results_db_service)\n        return result.unique_rep()\n</code></pre>"},{"location":"api/tasks/#lume_services.tasks.db.SaveDBResult.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>This task is defined as a subclass of the Prefect Task object and accepts all Task arguments during initialization.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of this task.</p> required <code>slug</code> <code>Optional[str]</code> <p>The slug for this task. Slugs provide a stable ID for tasks so that the Prefect API can identify task run states. If a slug is not provided, one will be generated automatically once the task is added to a Flow.</p> required <code>tags</code> <code>Optional[List[str]]</code> <p>A list of tags for this task.</p> required <code>max_retries</code> <code>Optional[int]</code> <p>The maximum amount of times this task can be retried</p> required <code>retry_delay</code> <code>Optional[timedelta]</code> <p>The amount of time to wait until task is retried</p> required <code>retry_on</code> <code>Optional[Union[Exception, Iterable[Type[Exception]]]]</code> <p>Exception types that will allow retry behavior to occur. If not set, all exceptions will allow retries. If set, retries will only occur if the exception is a subtype of the exception types provided.</p> required <code>timeout</code> <code>Optional[Union[int, timedelta]]</code> <p>The amount of time (in seconds) to wait while running this task before a timeout occurs; note that sub-second resolution is not supported, even when passing in a timedelta.</p> required <code>trigger</code> <code>Optional[callable]</code> <p>a function that determines whether the task should run, based on the states of any upstream tasks.</p> required <code>skip_on_upstream_skip</code> <code>Optional[bool]</code> <p>if True, if any immediately upstream tasks are skipped, this task will automatically be skipped as well, regardless of trigger. By default, this prevents tasks from attempting to use either state or data from tasks that didn't run. If False, the task's trigger will be called as normal, with skips considered successes. Defaults to True.</p> required <code>cache_for</code> <code>Optional[timedelta]</code> <p>The amount of time to maintain a cache</p> required <code>cache_validator</code> <code>Optional[Callable]</code> <p>Validator that will determine whether the cache for this task is still valid (only required if <code>cache_for</code> is provided; defaults to <code>prefect.engine.cache_validators.duration_only</code>)</p> required <code>cache_key</code> <code>Optional[str]</code> <p>if provided, a <code>cache_key</code> serves as a unique identifier for this Task's cache, and can be shared across both Tasks and Flows; if not provided, the Task's name will be used if running locally, or the Task's database ID if running in Cloud</p> required <code>checkpoint</code> <code>Optional[bool]</code> <p>if this Task is successful, whether to store its result using the configured result available during the run; Also note that checkpointing will only occur locally if <code>prefect.config.flows.checkpointing</code> is set to <code>True</code></p> required <code>result</code> <code>Optional[Result]</code> <p>the result instance used to retrieve and store task results during execution</p> required <code>target</code> <code>Optional[Union[str, Callable]]</code> <p>location to check for task Result. If a result exists at that location then the task run will enter a cached state. <code>target</code> strings can be templated formatting strings which will be formatted at runtime with values from <code>prefect.context</code>. If a callable function is provided, it should have signature <code>callable(**kwargs) -&gt; str</code> and at write time all formatting kwargs will be passed and a fully formatted location is expected as the return value. The callable can be used for string formatting logic that <code>.format(**kwargs)</code> doesn't support.</p> required <code>state_handlers</code> <code>Optional[Iterable[Callable]]</code> <p>A list of state change handlers that will be called whenever the task changes state, providing an opportunity to inspect or modify the new state. The handler will be passed the task instance, the old (prior) state, and the new (current) state, with the following signature:     <code>state_handler(task: Task, old_state: State, new_state: State) -&gt;     Optional[State]</code> If multiple functions are passed, then the <code>new_state</code> argument will be the result of the previous handler.</p> required <code>on_failure</code> <code>Optional[Callable]</code> <p>A function with signature <code>fn(task: Task, state: State) -&gt; None</code> that will be called anytime this Task enters a failure state</p> required <code>log_stdout</code> <code>Optional[bool]</code> <p>Toggle whether or not to send stdout messages to the Prefect logger. Defaults to <code>False</code>.</p> required <code>task_run_name</code> <code>Optional[Union[str, Callable]]</code> <p>a name to set for this task at runtime. <code>task_run_name</code> strings can be templated formatting strings which will be formatted at runtime with values from task arguments, <code>prefect.context</code>, and flow parameters (in the case of a name conflict between these, earlier values take precedence). If a callable function is provided, it should have signature <code>callable(**kwargs) -&gt; str</code> and at write time all formatting kwargs will be passed and a fully formatted location is expected as the return value. The callable can be used for string formatting logic that <code>.format(**kwargs)</code> doesn't support. Note: this only works for tasks running against a backend API.</p> required <code>nout</code> <code>Optional[int]</code> <p>for tasks that return multiple results, the number of outputs to expect. If not provided, will be inferred from the task return annotation, if possible.  Note that <code>nout=1</code> implies the task returns a tuple of one value (leave as <code>None</code> for non-tuple return types).</p> required Source code in <code>lume_services/tasks/db.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"This task is defined as a subclass of the Prefect [Task](https://docs-v1.prefect.io/api/latest/core/task.html#task-2)\n    object and accepts all Task arguments during initialization.\n\n    Args:\n        name (Optional[str]): The name of this task.\n        slug (Optional[str]):  The slug for this task. Slugs provide a stable ID\n            for tasks so that the Prefect API can identify task run states. If a\n            slug is not provided, one will be generated automatically once the\n            task is added to a Flow.\n        tags (Optional[List[str]]): A list of tags for this task.\n        max_retries (Optional[int]): The maximum amount of times this task can be\n            retried\n        retry_delay (Optional[datetime.timedelta]): The amount of time to wait\n            until task is retried\n        retry_on (Optional[Union[Exception, Iterable[Type[Exception]]]]): Exception\n            types that will allow retry behavior to occur. If not set, all\n            exceptions will allow retries. If set, retries will only occur if the\n            exception is a subtype of the exception types provided.\n        timeout (Optional[Union[int, timedelta]]): The amount of time (in seconds)\n            to wait while running this task before a timeout occurs; note that\n            sub-second resolution is not supported, even when passing in a\n            timedelta.\n        trigger (Optional[callable]):  a function that determines whether the task\n            should run, based on the states of any upstream tasks.\n        skip_on_upstream_skip (Optional[bool]): if True, if any immediately upstream\n            tasks are skipped, this task will automatically be skipped as well,\n            regardless of trigger. By default, this prevents tasks from attempting\n            to use either state or data from tasks that didn't run. If False, the\n            task's trigger will be called as normal, with skips considered\n            successes. Defaults to True.\n        cache_for (Optional[timedelta]): The amount of time to maintain a cache\n        of the outputs of this task.  Useful for situations where the containing\n        Flow will be rerun multiple times, but this task doesn't need to be.\n        cache_validator (Optional[Callable]): Validator that will determine\n            whether the cache for this task is still valid (only required if\n            `cache_for` is provided; defaults to\n            `prefect.engine.cache_validators.duration_only`)\n        cache_key (Optional[str]): if provided, a `cache_key`\n            serves as a unique identifier for this Task's cache, and can be shared\n            across both Tasks _and_ Flows; if not provided, the Task's _name_ will\n            be used if running locally, or the Task's database ID if running in\n            Cloud\n        checkpoint (Optional[bool]): if this Task is successful, whether to\n            store its result using the configured result available during the run;\n            Also note that checkpointing will only occur locally if\n            `prefect.config.flows.checkpointing` is set to `True`\n        result (Optional[Result]): the result instance used to retrieve and\n            store task results during execution\n        target (Optional[Union[str, Callable]]): location to check for task Result.\n            If a result exists at that location then the task run will enter a\n            cached state. `target` strings can be templated formatting strings\n            which will be formatted at runtime with values from `prefect.context`.\n            If a callable function is provided, it should have signature\n            `callable(**kwargs) -&gt; str` and at write time all formatting kwargs\n            will be passed and a fully formatted location is expected as the return\n            value. The callable can be used for string formatting logic that\n            `.format(**kwargs)` doesn't support.\n        state_handlers (Optional[Iterable[Callable]]): A list of state change\n            handlers that will be called whenever the task changes state,\n            providing an opportunity to inspect or modify the new state. The\n            handler will be passed the task instance, the old (prior) state,\n            and the new\n            (current) state, with the following signature:\n                `state_handler(task: Task, old_state: State, new_state: State) -&gt;\n                Optional[State]`\n            If multiple functions are passed, then the `new_state` argument will\n            be the result of the previous handler.\n        on_failure (Optional[Callable]): A function with signature\n            `fn(task: Task, state: State) -&gt; None` that will be called anytime this\n            Task enters a failure state\n        log_stdout (Optional[bool]): Toggle whether or not to send stdout messages\n            to the Prefect logger. Defaults to `False`.\n        task_run_name (Optional[Union[str, Callable]]): a name to set for this task\n            at runtime. `task_run_name` strings can be templated formatting strings\n            which will be formatted at runtime with values from task arguments,\n            `prefect.context`, and flow parameters (in the case of a name conflict\n            between these, earlier values take precedence). If a callable function\n            is provided, it should have signature `callable(**kwargs) -&gt; str` and\n            at write time all formatting kwargs will be passed and a fully\n            formatted location is expected as the return value. The callable can\n            be used for string formatting logic that `.format(**kwargs)` doesn't\n            support. **Note**: this only works for tasks running against a\n            backend API.\n        nout (Optional[int]): for tasks that return multiple results, the number of\n            outputs to expect. If not provided, will be inferred from the task\n            return annotation, if possible.  Note that `nout=1` implies the task\n            returns a tuple of one value (leave as `None` for non-tuple return\n            types).\n\n    \"\"\"  # noqa\n\n    # apply some defaults but allow overrides\n    log_stdout = kwargs.get(\"log_stdout\")\n    if not kwargs.get(\"log_stdout\"):\n        log_stdout = True\n    else:\n        log_stdout = kwargs.pop(\"log_stdout\")\n\n    if not kwargs.get(\"name\"):\n        name = \"save_db_result\"\n    else:\n        name = kwargs.pop(\"name\")\n\n    if not kwargs.get(\"result\"):\n        result = PrefectResult(location=_unique_db_location)\n    else:\n        result = kwargs.pop(\"result\")\n\n    super().__init__(log_stdout=log_stdout, name=name, result=result, **kwargs)\n</code></pre>"},{"location":"api/tasks/#lume_services.tasks.db.SaveDBResult.run","title":"<code>run(result, results_db_service=Provide[Context.results_db_service])</code>","text":"<p>Insert result into the results database service. Creates a PrefectResult that contains minimal representative information for reconstruction.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>Result</code> <p>Result object to save</p> required <code>results_db_service</code> <code>ResultsDB</code> <p>Results database service</p> <code>Provide[results_db_service]</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Unique representation for collecting results.</p> Source code in <code>lume_services/tasks/db.py</code> <pre><code>@inject\ndef run(\n    self,\n    result,\n    results_db_service: ResultsDB = Provide[Context.results_db_service],\n) -&gt; dict:\n    \"\"\"Insert result into the results database service. Creates a PrefectResult that\n    contains minimal representative information for reconstruction.\n\n    Args:\n        result (Result): Result object to save\n        results_db_service (ResultsDB): Results database service\n\n\n    Returns:\n        dict: Unique representation for collecting results.\n\n    \"\"\"\n\n    result.insert(results_db_service=results_db_service)\n    return result.unique_rep()\n</code></pre>"},{"location":"api/tasks/#lume_services.tasks.file.LoadFile","title":"<code>LoadFile</code>","text":"<p>               Bases: <code>Task</code></p> Source code in <code>lume_services/tasks/file.py</code> <pre><code>class LoadFile(Task):\n    parameters = {\n        \"file_rep\": Parameter(\"file_rep\"),\n    }\n\n    def __init__(self, **kwargs):\n        \"\"\"This task is defined as a subclass of the Prefect [Task](https://docs-v1.prefect.io/api/latest/core/task.html#task-2)\n        object and accepts all Task arguments during initialization.\n\n        Args:\n            name (Optional[str]): The name of this task.\n            slug (Optional[str]):  The slug for this task. Slugs provide a stable ID\n                for tasks so that the Prefect API can identify task run states. If a\n                slug is not provided, one will be generated automatically once the\n                task is added to a Flow.\n            tags (Optional[List[str]]): A list of tags for this task.\n            max_retries (Optional[int]): The maximum amount of times this task can be\n                retried\n            retry_delay (Optional[datetime.timedelta]): The amount of time to wait\n                until task is retried\n            retry_on (Optional[Union[Exception, Iterable[Type[Exception]]]]): Exception\n                types that will allow retry behavior to occur. If not set, all\n                exceptions will allow retries. If set, retries will only occur if the\n                exception is a subtype of the exception types provided.\n            timeout (Optional[Union[int, timedelta]]): The amount of time (in seconds)\n                to wait while running this task before a timeout occurs; note that\n                sub-second resolution is not supported, even when passing in a\n                timedelta.\n            trigger (Optional[callable]):  a function that determines whether the task\n                should run, based on the states of any upstream tasks.\n            skip_on_upstream_skip (Optional[bool]): if True, if any immediately upstream\n                tasks are skipped, this task will automatically be skipped as well,\n                regardless of trigger. By default, this prevents tasks from attempting\n                to use either state or data from tasks that didn't run. If False, the\n                task's trigger will be called as normal, with skips considered\n                successes. Defaults to True.\n            cache_for (Optional[timedelta]): The amount of time to maintain a cache\n            of the outputs of this task.  Useful for situations where the containing\n            Flow will be rerun multiple times, but this task doesn't need to be.\n            cache_validator (Optional[Callable]): Validator that will determine\n                whether the cache for this task is still valid (only required if\n                `cache_for` is provided; defaults to\n                `prefect.engine.cache_validators.duration_only`)\n            cache_key (Optional[str]): if provided, a `cache_key`\n                serves as a unique identifier for this Task's cache, and can be shared\n                across both Tasks _and_ Flows; if not provided, the Task's _name_ will\n                be used if running locally, or the Task's database ID if running in\n                Cloud\n            checkpoint (Optional[bool]): if this Task is successful, whether to\n                store its result using the configured result available during the run;\n                Also note that checkpointing will only occur locally if\n                `prefect.config.flows.checkpointing` is set to `True`\n            result (Optional[Result]): the result instance used to retrieve and\n                store task results during execution\n            target (Optional[Union[str, Callable]]): location to check for task Result.\n                If a result exists at that location then the task run will enter a\n                cached state. `target` strings can be templated formatting strings\n                which will be formatted at runtime with values from `prefect.context`.\n                If a callable function is provided, it should have signature\n                `callable(**kwargs) -&gt; str` and at write time all formatting kwargs\n                will be passed and a fully formatted location is expected as the return\n                value. The callable can be used for string formatting logic that\n                `.format(**kwargs)` doesn't support.\n            state_handlers (Optional[Iterable[Callable]]): A list of state change\n                handlers that will be called whenever the task changes state,\n                providing an opportunity to inspect or modify the new state. The\n                handler will be passed the task instance, the old (prior) state,\n                and the new\n                (current) state, with the following signature:\n                    `state_handler(task: Task, old_state: State, new_state: State) -&gt;\n                    Optional[State]`\n                If multiple functions are passed, then the `new_state` argument will\n                be the result of the previous handler.\n            on_failure (Optional[Callable]): A function with signature\n                `fn(task: Task, state: State) -&gt; None` that will be called anytime this\n                Task enters a failure state\n            log_stdout (Optional[bool]): Toggle whether or not to send stdout messages\n                to the Prefect logger. Defaults to `False`.\n            task_run_name (Optional[Union[str, Callable]]): a name to set for this task\n                at runtime. `task_run_name` strings can be templated formatting strings\n                which will be formatted at runtime with values from task arguments,\n                `prefect.context`, and flow parameters (in the case of a name conflict\n                between these, earlier values take precedence). If a callable function\n                is provided, it should have signature `callable(**kwargs) -&gt; str` and\n                at write time all formatting kwargs will be passed and a fully\n                formatted location is expected as the return value. The callable can\n                be used for string formatting logic that `.format(**kwargs)` doesn't\n                support. **Note**: this only works for tasks running against a\n                backend API.\n            nout (Optional[int]): for tasks that return multiple results, the number of\n                outputs to expect. If not provided, will be inferred from the task\n                return annotation, if possible.  Note that `nout=1` implies the task\n                returns a tuple of one value (leave as `None` for non-tuple return\n                types).\n\n        \"\"\"  # noqa\n\n        # apply some defaults but allow overrides\n        log_stdout = kwargs.get(\"log_stdout\")\n        if not kwargs.get(\"log_stdout\"):\n            log_stdout = True\n        else:\n            log_stdout = kwargs.pop(\"log_stdout\")\n\n        if not kwargs.get(\"name\"):\n            name = \"load_file\"\n        else:\n            name = kwargs.pop(\"name\")\n\n        super().__init__(log_stdout=log_stdout, name=name, **kwargs)\n\n    @inject\n    def run(\n        self, file_rep: dict, file_service: FileService = Provide[Context.file_service]\n    ) -&gt; Any:\n        \"\"\"Load a file\n\n        Args:\n            file_rep (dict): File data representation\n            file_service (FileService): File service for interacting w/ filesystems\n\n        Returns:\n            Any: Unserialize file object\n\n        \"\"\"\n\n        file_type = get_file_from_serializer_string(file_rep[\"file_type_string\"])\n        file_result = file_type(**file_rep)\n\n        return file_result.read(file_service=file_service)\n</code></pre>"},{"location":"api/tasks/#lume_services.tasks.file.LoadFile.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>This task is defined as a subclass of the Prefect Task object and accepts all Task arguments during initialization.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of this task.</p> required <code>slug</code> <code>Optional[str]</code> <p>The slug for this task. Slugs provide a stable ID for tasks so that the Prefect API can identify task run states. If a slug is not provided, one will be generated automatically once the task is added to a Flow.</p> required <code>tags</code> <code>Optional[List[str]]</code> <p>A list of tags for this task.</p> required <code>max_retries</code> <code>Optional[int]</code> <p>The maximum amount of times this task can be retried</p> required <code>retry_delay</code> <code>Optional[timedelta]</code> <p>The amount of time to wait until task is retried</p> required <code>retry_on</code> <code>Optional[Union[Exception, Iterable[Type[Exception]]]]</code> <p>Exception types that will allow retry behavior to occur. If not set, all exceptions will allow retries. If set, retries will only occur if the exception is a subtype of the exception types provided.</p> required <code>timeout</code> <code>Optional[Union[int, timedelta]]</code> <p>The amount of time (in seconds) to wait while running this task before a timeout occurs; note that sub-second resolution is not supported, even when passing in a timedelta.</p> required <code>trigger</code> <code>Optional[callable]</code> <p>a function that determines whether the task should run, based on the states of any upstream tasks.</p> required <code>skip_on_upstream_skip</code> <code>Optional[bool]</code> <p>if True, if any immediately upstream tasks are skipped, this task will automatically be skipped as well, regardless of trigger. By default, this prevents tasks from attempting to use either state or data from tasks that didn't run. If False, the task's trigger will be called as normal, with skips considered successes. Defaults to True.</p> required <code>cache_for</code> <code>Optional[timedelta]</code> <p>The amount of time to maintain a cache</p> required <code>cache_validator</code> <code>Optional[Callable]</code> <p>Validator that will determine whether the cache for this task is still valid (only required if <code>cache_for</code> is provided; defaults to <code>prefect.engine.cache_validators.duration_only</code>)</p> required <code>cache_key</code> <code>Optional[str]</code> <p>if provided, a <code>cache_key</code> serves as a unique identifier for this Task's cache, and can be shared across both Tasks and Flows; if not provided, the Task's name will be used if running locally, or the Task's database ID if running in Cloud</p> required <code>checkpoint</code> <code>Optional[bool]</code> <p>if this Task is successful, whether to store its result using the configured result available during the run; Also note that checkpointing will only occur locally if <code>prefect.config.flows.checkpointing</code> is set to <code>True</code></p> required <code>result</code> <code>Optional[Result]</code> <p>the result instance used to retrieve and store task results during execution</p> required <code>target</code> <code>Optional[Union[str, Callable]]</code> <p>location to check for task Result. If a result exists at that location then the task run will enter a cached state. <code>target</code> strings can be templated formatting strings which will be formatted at runtime with values from <code>prefect.context</code>. If a callable function is provided, it should have signature <code>callable(**kwargs) -&gt; str</code> and at write time all formatting kwargs will be passed and a fully formatted location is expected as the return value. The callable can be used for string formatting logic that <code>.format(**kwargs)</code> doesn't support.</p> required <code>state_handlers</code> <code>Optional[Iterable[Callable]]</code> <p>A list of state change handlers that will be called whenever the task changes state, providing an opportunity to inspect or modify the new state. The handler will be passed the task instance, the old (prior) state, and the new (current) state, with the following signature:     <code>state_handler(task: Task, old_state: State, new_state: State) -&gt;     Optional[State]</code> If multiple functions are passed, then the <code>new_state</code> argument will be the result of the previous handler.</p> required <code>on_failure</code> <code>Optional[Callable]</code> <p>A function with signature <code>fn(task: Task, state: State) -&gt; None</code> that will be called anytime this Task enters a failure state</p> required <code>log_stdout</code> <code>Optional[bool]</code> <p>Toggle whether or not to send stdout messages to the Prefect logger. Defaults to <code>False</code>.</p> required <code>task_run_name</code> <code>Optional[Union[str, Callable]]</code> <p>a name to set for this task at runtime. <code>task_run_name</code> strings can be templated formatting strings which will be formatted at runtime with values from task arguments, <code>prefect.context</code>, and flow parameters (in the case of a name conflict between these, earlier values take precedence). If a callable function is provided, it should have signature <code>callable(**kwargs) -&gt; str</code> and at write time all formatting kwargs will be passed and a fully formatted location is expected as the return value. The callable can be used for string formatting logic that <code>.format(**kwargs)</code> doesn't support. Note: this only works for tasks running against a backend API.</p> required <code>nout</code> <code>Optional[int]</code> <p>for tasks that return multiple results, the number of outputs to expect. If not provided, will be inferred from the task return annotation, if possible.  Note that <code>nout=1</code> implies the task returns a tuple of one value (leave as <code>None</code> for non-tuple return types).</p> required Source code in <code>lume_services/tasks/file.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"This task is defined as a subclass of the Prefect [Task](https://docs-v1.prefect.io/api/latest/core/task.html#task-2)\n    object and accepts all Task arguments during initialization.\n\n    Args:\n        name (Optional[str]): The name of this task.\n        slug (Optional[str]):  The slug for this task. Slugs provide a stable ID\n            for tasks so that the Prefect API can identify task run states. If a\n            slug is not provided, one will be generated automatically once the\n            task is added to a Flow.\n        tags (Optional[List[str]]): A list of tags for this task.\n        max_retries (Optional[int]): The maximum amount of times this task can be\n            retried\n        retry_delay (Optional[datetime.timedelta]): The amount of time to wait\n            until task is retried\n        retry_on (Optional[Union[Exception, Iterable[Type[Exception]]]]): Exception\n            types that will allow retry behavior to occur. If not set, all\n            exceptions will allow retries. If set, retries will only occur if the\n            exception is a subtype of the exception types provided.\n        timeout (Optional[Union[int, timedelta]]): The amount of time (in seconds)\n            to wait while running this task before a timeout occurs; note that\n            sub-second resolution is not supported, even when passing in a\n            timedelta.\n        trigger (Optional[callable]):  a function that determines whether the task\n            should run, based on the states of any upstream tasks.\n        skip_on_upstream_skip (Optional[bool]): if True, if any immediately upstream\n            tasks are skipped, this task will automatically be skipped as well,\n            regardless of trigger. By default, this prevents tasks from attempting\n            to use either state or data from tasks that didn't run. If False, the\n            task's trigger will be called as normal, with skips considered\n            successes. Defaults to True.\n        cache_for (Optional[timedelta]): The amount of time to maintain a cache\n        of the outputs of this task.  Useful for situations where the containing\n        Flow will be rerun multiple times, but this task doesn't need to be.\n        cache_validator (Optional[Callable]): Validator that will determine\n            whether the cache for this task is still valid (only required if\n            `cache_for` is provided; defaults to\n            `prefect.engine.cache_validators.duration_only`)\n        cache_key (Optional[str]): if provided, a `cache_key`\n            serves as a unique identifier for this Task's cache, and can be shared\n            across both Tasks _and_ Flows; if not provided, the Task's _name_ will\n            be used if running locally, or the Task's database ID if running in\n            Cloud\n        checkpoint (Optional[bool]): if this Task is successful, whether to\n            store its result using the configured result available during the run;\n            Also note that checkpointing will only occur locally if\n            `prefect.config.flows.checkpointing` is set to `True`\n        result (Optional[Result]): the result instance used to retrieve and\n            store task results during execution\n        target (Optional[Union[str, Callable]]): location to check for task Result.\n            If a result exists at that location then the task run will enter a\n            cached state. `target` strings can be templated formatting strings\n            which will be formatted at runtime with values from `prefect.context`.\n            If a callable function is provided, it should have signature\n            `callable(**kwargs) -&gt; str` and at write time all formatting kwargs\n            will be passed and a fully formatted location is expected as the return\n            value. The callable can be used for string formatting logic that\n            `.format(**kwargs)` doesn't support.\n        state_handlers (Optional[Iterable[Callable]]): A list of state change\n            handlers that will be called whenever the task changes state,\n            providing an opportunity to inspect or modify the new state. The\n            handler will be passed the task instance, the old (prior) state,\n            and the new\n            (current) state, with the following signature:\n                `state_handler(task: Task, old_state: State, new_state: State) -&gt;\n                Optional[State]`\n            If multiple functions are passed, then the `new_state` argument will\n            be the result of the previous handler.\n        on_failure (Optional[Callable]): A function with signature\n            `fn(task: Task, state: State) -&gt; None` that will be called anytime this\n            Task enters a failure state\n        log_stdout (Optional[bool]): Toggle whether or not to send stdout messages\n            to the Prefect logger. Defaults to `False`.\n        task_run_name (Optional[Union[str, Callable]]): a name to set for this task\n            at runtime. `task_run_name` strings can be templated formatting strings\n            which will be formatted at runtime with values from task arguments,\n            `prefect.context`, and flow parameters (in the case of a name conflict\n            between these, earlier values take precedence). If a callable function\n            is provided, it should have signature `callable(**kwargs) -&gt; str` and\n            at write time all formatting kwargs will be passed and a fully\n            formatted location is expected as the return value. The callable can\n            be used for string formatting logic that `.format(**kwargs)` doesn't\n            support. **Note**: this only works for tasks running against a\n            backend API.\n        nout (Optional[int]): for tasks that return multiple results, the number of\n            outputs to expect. If not provided, will be inferred from the task\n            return annotation, if possible.  Note that `nout=1` implies the task\n            returns a tuple of one value (leave as `None` for non-tuple return\n            types).\n\n    \"\"\"  # noqa\n\n    # apply some defaults but allow overrides\n    log_stdout = kwargs.get(\"log_stdout\")\n    if not kwargs.get(\"log_stdout\"):\n        log_stdout = True\n    else:\n        log_stdout = kwargs.pop(\"log_stdout\")\n\n    if not kwargs.get(\"name\"):\n        name = \"load_file\"\n    else:\n        name = kwargs.pop(\"name\")\n\n    super().__init__(log_stdout=log_stdout, name=name, **kwargs)\n</code></pre>"},{"location":"api/tasks/#lume_services.tasks.file.LoadFile.run","title":"<code>run(file_rep, file_service=Provide[Context.file_service])</code>","text":"<p>Load a file</p> <p>Parameters:</p> Name Type Description Default <code>file_rep</code> <code>dict</code> <p>File data representation</p> required <code>file_service</code> <code>FileService</code> <p>File service for interacting w/ filesystems</p> <code>Provide[file_service]</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Unserialize file object</p> Source code in <code>lume_services/tasks/file.py</code> <pre><code>@inject\ndef run(\n    self, file_rep: dict, file_service: FileService = Provide[Context.file_service]\n) -&gt; Any:\n    \"\"\"Load a file\n\n    Args:\n        file_rep (dict): File data representation\n        file_service (FileService): File service for interacting w/ filesystems\n\n    Returns:\n        Any: Unserialize file object\n\n    \"\"\"\n\n    file_type = get_file_from_serializer_string(file_rep[\"file_type_string\"])\n    file_result = file_type(**file_rep)\n\n    return file_result.read(file_service=file_service)\n</code></pre>"},{"location":"api/tasks/#lume_services.tasks.file.SaveFile","title":"<code>SaveFile</code>","text":"<p>               Bases: <code>Task</code></p> <p>This task is used to save a workflow file to a filesystem for subsequent retrieval. File saving supports all File objects described in LUME-services. Additional file types can be created by subclassing the base in <code>lume_services.files.file</code>. The <code>SaveFile</code> task relies on a ... EXPOUND ON FILESYSTEM IDENTIFIER</p> <p>Examples:</p> <pre><code>from prefect import Flow, task, Parameter\nfrom lume_services.results import Result\nfrom lume_services.tasks import configure_lume_services, SaveFile\nfrom lume_services.files import TextFile\n\n# construct task with prefect Task options\nsave_file_task = SaveFile(timeout=20)\n\n@task\ndef concatenate_text(text1, text2):\n    text = text1 + text2\n    return TextFile(\n                text,\n                filesystem_identifier=\"local\",\n                filename=\"concatenated_text.txt\"\n            )\n\n\nwith Flow(\n    \"my_flow\"\n) as flow:\n    # must first configure services because using injected results\n    # database service\n    configure_lume_services()\n\n    text1 = Parameter(\"text1\")\n    text2 = Parameter(\"text2\")\n\n    my_file_obj = concatenate_text(text1, text2)\n\n    file_parameters = save_file_task.parameters\n\n    my_result = save_file_task(\n        my_file_obj,\n        filename = file_parameters[\"filename\"],\n        filesystem_identifier = file_parameters[\"filesystem_identifier\"],\n        file_type = TextFile # THIS MUST BE PASSED IN THE TASK CALL\n    )\n</code></pre> Source code in <code>lume_services/tasks/file.py</code> <pre><code>class SaveFile(Task):\n    \"\"\"This task is used to save a workflow file to a filesystem for subsequent\n    retrieval. File saving supports all File objects described in\n    [LUME-services](https://slaclab.github.io/lume-services/api/files/files/).\n    Additional file types can be created by subclassing the base in\n    `lume_services.files.file`. The `SaveFile` task relies on a ...\n    EXPOUND ON FILESYSTEM IDENTIFIER\n\n    Examples:\n        ```python\n        from prefect import Flow, task, Parameter\n        from lume_services.results import Result\n        from lume_services.tasks import configure_lume_services, SaveFile\n        from lume_services.files import TextFile\n\n        # construct task with prefect Task options\n        save_file_task = SaveFile(timeout=20)\n\n        @task\n        def concatenate_text(text1, text2):\n            text = text1 + text2\n            return TextFile(\n                        text,\n                        filesystem_identifier=\"local\",\n                        filename=\"concatenated_text.txt\"\n                    )\n\n\n        with Flow(\n            \"my_flow\"\n        ) as flow:\n            # must first configure services because using injected results\n            # database service\n            configure_lume_services()\n\n            text1 = Parameter(\"text1\")\n            text2 = Parameter(\"text2\")\n\n            my_file_obj = concatenate_text(text1, text2)\n\n            file_parameters = save_file_task.parameters\n\n            my_result = save_file_task(\n                my_file_obj,\n                filename = file_parameters[\"filename\"],\n                filesystem_identifier = file_parameters[\"filesystem_identifier\"],\n                file_type = TextFile # THIS MUST BE PASSED IN THE TASK CALL\n            )\n\n        ```\n\n    \"\"\"\n\n    parameters = {\n        \"filename\": Parameter(\"filename\"),\n        \"filesystem_identifier\": Parameter(\"filesystem_identifier\"),\n    }\n\n    def __init__(self, **kwargs):\n        \"\"\"This task is defined as a subclass of the Prefect [Task](https://docs-v1.prefect.io/api/latest/core/task.html#task-2)\n        object and accepts all Task arguments during initialization.\n\n        Args:\n            name (Optional[str]): The name of this task.\n            slug (Optional[str]):  The slug for this task. Slugs provide a stable ID\n                for tasks so that the Prefect API can identify task run states. If a\n                slug is not provided, one will be generated automatically once the\n                task is added to a Flow.\n            tags (Optional[List[str]]): A list of tags for this task.\n            max_retries (Optional[int]): The maximum amount of times this task can be\n                retried\n            retry_delay (Optional[datetime.timedelta]): The amount of time to wait\n                until task is retried\n            retry_on (Optional[Union[Exception, Iterable[Type[Exception]]]]): Exception\n                types that will allow retry behavior to occur. If not set, all\n                exceptions will allow retries. If set, retries will only occur if the\n                exception is a subtype of the exception types provided.\n            timeout (Optional[Union[int, timedelta]]): The amount of time (in seconds)\n                to wait while running this task before a timeout occurs; note that\n                sub-second resolution is not supported, even when passing in a\n                timedelta.\n            trigger (Optional[callable]):  a function that determines whether the task\n                should run, based on the states of any upstream tasks.\n            skip_on_upstream_skip (Optional[bool]): if True, if any immediately upstream\n                tasks are skipped, this task will automatically be skipped as well,\n                regardless of trigger. By default, this prevents tasks from attempting\n                to use either state or data from tasks that didn't run. If False, the\n                task's trigger will be called as normal, with skips considered\n                successes. Defaults to True.\n            cache_for (Optional[timedelta]): The amount of time to maintain a cache\n            of the outputs of this task.  Useful for situations where the containing\n            Flow will be rerun multiple times, but this task doesn't need to be.\n            cache_validator (Optional[Callable]): Validator that will determine\n                whether the cache for this task is still valid (only required if\n                `cache_for` is provided; defaults to\n                `prefect.engine.cache_validators.duration_only`)\n            cache_key (Optional[str]): if provided, a `cache_key`\n                serves as a unique identifier for this Task's cache, and can be shared\n                across both Tasks _and_ Flows; if not provided, the Task's _name_ will\n                be used if running locally, or the Task's database ID if running in\n                Cloud\n            checkpoint (Optional[bool]): if this Task is successful, whether to\n                store its result using the configured result available during the run;\n                Also note that checkpointing will only occur locally if\n                `prefect.config.flows.checkpointing` is set to `True`\n            result (Optional[Result]): the result instance used to retrieve and\n                store task results during execution\n            target (Optional[Union[str, Callable]]): location to check for task Result.\n                If a result exists at that location then the task run will enter a\n                cached state. `target` strings can be templated formatting strings\n                which will be formatted at runtime with values from `prefect.context`.\n                If a callable function is provided, it should have signature\n                `callable(**kwargs) -&gt; str` and at write time all formatting kwargs\n                will be passed and a fully formatted location is expected as the return\n                value. The callable can be used for string formatting logic that\n                `.format(**kwargs)` doesn't support.\n            state_handlers (Optional[Iterable[Callable]]): A list of state change\n                handlers that will be called whenever the task changes state,\n                providing an opportunity to inspect or modify the new state. The\n                handler will be passed the task instance, the old (prior) state,\n                and the new\n                (current) state, with the following signature:\n                    `state_handler(task: Task, old_state: State, new_state: State) -&gt;\n                    Optional[State]`\n                If multiple functions are passed, then the `new_state` argument will\n                be the result of the previous handler.\n            on_failure (Optional[Callable]): A function with signature\n                `fn(task: Task, state: State) -&gt; None` that will be called anytime this\n                Task enters a failure state\n            log_stdout (Optional[bool]): Toggle whether or not to send stdout messages\n                to the Prefect logger. Defaults to `False`.\n            task_run_name (Optional[Union[str, Callable]]): a name to set for this task\n                at runtime. `task_run_name` strings can be templated formatting strings\n                which will be formatted at runtime with values from task arguments,\n                `prefect.context`, and flow parameters (in the case of a name conflict\n                between these, earlier values take precedence). If a callable function\n                is provided, it should have signature `callable(**kwargs) -&gt; str` and\n                at write time all formatting kwargs will be passed and a fully\n                formatted location is expected as the return value. The callable can\n                be used for string formatting logic that `.format(**kwargs)` doesn't\n                support. **Note**: this only works for tasks running against a\n                backend API.\n            nout (Optional[int]): for tasks that return multiple results, the number of\n                outputs to expect. If not provided, will be inferred from the task\n                return annotation, if possible.  Note that `nout=1` implies the task\n                returns a tuple of one value (leave as `None` for non-tuple return\n                types).\n\n        \"\"\"  # noqa\n\n        # apply some defaults but allow overrides\n        log_stdout = kwargs.get(\"log_stdout\")\n        if not kwargs.get(\"log_stdout\"):\n            log_stdout = True\n        else:\n            log_stdout = kwargs.pop(\"log_stdout\")\n\n        if not kwargs.get(\"name\"):\n            name = \"save_file\"\n        else:\n            name = kwargs.pop(\"name\")\n\n        if not kwargs.get(\"result\"):\n            result = PrefectResult(location=_unique_file_location)\n        else:\n            result = kwargs.pop(\"result\")\n\n        super().__init__(log_stdout=log_stdout, name=name, result=result, **kwargs)\n\n    @inject\n    def run(\n        self,\n        obj: Any,\n        filename: str,\n        filesystem_identifier: str,\n        file_type: type,\n        file_service: FileService = Provide[Context.file_service],\n    ):\n        \"\"\"Save a file.\n\n        Args:\n            obj (Any): Object to be saved\n            filename (str): File path to save\n            filesystem_identifier (str): String identifier for filesystem configured\n                with File Service\n            file_type (type): Type of file to save as. This is not exposed as a\n                task parameter and should be passed explicitely during task run call.\n                See examples.\n            file_service (FileService): File service for interacting w/ filesystems\n\n        Returns:\n            dict: Loaded file type\n\n        \"\"\"\n        file = file_type(\n            obj=obj, filesystem_identifier=filesystem_identifier, filename=filename\n        )\n        file.write(file_service=file_service)\n        return file.jsonable_dict()\n</code></pre>"},{"location":"api/tasks/#lume_services.tasks.file.SaveFile.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>This task is defined as a subclass of the Prefect Task object and accepts all Task arguments during initialization.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of this task.</p> required <code>slug</code> <code>Optional[str]</code> <p>The slug for this task. Slugs provide a stable ID for tasks so that the Prefect API can identify task run states. If a slug is not provided, one will be generated automatically once the task is added to a Flow.</p> required <code>tags</code> <code>Optional[List[str]]</code> <p>A list of tags for this task.</p> required <code>max_retries</code> <code>Optional[int]</code> <p>The maximum amount of times this task can be retried</p> required <code>retry_delay</code> <code>Optional[timedelta]</code> <p>The amount of time to wait until task is retried</p> required <code>retry_on</code> <code>Optional[Union[Exception, Iterable[Type[Exception]]]]</code> <p>Exception types that will allow retry behavior to occur. If not set, all exceptions will allow retries. If set, retries will only occur if the exception is a subtype of the exception types provided.</p> required <code>timeout</code> <code>Optional[Union[int, timedelta]]</code> <p>The amount of time (in seconds) to wait while running this task before a timeout occurs; note that sub-second resolution is not supported, even when passing in a timedelta.</p> required <code>trigger</code> <code>Optional[callable]</code> <p>a function that determines whether the task should run, based on the states of any upstream tasks.</p> required <code>skip_on_upstream_skip</code> <code>Optional[bool]</code> <p>if True, if any immediately upstream tasks are skipped, this task will automatically be skipped as well, regardless of trigger. By default, this prevents tasks from attempting to use either state or data from tasks that didn't run. If False, the task's trigger will be called as normal, with skips considered successes. Defaults to True.</p> required <code>cache_for</code> <code>Optional[timedelta]</code> <p>The amount of time to maintain a cache</p> required <code>cache_validator</code> <code>Optional[Callable]</code> <p>Validator that will determine whether the cache for this task is still valid (only required if <code>cache_for</code> is provided; defaults to <code>prefect.engine.cache_validators.duration_only</code>)</p> required <code>cache_key</code> <code>Optional[str]</code> <p>if provided, a <code>cache_key</code> serves as a unique identifier for this Task's cache, and can be shared across both Tasks and Flows; if not provided, the Task's name will be used if running locally, or the Task's database ID if running in Cloud</p> required <code>checkpoint</code> <code>Optional[bool]</code> <p>if this Task is successful, whether to store its result using the configured result available during the run; Also note that checkpointing will only occur locally if <code>prefect.config.flows.checkpointing</code> is set to <code>True</code></p> required <code>result</code> <code>Optional[Result]</code> <p>the result instance used to retrieve and store task results during execution</p> required <code>target</code> <code>Optional[Union[str, Callable]]</code> <p>location to check for task Result. If a result exists at that location then the task run will enter a cached state. <code>target</code> strings can be templated formatting strings which will be formatted at runtime with values from <code>prefect.context</code>. If a callable function is provided, it should have signature <code>callable(**kwargs) -&gt; str</code> and at write time all formatting kwargs will be passed and a fully formatted location is expected as the return value. The callable can be used for string formatting logic that <code>.format(**kwargs)</code> doesn't support.</p> required <code>state_handlers</code> <code>Optional[Iterable[Callable]]</code> <p>A list of state change handlers that will be called whenever the task changes state, providing an opportunity to inspect or modify the new state. The handler will be passed the task instance, the old (prior) state, and the new (current) state, with the following signature:     <code>state_handler(task: Task, old_state: State, new_state: State) -&gt;     Optional[State]</code> If multiple functions are passed, then the <code>new_state</code> argument will be the result of the previous handler.</p> required <code>on_failure</code> <code>Optional[Callable]</code> <p>A function with signature <code>fn(task: Task, state: State) -&gt; None</code> that will be called anytime this Task enters a failure state</p> required <code>log_stdout</code> <code>Optional[bool]</code> <p>Toggle whether or not to send stdout messages to the Prefect logger. Defaults to <code>False</code>.</p> required <code>task_run_name</code> <code>Optional[Union[str, Callable]]</code> <p>a name to set for this task at runtime. <code>task_run_name</code> strings can be templated formatting strings which will be formatted at runtime with values from task arguments, <code>prefect.context</code>, and flow parameters (in the case of a name conflict between these, earlier values take precedence). If a callable function is provided, it should have signature <code>callable(**kwargs) -&gt; str</code> and at write time all formatting kwargs will be passed and a fully formatted location is expected as the return value. The callable can be used for string formatting logic that <code>.format(**kwargs)</code> doesn't support. Note: this only works for tasks running against a backend API.</p> required <code>nout</code> <code>Optional[int]</code> <p>for tasks that return multiple results, the number of outputs to expect. If not provided, will be inferred from the task return annotation, if possible.  Note that <code>nout=1</code> implies the task returns a tuple of one value (leave as <code>None</code> for non-tuple return types).</p> required Source code in <code>lume_services/tasks/file.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"This task is defined as a subclass of the Prefect [Task](https://docs-v1.prefect.io/api/latest/core/task.html#task-2)\n    object and accepts all Task arguments during initialization.\n\n    Args:\n        name (Optional[str]): The name of this task.\n        slug (Optional[str]):  The slug for this task. Slugs provide a stable ID\n            for tasks so that the Prefect API can identify task run states. If a\n            slug is not provided, one will be generated automatically once the\n            task is added to a Flow.\n        tags (Optional[List[str]]): A list of tags for this task.\n        max_retries (Optional[int]): The maximum amount of times this task can be\n            retried\n        retry_delay (Optional[datetime.timedelta]): The amount of time to wait\n            until task is retried\n        retry_on (Optional[Union[Exception, Iterable[Type[Exception]]]]): Exception\n            types that will allow retry behavior to occur. If not set, all\n            exceptions will allow retries. If set, retries will only occur if the\n            exception is a subtype of the exception types provided.\n        timeout (Optional[Union[int, timedelta]]): The amount of time (in seconds)\n            to wait while running this task before a timeout occurs; note that\n            sub-second resolution is not supported, even when passing in a\n            timedelta.\n        trigger (Optional[callable]):  a function that determines whether the task\n            should run, based on the states of any upstream tasks.\n        skip_on_upstream_skip (Optional[bool]): if True, if any immediately upstream\n            tasks are skipped, this task will automatically be skipped as well,\n            regardless of trigger. By default, this prevents tasks from attempting\n            to use either state or data from tasks that didn't run. If False, the\n            task's trigger will be called as normal, with skips considered\n            successes. Defaults to True.\n        cache_for (Optional[timedelta]): The amount of time to maintain a cache\n        of the outputs of this task.  Useful for situations where the containing\n        Flow will be rerun multiple times, but this task doesn't need to be.\n        cache_validator (Optional[Callable]): Validator that will determine\n            whether the cache for this task is still valid (only required if\n            `cache_for` is provided; defaults to\n            `prefect.engine.cache_validators.duration_only`)\n        cache_key (Optional[str]): if provided, a `cache_key`\n            serves as a unique identifier for this Task's cache, and can be shared\n            across both Tasks _and_ Flows; if not provided, the Task's _name_ will\n            be used if running locally, or the Task's database ID if running in\n            Cloud\n        checkpoint (Optional[bool]): if this Task is successful, whether to\n            store its result using the configured result available during the run;\n            Also note that checkpointing will only occur locally if\n            `prefect.config.flows.checkpointing` is set to `True`\n        result (Optional[Result]): the result instance used to retrieve and\n            store task results during execution\n        target (Optional[Union[str, Callable]]): location to check for task Result.\n            If a result exists at that location then the task run will enter a\n            cached state. `target` strings can be templated formatting strings\n            which will be formatted at runtime with values from `prefect.context`.\n            If a callable function is provided, it should have signature\n            `callable(**kwargs) -&gt; str` and at write time all formatting kwargs\n            will be passed and a fully formatted location is expected as the return\n            value. The callable can be used for string formatting logic that\n            `.format(**kwargs)` doesn't support.\n        state_handlers (Optional[Iterable[Callable]]): A list of state change\n            handlers that will be called whenever the task changes state,\n            providing an opportunity to inspect or modify the new state. The\n            handler will be passed the task instance, the old (prior) state,\n            and the new\n            (current) state, with the following signature:\n                `state_handler(task: Task, old_state: State, new_state: State) -&gt;\n                Optional[State]`\n            If multiple functions are passed, then the `new_state` argument will\n            be the result of the previous handler.\n        on_failure (Optional[Callable]): A function with signature\n            `fn(task: Task, state: State) -&gt; None` that will be called anytime this\n            Task enters a failure state\n        log_stdout (Optional[bool]): Toggle whether or not to send stdout messages\n            to the Prefect logger. Defaults to `False`.\n        task_run_name (Optional[Union[str, Callable]]): a name to set for this task\n            at runtime. `task_run_name` strings can be templated formatting strings\n            which will be formatted at runtime with values from task arguments,\n            `prefect.context`, and flow parameters (in the case of a name conflict\n            between these, earlier values take precedence). If a callable function\n            is provided, it should have signature `callable(**kwargs) -&gt; str` and\n            at write time all formatting kwargs will be passed and a fully\n            formatted location is expected as the return value. The callable can\n            be used for string formatting logic that `.format(**kwargs)` doesn't\n            support. **Note**: this only works for tasks running against a\n            backend API.\n        nout (Optional[int]): for tasks that return multiple results, the number of\n            outputs to expect. If not provided, will be inferred from the task\n            return annotation, if possible.  Note that `nout=1` implies the task\n            returns a tuple of one value (leave as `None` for non-tuple return\n            types).\n\n    \"\"\"  # noqa\n\n    # apply some defaults but allow overrides\n    log_stdout = kwargs.get(\"log_stdout\")\n    if not kwargs.get(\"log_stdout\"):\n        log_stdout = True\n    else:\n        log_stdout = kwargs.pop(\"log_stdout\")\n\n    if not kwargs.get(\"name\"):\n        name = \"save_file\"\n    else:\n        name = kwargs.pop(\"name\")\n\n    if not kwargs.get(\"result\"):\n        result = PrefectResult(location=_unique_file_location)\n    else:\n        result = kwargs.pop(\"result\")\n\n    super().__init__(log_stdout=log_stdout, name=name, result=result, **kwargs)\n</code></pre>"},{"location":"api/tasks/#lume_services.tasks.file.SaveFile.run","title":"<code>run(obj, filename, filesystem_identifier, file_type, file_service=Provide[Context.file_service])</code>","text":"<p>Save a file.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Object to be saved</p> required <code>filename</code> <code>str</code> <p>File path to save</p> required <code>filesystem_identifier</code> <code>str</code> <p>String identifier for filesystem configured with File Service</p> required <code>file_type</code> <code>type</code> <p>Type of file to save as. This is not exposed as a task parameter and should be passed explicitely during task run call. See examples.</p> required <code>file_service</code> <code>FileService</code> <p>File service for interacting w/ filesystems</p> <code>Provide[file_service]</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Loaded file type</p> Source code in <code>lume_services/tasks/file.py</code> <pre><code>@inject\ndef run(\n    self,\n    obj: Any,\n    filename: str,\n    filesystem_identifier: str,\n    file_type: type,\n    file_service: FileService = Provide[Context.file_service],\n):\n    \"\"\"Save a file.\n\n    Args:\n        obj (Any): Object to be saved\n        filename (str): File path to save\n        filesystem_identifier (str): String identifier for filesystem configured\n            with File Service\n        file_type (type): Type of file to save as. This is not exposed as a\n            task parameter and should be passed explicitely during task run call.\n            See examples.\n        file_service (FileService): File service for interacting w/ filesystems\n\n    Returns:\n        dict: Loaded file type\n\n    \"\"\"\n    file = file_type(\n        obj=obj, filesystem_identifier=filesystem_identifier, filename=filename\n    )\n    file.write(file_service=file_service)\n    return file.jsonable_dict()\n</code></pre>"},{"location":"api/utils/","title":"Utils","text":""},{"location":"api/utils/#lume_services.utils.filter_keys_in_settings","title":"<code>filter_keys_in_settings(dictionary, settings_obj)</code>","text":"<p>Utility function for checking the membership of dictionary keys in a settings class definition.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>Dictionary to check</p> required <code>settings_obj</code> <code>BaseSettings</code> <p>Settings object for composing</p> required Source code in <code>lume_services/utils.py</code> <pre><code>def filter_keys_in_settings(dictionary: dict, settings_obj: BaseSettings) -&gt; dict:\n    \"\"\"Utility function for checking the membership of dictionary keys in a settings\n    class definition.\n\n    Args:\n        dictionary (dict): Dictionary to check\n        settings_obj (BaseSettings): Settings object for composing\n\n    \"\"\"\n    not_in_settings = [\n        key for key in dictionary.keys() if key not in settings_obj.attributes\n    ]\n    in_settings = [\n        key for key in dictionary.keys() if key not in settings_obj.attributes\n    ]\n\n    if len(not_in_settings):\n        logger.warning(\n            \"Key %s not found in settings. Allowed keys are for %s are %s\",\n            \",\".join(not_in_settings),\n            settings_obj.class_name,\n            \",\".join(settings_obj.attributes),\n        )\n\n    return {key: value for key, value in dictionary.items() if key in in_settings}\n</code></pre>"},{"location":"api/utils/#lume_services.utils.fingerprint_dict","title":"<code>fingerprint_dict(dictionary)</code>","text":"<p>Create a hash for a dictionary</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>Dictionary for which to create a fingerprint hash.</p> required Source code in <code>lume_services/utils.py</code> <pre><code>def fingerprint_dict(dictionary: dict):\n    \"\"\"Create a hash for a dictionary\n\n    Args:\n        dictionary (dict): Dictionary for which to create a fingerprint hash.\n\n    \"\"\"\n\n    dictionary = get_jsonable_dict(dictionary)\n\n    hasher = hashlib.md5()\n    hasher.update(json.dumps(dictionary).encode(\"utf-8\"))\n    return hasher.hexdigest()\n</code></pre>"},{"location":"api/utils/#lume_services.utils.flatten_dict_for_query","title":"<code>flatten_dict_for_query(dictionary, level_key=None)</code>","text":"<p>Flatten a dictionary of values for pymongo query</p> Source code in <code>lume_services/utils.py</code> <pre><code>def flatten_dict_for_query(dictionary: dict, level_key: str = None):\n    \"\"\"Flatten a dictionary of values for pymongo query\"\"\"\n\n    flattened_dict = {}\n\n    for key, value in dictionary.items():\n\n        if level_key is not None:\n            key = f\"{level_key}.{key}\"\n\n        if isinstance(value, dict):\n            value_dict = flatten_dict_for_query(value, level_key=key)\n            flattened_dict.update(value_dict)\n\n        else:\n            flattened_dict[key] = value\n\n    return flattened_dict\n</code></pre>"},{"location":"api/utils/#lume_services.utils.get_callable_from_string","title":"<code>get_callable_from_string(callable, bind=None)</code>","text":"<p>Get callable from a string. In the case that the callable points to a bound method, the function returns a callable taking the bind instance as the first arg.</p> <p>Parameters:</p> Name Type Description Default <code>callable</code> <code>str</code> <p>String representation of callable abiding convention  module:callable</p> required <code>bind</code> <code>Any</code> <p>Class to bind as self</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>Callable</p> Source code in <code>lume_services/utils.py</code> <pre><code>def get_callable_from_string(callable: str, bind: Any = None) -&gt; Callable:\n    \"\"\"Get callable from a string. In the case that the callable points to a bound\n    method, the function returns a callable taking the bind instance as the first arg.\n\n    Args:\n        callable: String representation of callable abiding convention\n             __module__:callable\n        bind: Class to bind as self\n\n    Returns:\n        Callable\n    \"\"\"\n    callable_split = callable.rsplit(\".\", 1)\n\n    if len(callable_split) != 2:\n        raise ValueError(f\"Improperly formatted callable string: {callable_split}\")\n\n    module_name, callable_name = callable_split\n\n    try:\n        module = import_module(module_name)\n\n    except ModuleNotFoundError:\n\n        try:\n            module_split = module_name.rsplit(\".\", 1)\n\n            if len(module_split) != 2:\n                raise ValueError(f\"Unable to access: {callable}\")\n\n            module_name, class_name = module_split\n\n            module = import_module(module_name)\n            callable_name = f\"{class_name}.{callable_name}\"\n\n        except ModuleNotFoundError as err:\n            logger.error(\"Unable to import module %s\", module_name)\n            raise err\n\n        except ValueError as err:\n            logger.error(err)\n            raise err\n\n    # construct partial in case of bound method\n    if \".\" in callable_name:\n        bound_class, callable_name = callable_name.rsplit(\".\")\n\n        try:\n            bound_class = getattr(module, bound_class)\n        except Exception as e:\n            logger.error(\"Unable to get %s from %s\", bound_class, module_name)\n            raise e\n\n        # require right partial for assembly of callable\n        # https://funcy.readthedocs.io/en/stable/funcs.html#rpartial\n        def rpartial(func, *args):\n            return lambda *a: func(*(a + args))\n\n        callable = getattr(bound_class, callable_name)\n        params = inspect.signature(callable).parameters\n\n        # check bindings\n        is_bound = params.get(\"self\", None) is not None\n        if not is_bound and bind is not None:\n            raise ValueError(\"Cannot bind %s to %s.\", callable_name, bind)\n\n        # bound, return partial\n        if bind is not None:\n            if not isinstance(bind, (bound_class,)):\n                raise ValueError(\n                    \"Provided bind %s is not instance of %s\",\n                    bind,\n                    bound_class.__qualname__,\n                )\n\n        if is_bound and isinstance(callable, (FunctionType,)) and bind is None:\n            callable = rpartial(getattr, callable_name)\n\n        elif is_bound and isinstance(callable, (FunctionType,)) and bind is not None:\n            callable = getattr(bind, callable_name)\n\n    else:\n        if bind is not None:\n            raise ValueError(\"Cannot bind %s to %s.\", callable_name, type(bind))\n\n        try:\n            callable = getattr(module, callable_name)\n        except Exception as e:\n            logger.error(\"Unable to get %s from %s\", callable_name, module_name)\n            raise e\n\n    return callable\n</code></pre>"},{"location":"api/utils/#lume_services.utils.get_jsonable_dict","title":"<code>get_jsonable_dict(dictionary)</code>","text":"<p>Converts numpy arrays inside a dictionary to list items.</p> Source code in <code>lume_services/utils.py</code> <pre><code>def get_jsonable_dict(dictionary: dict):\n    \"\"\"Converts numpy arrays inside a dictionary to list items.\"\"\"\n\n    def convert_array_values(dictionary):\n        \"\"\"Convert array values to list so the dictionary is json serializable.\"\"\"\n        dictionary = {\n            key: value.tolist() if isinstance(value, (np.ndarray,)) else value\n            for key, value in dictionary.items()\n        }\n        # convert pandas dataframe to json\n        dictionary = {\n            key: value.to_json() if isinstance(value, (pd.DataFrame,)) else value\n            for key, value in dictionary.items()\n        }\n        dictionary = {\n            key: convert_array_values(value) if isinstance(value, (dict,)) else value\n            for key, value in dictionary.items()\n        }\n        return dictionary\n\n    return convert_array_values(dictionary)\n</code></pre>"},{"location":"api/utils/#lume_services.utils.select_python_version","title":"<code>select_python_version(version)</code>","text":"<p>Utility for selecting a python version given a version string formatted with a pin.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>String rep of version with pin</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Sting rep of version to use</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Unable to parse python version.</p> Source code in <code>lume_services/utils.py</code> <pre><code>def select_python_version(version: str) -&gt; str:\n    \"\"\"Utility for selecting a python version given a version string formatted with\n    a pin.\n\n    Args:\n        version (str): String rep of version with pin\n\n    Returns:\n        str: Sting rep of version to use\n\n    Raises:\n        ValueError: Unable to parse python version.\n\n    \"\"\"\n\n    # if &gt;=, use base\n    if \"&gt;=\" in version or \"&lt;=\" in version or \"=\" in version:\n        v = float(version.split(\"=\")[1])\n\n    elif \"&gt;\" in version:\n        v = float(version.replace(\"&gt;\", \"\")) + 0.1\n\n    elif \"&lt;\" in version:\n        v = float(version.replace(\"&lt;\", \"\")) - 0.1\n\n    else:\n        raise ValueError(\"Cannot parse python version %s\", version)\n\n    return str(v)\n</code></pre>"},{"location":"api/files/files/","title":"File","text":""},{"location":"api/files/files/#lume_services.files.file.File","title":"<code>File</code>","text":"<p>               Bases: <code>GenericModel</code>, <code>Generic[ObjType]</code></p> Source code in <code>lume_services/files/file.py</code> <pre><code>class File(\n    GenericModel,\n    Generic[ObjType],\n    arbitrary_types_allowed=True,\n    json_encoders=JSON_ENCODERS,\n    allow_population_by_field_name=True,\n):\n    filename: str\n    file_type_string: str\n    filesystem_identifier: str = \"local\"\n\n    # Object to-be-serialized/result of deserialization\n    obj: Any = Field(None, exclude=True)\n\n    loader: Optional[ObjLoader[ObjType]] = Field(exclude=True)\n    serializer: ObjType = Field(exclude=True)\n\n    load: bool = Field(False, exclude=True)\n\n    @root_validator(pre=True)\n    def validate_all(cls, values):\n        serializer_type = cls.__fields__[\"serializer\"].type_\n\n        # Compose loader utility\n        if values.get(\"loader\") is not None:\n            loader_values = values[\"loader\"]\n            loader = ObjLoader[serializer_type](**loader_values)\n\n        else:\n            # maintain reference to original object\n            loader_values = copy.copy(values)\n\n            # if serializer in values, need to remove\n            if \"serializer\" in loader_values:\n                loader_values.pop(\"serializer\")\n\n            # if serializer_type in values, need to remove\n            if \"serializer_type\" in loader_values:\n                loader_values.pop(\"serializer_type\")\n\n            # if serializer_type in values, need to remove\n            if \"filename\" in loader_values:\n                loader_values.pop(\"filename\")\n\n            # if filesystem_identifier in values, need to remove\n            if \"filesystem_identifier\" in loader_values:\n                loader_values.pop(\"filesystem_identifier\")\n\n            # if filesystem_identifier in values, need to remove\n            if \"file_type_string\" in loader_values:\n                loader_values.pop(\"file_type_string\")\n\n            # if obj in values, need to remove\n            if \"obj\" in loader_values:\n                loader_values.pop(\"obj\")\n\n            loader = ObjLoader[serializer_type](**loader_values)\n\n        values[\"serializer_type\"] = serializer_type\n        values[\"loader\"] = loader\n        values[\"serializer\"] = loader.load()\n\n        values[\"file_type_string\"] = f\"{cls.__module__}:{cls.__name__}\"\n\n        # update the class json encoders. Will only execute on initial type construction\n        if serializer_type not in cls.__config__.json_encoders:\n            cls.__config__.json_encoders[\n                serializer_type\n            ] = cls.__config__.json_encoders.pop(ObjType)\n\n        return values\n\n    @inject\n    def write(\n        self,\n        obj=None,\n        file_service: FileService = Provide[Context.file_service],\n        create_dir=False,\n    ):\n        if not obj:\n            if not self.obj:\n                raise ValueError(\"Must provide object to write.\")\n\n            file_service.write(\n                self.filesystem_identifier,\n                self.filename,\n                self.obj,\n                self.serializer,\n                create_dir=create_dir,\n            )\n        else:\n            self.obj = obj\n            file_service.write(\n                self.filesystem_identifier,\n                self.filename,\n                obj,\n                self.serializer,\n                create_dir=create_dir,\n            )\n\n    @inject\n    def read(self, file_service: FileService = Provide[Context.file_service]):\n        return file_service.read(\n            self.filesystem_identifier,\n            self.filename,\n            self.serializer,\n        )\n\n    @inject\n    def load_file(\n        self, file_service: FileService = Provide[Context.file_service]\n    ) -&gt; None:\n        \"\"\"Load file object from instantiated File.\"\"\"\n        self.obj = self.read(file_service=file_service)\n\n    def jsonable_dict(self) -&gt; dict:\n        return json.loads(self.json(by_alias=True))\n</code></pre>"},{"location":"api/files/files/#lume_services.files.file.File.load_file","title":"<code>load_file(file_service=Provide[Context.file_service])</code>","text":"<p>Load file object from instantiated File.</p> Source code in <code>lume_services/files/file.py</code> <pre><code>@inject\ndef load_file(\n    self, file_service: FileService = Provide[Context.file_service]\n) -&gt; None:\n    \"\"\"Load file object from instantiated File.\"\"\"\n    self.obj = self.read(file_service=file_service)\n</code></pre>"},{"location":"api/files/serializers/","title":"Serializers","text":""},{"location":"api/files/serializers/#lume_services.files.serializers.image.ImageSerializer","title":"<code>ImageSerializer</code>","text":"<p>               Bases: <code>SerializerBase</code></p> <p>Pillow image serializer.</p> Source code in <code>lume_services/files/serializers/image.py</code> <pre><code>class ImageSerializer(SerializerBase):\n    \"\"\"Pillow image serializer.\"\"\"\n\n    def serialize(self, filename, image: Image):\n        image.save(filename)\n\n    @classmethod\n    def deserialize(cls, filename) -&gt; Image:\n\n        return Image.open(filename)\n</code></pre>"},{"location":"api/files/serializers/#lume_services.files.serializers.yaml.YAMLSerializer","title":"<code>YAMLSerializer</code>","text":"<p>               Bases: <code>SerializerBase</code></p> <p>Serializer sublass handling YAML files.</p> Source code in <code>lume_services/files/serializers/yaml.py</code> <pre><code>class YAMLSerializer(SerializerBase):\n    \"\"\"Serializer sublass handling YAML files.\"\"\"\n\n    def serialize(self, filename: str, object: List[dict]) -&gt; None:\n        \"\"\"Serialize an object to a YAML.\n\n\n        Args:\n            filename (str): Name of file to write.\n            object (List[dict]): Object to serialize.\n\n        \"\"\"\n\n        with open(filename, \"w\") as f:\n            yaml.dump(object, f)\n\n    @classmethod\n    def deserialize(cls, filename: str) -&gt; List[dict]:\n        \"\"\"Deserializes a given YAML file.\n\n        Args:\n            filename (str): Name of file to deserialize.\n\n        Returns:\n            List[dict]: Loaded YAML representation.\n\n        \"\"\"\n        yaml_rep = None\n\n        with open(filename, \"r\") as f:\n\n            try:\n                yaml_rep = yaml.safe_load(f)\n\n            except yaml.YAMLError as exc:\n                logger.exception(exc)\n\n        return yaml_rep\n</code></pre>"},{"location":"api/files/serializers/#lume_services.files.serializers.yaml.YAMLSerializer.deserialize","title":"<code>deserialize(filename)</code>  <code>classmethod</code>","text":"<p>Deserializes a given YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of file to deserialize.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: Loaded YAML representation.</p> Source code in <code>lume_services/files/serializers/yaml.py</code> <pre><code>@classmethod\ndef deserialize(cls, filename: str) -&gt; List[dict]:\n    \"\"\"Deserializes a given YAML file.\n\n    Args:\n        filename (str): Name of file to deserialize.\n\n    Returns:\n        List[dict]: Loaded YAML representation.\n\n    \"\"\"\n    yaml_rep = None\n\n    with open(filename, \"r\") as f:\n\n        try:\n            yaml_rep = yaml.safe_load(f)\n\n        except yaml.YAMLError as exc:\n            logger.exception(exc)\n\n    return yaml_rep\n</code></pre>"},{"location":"api/files/serializers/#lume_services.files.serializers.yaml.YAMLSerializer.serialize","title":"<code>serialize(filename, object)</code>","text":"<p>Serialize an object to a YAML.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of file to write.</p> required <code>object</code> <code>List[dict]</code> <p>Object to serialize.</p> required Source code in <code>lume_services/files/serializers/yaml.py</code> <pre><code>def serialize(self, filename: str, object: List[dict]) -&gt; None:\n    \"\"\"Serialize an object to a YAML.\n\n\n    Args:\n        filename (str): Name of file to write.\n        object (List[dict]): Object to serialize.\n\n    \"\"\"\n\n    with open(filename, \"w\") as f:\n        yaml.dump(object, f)\n</code></pre>"},{"location":"api/files/utils/","title":"Utils","text":""},{"location":"api/services/files/files/","title":"File Service","text":""},{"location":"api/services/files/files/#lume_services.services.files.service.FileService","title":"<code>FileService</code>","text":"<p>Allows saving and retrieval to multiple file storage locations.</p> Source code in <code>lume_services/services/files/service.py</code> <pre><code>class FileService:\n    \"\"\"Allows saving and retrieval to multiple file storage locations.\"\"\"\n\n    def __init__(self, filesystems: List[Filesystem]):\n        \"\"\"\n\n        Args:\n            filesystems (List[Filesystem]): List of filesystems\n\n        \"\"\"\n        self._filesystems = {\n            filesystem.identifier: filesystem for filesystem in filesystems\n        }\n\n    def dir_exists(\n        self, filesystem_identifier: str, dir: str, create_dir: bool = True\n    ) -&gt; bool:\n        \"\"\"Check that a directory exists in a filesystem.\n\n        Args:\n            filesystem_identifier (str): String dentifier for filesystem\n            dir (str): Directory path\n            create_dir (bool): Whether to create directory in case not implemented\n\n        Returns:\n            bool\n\n        \"\"\"\n        filesystem = self._get_filesystem(filesystem_identifier)\n        filesystem.dir_exists(dir, create_dir=create_dir)\n\n    def file_exists(self, filesystem_identifier: str, file: str) -&gt; bool:\n        \"\"\"Check that a file exists in a filesystem.\n\n        Args:\n            filesystem_identifier (str): String dentifier for filesystem\n\n        Returns:\n            bool\n\n        \"\"\"\n\n        filesystem = self._get_filesystem(filesystem_identifier)\n        filesystem.file_exists(file)\n\n    def create_dir(self, filesystem_identifier: str, dir: str) -&gt; None:\n        \"\"\"Create a directory in a filesystem.\n\n        Args:\n            filesystem_identifier (str): String dentifier for filesystem\n            dir (str): Directory path\n\n        \"\"\"\n        filesystem = self._get_filesystem(filesystem_identifier)\n        filesystem.create_dir(dir)\n\n    def read(\n        self, filesystem_identifier: str, file: str, serializer: SerializerBase\n    ) -&gt; Any:\n        \"\"\"Read file from a filesystem.\n\n        Args:\n            filesystem_identifier (str): String dentifier for filesystem\n            file (str): Path of file to read\n            serializer (SerializerBase): Implementation of lume-base SerializerBase\n                abstract base class\n\n        \"\"\"\n        filesystem = self._get_filesystem(filesystem_identifier)\n        return filesystem.read(file, serializer)\n\n    def write(\n        self,\n        filesystem_identifier: str,\n        filepath: str,\n        object: Any,\n        serializer: SerializerBase,\n        create_dir: bool = True,\n    ):\n        \"\"\"Write a file to a filesystem.\n\n        Args:\n            filesystem_identifier (str): String dentifier for filesystem\n            filepath (str): Save path for file\n            object (Any): Object to serialize\n            serializer (SerializerBase): Implementation of lume-base SerializerBase\n                abstract base class\n            create_dir (bool): Whether to create directory in case not implemented\n\n        \"\"\"\n        filesystem = self._get_filesystem(filesystem_identifier)\n        filesystem.write(filepath, object, serializer, create_dir=create_dir)\n\n    def _get_filesystem(self, filesystem_identifier: str) -&gt; Filesystem:\n        \"\"\"Get filesystem by identifier.\n\n        Args:\n            filesystem_identifier (str): Identifier for filesystem.\n\n        Returns:\n            Filesystem\n\n        \"\"\"\n\n        filesystem = self._filesystems.get(filesystem_identifier, None)\n\n        if not filesystem:\n            logger.error()\n            raise FilesystemNotConfigured(\n                filesystem_identifier, list(self._filesystems.keys())\n            )\n\n        return filesystem\n\n    def get_mounted_filesystems(self) -&gt; Dict[str, MountedFilesystem]:\n        \"\"\"Return mounted filesystems\n\n        Returns:\n            Dict[str, MountedFilesystem]\n\n        \"\"\"\n        return {\n            filesystem_identifier: filesystem\n            for filesystem_identifier, filesystem in self._filesystems.items()\n            if isinstance(filesystem, (MountedFilesystem,))\n        }\n</code></pre>"},{"location":"api/services/files/files/#lume_services.services.files.service.FileService.__init__","title":"<code>__init__(filesystems)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>filesystems</code> <code>List[Filesystem]</code> <p>List of filesystems</p> required Source code in <code>lume_services/services/files/service.py</code> <pre><code>def __init__(self, filesystems: List[Filesystem]):\n    \"\"\"\n\n    Args:\n        filesystems (List[Filesystem]): List of filesystems\n\n    \"\"\"\n    self._filesystems = {\n        filesystem.identifier: filesystem for filesystem in filesystems\n    }\n</code></pre>"},{"location":"api/services/files/files/#lume_services.services.files.service.FileService.create_dir","title":"<code>create_dir(filesystem_identifier, dir)</code>","text":"<p>Create a directory in a filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>filesystem_identifier</code> <code>str</code> <p>String dentifier for filesystem</p> required <code>dir</code> <code>str</code> <p>Directory path</p> required Source code in <code>lume_services/services/files/service.py</code> <pre><code>def create_dir(self, filesystem_identifier: str, dir: str) -&gt; None:\n    \"\"\"Create a directory in a filesystem.\n\n    Args:\n        filesystem_identifier (str): String dentifier for filesystem\n        dir (str): Directory path\n\n    \"\"\"\n    filesystem = self._get_filesystem(filesystem_identifier)\n    filesystem.create_dir(dir)\n</code></pre>"},{"location":"api/services/files/files/#lume_services.services.files.service.FileService.dir_exists","title":"<code>dir_exists(filesystem_identifier, dir, create_dir=True)</code>","text":"<p>Check that a directory exists in a filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>filesystem_identifier</code> <code>str</code> <p>String dentifier for filesystem</p> required <code>dir</code> <code>str</code> <p>Directory path</p> required <code>create_dir</code> <code>bool</code> <p>Whether to create directory in case not implemented</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>bool</p> Source code in <code>lume_services/services/files/service.py</code> <pre><code>def dir_exists(\n    self, filesystem_identifier: str, dir: str, create_dir: bool = True\n) -&gt; bool:\n    \"\"\"Check that a directory exists in a filesystem.\n\n    Args:\n        filesystem_identifier (str): String dentifier for filesystem\n        dir (str): Directory path\n        create_dir (bool): Whether to create directory in case not implemented\n\n    Returns:\n        bool\n\n    \"\"\"\n    filesystem = self._get_filesystem(filesystem_identifier)\n    filesystem.dir_exists(dir, create_dir=create_dir)\n</code></pre>"},{"location":"api/services/files/files/#lume_services.services.files.service.FileService.file_exists","title":"<code>file_exists(filesystem_identifier, file)</code>","text":"<p>Check that a file exists in a filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>filesystem_identifier</code> <code>str</code> <p>String dentifier for filesystem</p> required <p>Returns:</p> Type Description <code>bool</code> <p>bool</p> Source code in <code>lume_services/services/files/service.py</code> <pre><code>def file_exists(self, filesystem_identifier: str, file: str) -&gt; bool:\n    \"\"\"Check that a file exists in a filesystem.\n\n    Args:\n        filesystem_identifier (str): String dentifier for filesystem\n\n    Returns:\n        bool\n\n    \"\"\"\n\n    filesystem = self._get_filesystem(filesystem_identifier)\n    filesystem.file_exists(file)\n</code></pre>"},{"location":"api/services/files/files/#lume_services.services.files.service.FileService.get_mounted_filesystems","title":"<code>get_mounted_filesystems()</code>","text":"<p>Return mounted filesystems</p> <p>Returns:</p> Type Description <code>Dict[str, MountedFilesystem]</code> <p>Dict[str, MountedFilesystem]</p> Source code in <code>lume_services/services/files/service.py</code> <pre><code>def get_mounted_filesystems(self) -&gt; Dict[str, MountedFilesystem]:\n    \"\"\"Return mounted filesystems\n\n    Returns:\n        Dict[str, MountedFilesystem]\n\n    \"\"\"\n    return {\n        filesystem_identifier: filesystem\n        for filesystem_identifier, filesystem in self._filesystems.items()\n        if isinstance(filesystem, (MountedFilesystem,))\n    }\n</code></pre>"},{"location":"api/services/files/files/#lume_services.services.files.service.FileService.read","title":"<code>read(filesystem_identifier, file, serializer)</code>","text":"<p>Read file from a filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>filesystem_identifier</code> <code>str</code> <p>String dentifier for filesystem</p> required <code>file</code> <code>str</code> <p>Path of file to read</p> required <code>serializer</code> <code>SerializerBase</code> <p>Implementation of lume-base SerializerBase abstract base class</p> required Source code in <code>lume_services/services/files/service.py</code> <pre><code>def read(\n    self, filesystem_identifier: str, file: str, serializer: SerializerBase\n) -&gt; Any:\n    \"\"\"Read file from a filesystem.\n\n    Args:\n        filesystem_identifier (str): String dentifier for filesystem\n        file (str): Path of file to read\n        serializer (SerializerBase): Implementation of lume-base SerializerBase\n            abstract base class\n\n    \"\"\"\n    filesystem = self._get_filesystem(filesystem_identifier)\n    return filesystem.read(file, serializer)\n</code></pre>"},{"location":"api/services/files/files/#lume_services.services.files.service.FileService.write","title":"<code>write(filesystem_identifier, filepath, object, serializer, create_dir=True)</code>","text":"<p>Write a file to a filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>filesystem_identifier</code> <code>str</code> <p>String dentifier for filesystem</p> required <code>filepath</code> <code>str</code> <p>Save path for file</p> required <code>object</code> <code>Any</code> <p>Object to serialize</p> required <code>serializer</code> <code>SerializerBase</code> <p>Implementation of lume-base SerializerBase abstract base class</p> required <code>create_dir</code> <code>bool</code> <p>Whether to create directory in case not implemented</p> <code>True</code> Source code in <code>lume_services/services/files/service.py</code> <pre><code>def write(\n    self,\n    filesystem_identifier: str,\n    filepath: str,\n    object: Any,\n    serializer: SerializerBase,\n    create_dir: bool = True,\n):\n    \"\"\"Write a file to a filesystem.\n\n    Args:\n        filesystem_identifier (str): String dentifier for filesystem\n        filepath (str): Save path for file\n        object (Any): Object to serialize\n        serializer (SerializerBase): Implementation of lume-base SerializerBase\n            abstract base class\n        create_dir (bool): Whether to create directory in case not implemented\n\n    \"\"\"\n    filesystem = self._get_filesystem(filesystem_identifier)\n    filesystem.write(filepath, object, serializer, create_dir=create_dir)\n</code></pre>"},{"location":"api/services/files/filesystems/","title":"Filesystems","text":""},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.filesystem.Filesystem","title":"<code>Filesystem</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> Source code in <code>lume_services/services/files/filesystems/filesystem.py</code> <pre><code>class Filesystem(ABC, BaseModel):\n    identifier: str\n\n    @abstractmethod\n    def dir_exists(self, dir: str, create_dir: bool = False) -&gt; bool:\n        \"\"\"Check that a directory exists\n\n        Args:\n            dir (str): Path of directory\n            create_dir (bool): Whether to create directory if it does not exist\n\n        Returns:\n            bool\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def file_exists(self, filepath: str) -&gt; bool:\n        \"\"\"Check that a file exists\n\n        Args:\n            filepath (str): Path to file\n\n        Returns:\n            bool\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def create_dir(self, dir: str) -&gt; None:\n        \"\"\"Create a directory on the filesystem.\n\n        Args:\n            dir (str): Directory path to create\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def read(self, filepath: str, serializer: SerializerBase) -&gt; Any:\n        \"\"\"Read file from the filesystem.\n\n        Args:\n            filepath (str): Path of file to read\n            serializer (SerializerBase): Implementation of lume-base SerializerBase\n                abstract base class\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def write(\n        self,\n        filepath: str,\n        object: Any,\n        serializer: SerializerBase,\n        create_dir: bool = False,\n    ) -&gt; None:\n        \"\"\"Write a file to the filesystem.\n\n        Args:\n            filepath (str):\n            object (Any): Object to serialize\n            serializer (SerializerBase): Implementation of lume-base SerializerBase\n                abstract base class\n            create_dir (bool): Whether to create directory in case not implemented\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.filesystem.Filesystem.create_dir","title":"<code>create_dir(dir)</code>  <code>abstractmethod</code>","text":"<p>Create a directory on the filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Directory path to create</p> required Source code in <code>lume_services/services/files/filesystems/filesystem.py</code> <pre><code>@abstractmethod\ndef create_dir(self, dir: str) -&gt; None:\n    \"\"\"Create a directory on the filesystem.\n\n    Args:\n        dir (str): Directory path to create\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.filesystem.Filesystem.dir_exists","title":"<code>dir_exists(dir, create_dir=False)</code>  <code>abstractmethod</code>","text":"<p>Check that a directory exists</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Path of directory</p> required <code>create_dir</code> <code>bool</code> <p>Whether to create directory if it does not exist</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>bool</p> Source code in <code>lume_services/services/files/filesystems/filesystem.py</code> <pre><code>@abstractmethod\ndef dir_exists(self, dir: str, create_dir: bool = False) -&gt; bool:\n    \"\"\"Check that a directory exists\n\n    Args:\n        dir (str): Path of directory\n        create_dir (bool): Whether to create directory if it does not exist\n\n    Returns:\n        bool\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.filesystem.Filesystem.file_exists","title":"<code>file_exists(filepath)</code>  <code>abstractmethod</code>","text":"<p>Check that a file exists</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to file</p> required <p>Returns:</p> Type Description <code>bool</code> <p>bool</p> Source code in <code>lume_services/services/files/filesystems/filesystem.py</code> <pre><code>@abstractmethod\ndef file_exists(self, filepath: str) -&gt; bool:\n    \"\"\"Check that a file exists\n\n    Args:\n        filepath (str): Path to file\n\n    Returns:\n        bool\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.filesystem.Filesystem.read","title":"<code>read(filepath, serializer)</code>  <code>abstractmethod</code>","text":"<p>Read file from the filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path of file to read</p> required <code>serializer</code> <code>SerializerBase</code> <p>Implementation of lume-base SerializerBase abstract base class</p> required Source code in <code>lume_services/services/files/filesystems/filesystem.py</code> <pre><code>@abstractmethod\ndef read(self, filepath: str, serializer: SerializerBase) -&gt; Any:\n    \"\"\"Read file from the filesystem.\n\n    Args:\n        filepath (str): Path of file to read\n        serializer (SerializerBase): Implementation of lume-base SerializerBase\n            abstract base class\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.filesystem.Filesystem.write","title":"<code>write(filepath, object, serializer, create_dir=False)</code>  <code>abstractmethod</code>","text":"<p>Write a file to the filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> required <code>object</code> <code>Any</code> <p>Object to serialize</p> required <code>serializer</code> <code>SerializerBase</code> <p>Implementation of lume-base SerializerBase abstract base class</p> required <code>create_dir</code> <code>bool</code> <p>Whether to create directory in case not implemented</p> <code>False</code> Source code in <code>lume_services/services/files/filesystems/filesystem.py</code> <pre><code>@abstractmethod\ndef write(\n    self,\n    filepath: str,\n    object: Any,\n    serializer: SerializerBase,\n    create_dir: bool = False,\n) -&gt; None:\n    \"\"\"Write a file to the filesystem.\n\n    Args:\n        filepath (str):\n        object (Any): Object to serialize\n        serializer (SerializerBase): Implementation of lume-base SerializerBase\n            abstract base class\n        create_dir (bool): Whether to create directory in case not implemented\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.local.LocalFilesystem","title":"<code>LocalFilesystem</code>","text":"<p>               Bases: <code>Filesystem</code></p> <p>Handler for local filesystem.</p> Source code in <code>lume_services/services/files/filesystems/local.py</code> <pre><code>class LocalFilesystem(Filesystem):\n    \"\"\"Handler for local filesystem.\"\"\"\n\n    identifier: str = \"local\"\n\n    def dir_exists(self, dir: str, create_dir: bool = False) -&gt; bool:\n        \"\"\"Check that a directory exists on the local filesystem.\n\n        Args:\n            dir (str): Path of directory\n            create_dir (bool): Whether to create directory if it does not exist\n\n        Returns:\n            bool\n\n        \"\"\"\n        # use absolute path\n        path = os.path.abspath(dir)\n        if os.path.isdir(path):\n            logger.info(\"Found directory %s on local filesystem.\", path)\n            return True\n\n        # if creating...\n        if create_dir:\n            self.create_dir(path)\n            return True\n        else:\n            logger.info(\"Unable to find directory %s on local filesystem.\", path)\n            return False\n\n    def file_exists(self, filepath: str) -&gt; bool:\n        \"\"\"Check that a file exists on the local filesystem.\n\n        Args:\n            filepath (str): Path to file\n\n        Returns:\n            bool\n\n        \"\"\"\n        path = os.path.abspath(filepath)\n        if os.path.isfile(path):\n            logger.info(\"Found file %s on local filesystem.\", path)\n            return True\n\n        else:\n            logger.info(\"Unable to find file %s on local filesystem.\", path)\n            return False\n\n    def create_dir(self, dir: str) -&gt; None:\n        \"\"\"Create a directory on the local filesystem.\n\n        Args:\n            dir (str): Directory path to create\n\n        \"\"\"\n        try:\n            os.makedirs(dir, exist_ok=False)\n        except Exception as e:\n            logger.error(\"Unable to create directory %s on local filesystem.\", dir)\n            raise e\n\n    def read(self, filepath: str, serializer: SerializerBase) -&gt; Any:\n        \"\"\"Read file from the local filesystem.\n\n        Args:\n            filepath (str): Path of file to read.\n            serializer (SerializerBase): Implementation of lume-base SerializerBase\n                abstract base class.\n\n        \"\"\"\n        path = os.path.abspath(filepath)\n        content = serializer.deserialize(path)\n        return content\n\n    def write(\n        self,\n        filepath: str,\n        object: Any,\n        serializer: SerializerBase,\n        create_dir: bool = False,\n    ) -&gt; None:\n        \"\"\"Write a file to the local filesystem.\n\n        Args:\n            filepath (str):\n            object (Any): Object to serialize\n            serializer (SerializerBase): Implementation of lume-base SerializerBase\n                abstract base class\n            create_dir (bool): Whether to create directory in case not implemented\n\n        \"\"\"\n        path = os.path.abspath(filepath)\n        dir = os.path.dirname(path)\n\n        if create_dir and not self.dir_exists(dir):\n            self.create_dir(dir)\n\n        serializer.serialize(path, object)\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.local.LocalFilesystem.create_dir","title":"<code>create_dir(dir)</code>","text":"<p>Create a directory on the local filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Directory path to create</p> required Source code in <code>lume_services/services/files/filesystems/local.py</code> <pre><code>def create_dir(self, dir: str) -&gt; None:\n    \"\"\"Create a directory on the local filesystem.\n\n    Args:\n        dir (str): Directory path to create\n\n    \"\"\"\n    try:\n        os.makedirs(dir, exist_ok=False)\n    except Exception as e:\n        logger.error(\"Unable to create directory %s on local filesystem.\", dir)\n        raise e\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.local.LocalFilesystem.dir_exists","title":"<code>dir_exists(dir, create_dir=False)</code>","text":"<p>Check that a directory exists on the local filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Path of directory</p> required <code>create_dir</code> <code>bool</code> <p>Whether to create directory if it does not exist</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>bool</p> Source code in <code>lume_services/services/files/filesystems/local.py</code> <pre><code>def dir_exists(self, dir: str, create_dir: bool = False) -&gt; bool:\n    \"\"\"Check that a directory exists on the local filesystem.\n\n    Args:\n        dir (str): Path of directory\n        create_dir (bool): Whether to create directory if it does not exist\n\n    Returns:\n        bool\n\n    \"\"\"\n    # use absolute path\n    path = os.path.abspath(dir)\n    if os.path.isdir(path):\n        logger.info(\"Found directory %s on local filesystem.\", path)\n        return True\n\n    # if creating...\n    if create_dir:\n        self.create_dir(path)\n        return True\n    else:\n        logger.info(\"Unable to find directory %s on local filesystem.\", path)\n        return False\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.local.LocalFilesystem.file_exists","title":"<code>file_exists(filepath)</code>","text":"<p>Check that a file exists on the local filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to file</p> required <p>Returns:</p> Type Description <code>bool</code> <p>bool</p> Source code in <code>lume_services/services/files/filesystems/local.py</code> <pre><code>def file_exists(self, filepath: str) -&gt; bool:\n    \"\"\"Check that a file exists on the local filesystem.\n\n    Args:\n        filepath (str): Path to file\n\n    Returns:\n        bool\n\n    \"\"\"\n    path = os.path.abspath(filepath)\n    if os.path.isfile(path):\n        logger.info(\"Found file %s on local filesystem.\", path)\n        return True\n\n    else:\n        logger.info(\"Unable to find file %s on local filesystem.\", path)\n        return False\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.local.LocalFilesystem.read","title":"<code>read(filepath, serializer)</code>","text":"<p>Read file from the local filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path of file to read.</p> required <code>serializer</code> <code>SerializerBase</code> <p>Implementation of lume-base SerializerBase abstract base class.</p> required Source code in <code>lume_services/services/files/filesystems/local.py</code> <pre><code>def read(self, filepath: str, serializer: SerializerBase) -&gt; Any:\n    \"\"\"Read file from the local filesystem.\n\n    Args:\n        filepath (str): Path of file to read.\n        serializer (SerializerBase): Implementation of lume-base SerializerBase\n            abstract base class.\n\n    \"\"\"\n    path = os.path.abspath(filepath)\n    content = serializer.deserialize(path)\n    return content\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.local.LocalFilesystem.write","title":"<code>write(filepath, object, serializer, create_dir=False)</code>","text":"<p>Write a file to the local filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> required <code>object</code> <code>Any</code> <p>Object to serialize</p> required <code>serializer</code> <code>SerializerBase</code> <p>Implementation of lume-base SerializerBase abstract base class</p> required <code>create_dir</code> <code>bool</code> <p>Whether to create directory in case not implemented</p> <code>False</code> Source code in <code>lume_services/services/files/filesystems/local.py</code> <pre><code>def write(\n    self,\n    filepath: str,\n    object: Any,\n    serializer: SerializerBase,\n    create_dir: bool = False,\n) -&gt; None:\n    \"\"\"Write a file to the local filesystem.\n\n    Args:\n        filepath (str):\n        object (Any): Object to serialize\n        serializer (SerializerBase): Implementation of lume-base SerializerBase\n            abstract base class\n        create_dir (bool): Whether to create directory in case not implemented\n\n    \"\"\"\n    path = os.path.abspath(filepath)\n    dir = os.path.dirname(path)\n\n    if create_dir and not self.dir_exists(dir):\n        self.create_dir(dir)\n\n    serializer.serialize(path, object)\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.mounted.MountedFilesystem","title":"<code>MountedFilesystem</code>","text":"<p>               Bases: <code>LocalFilesystem</code></p> <p>Handler for mounted filesystem. Modifies the LocalFilesystem to implements checks for mount path modifications. Container and container orchestration tools often allow the ability to provide an alias for a mounted directory. This handler accounts for the mount base and verifies that the file is within the path.</p> Source code in <code>lume_services/services/files/filesystems/mounted.py</code> <pre><code>class MountedFilesystem(LocalFilesystem):\n    \"\"\"Handler for mounted filesystem. Modifies the LocalFilesystem to implements\n    checks for mount path modifications. Container and container orchestration tools\n    often allow the ability to provide an alias for a mounted directory. This handler\n    accounts for the mount base and verifies that the file is within the path.\n\n    \"\"\"\n\n    identifier: str = \"mounted\"\n    mount_path: str\n    mount_alias: str\n    mount_type: _HostMountLiteral = \"DirectoryOrCreate\"\n\n    @validator(\"mount_path\", pre=True)\n    def validate_mount_path(cls, v, values):\n        mount_type = values.get(\"mount_type\")\n\n        if mount_type == \"DirectoryOrCreate\":\n            os.mkdir(v)\n\n        return v\n\n    def dir_exists(self, dir: str, create_dir: bool = False) -&gt; bool:\n        \"\"\"Check that a directory exists on the mounted filesystem.\n\n        Args:\n            dir (str): Path of directory\n            create_dir (bool): Whether to create directory if it does not exist\n\n        Returns:\n            bool\n\n        \"\"\"\n        dir = self._check_mounted_path(dir)\n        return super().dir_exists(dir, create_dir=create_dir)\n\n    def file_exists(self, filepath: str) -&gt; bool:\n        \"\"\"Check that a file exists on the mounted filesystem.\n\n        Args:\n            filepath (str): Path to file\n\n        Returns:\n            bool\n\n        \"\"\"\n\n        filepath = self._check_mounted_path(filepath)\n        return super().file_exists(filepath)\n\n    def create_dir(self, dir: str) -&gt; None:\n        \"\"\"Create a directory on the mounted filesystem.\n\n        Args:\n            dir (str): Directory path to create\n\n        \"\"\"\n        dir = self._check_mounted_path(dir)\n        super().create_dir(dir)\n\n    def read(self, filepath: str, serializer: SerializerBase) -&gt; Any:\n        \"\"\"Read file from the mounted filesystem.\n\n        Args:\n            filepath (str): Path of file to read\n            serializer (SerializerBase): Implementation of lume-base SerializerBase\n                abstract base class\n\n        \"\"\"\n        filepath = self._check_mounted_path(filepath)\n        return super().read(filepath, serializer)\n\n    def write(\n        self,\n        filepath: str,\n        object: Any,\n        serializer: SerializerBase,\n        create_dir: bool = False,\n    ) -&gt; None:\n        \"\"\"Write a file to the mounted filesystem.\n\n        Args:\n            filepath (str):\n            object (Any): Object to serialize\n            serializer (SerializerBase): Implementation of lume-base SerializerBase\n                abstract base class\n            create_dir (bool): Whether to create directory in case not implemented\n\n        \"\"\"\n        filepath = self._check_mounted_path(filepath)\n\n        super().write(filepath, object, serializer, create_dir=create_dir)\n\n    def _check_mounted_path(self, path: str):\n        \"\"\"Checks that the path exists inside the mount point relative to mount path or\n            alias.\n\n        Args:\n            path (str): Path to validate\n\n        \"\"\"\n        print(path)\n\n        if self.mount_alias in path:\n            return path\n\n        elif self.mount_path in path:\n            return path.replace(self.mount_path, self.mount_alias)\n\n        else:\n            raise PathNotInMount(\n                self.identifier, path, self.mount_path, self.mount_alias\n            )\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.mounted.MountedFilesystem.create_dir","title":"<code>create_dir(dir)</code>","text":"<p>Create a directory on the mounted filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Directory path to create</p> required Source code in <code>lume_services/services/files/filesystems/mounted.py</code> <pre><code>def create_dir(self, dir: str) -&gt; None:\n    \"\"\"Create a directory on the mounted filesystem.\n\n    Args:\n        dir (str): Directory path to create\n\n    \"\"\"\n    dir = self._check_mounted_path(dir)\n    super().create_dir(dir)\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.mounted.MountedFilesystem.dir_exists","title":"<code>dir_exists(dir, create_dir=False)</code>","text":"<p>Check that a directory exists on the mounted filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Path of directory</p> required <code>create_dir</code> <code>bool</code> <p>Whether to create directory if it does not exist</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>bool</p> Source code in <code>lume_services/services/files/filesystems/mounted.py</code> <pre><code>def dir_exists(self, dir: str, create_dir: bool = False) -&gt; bool:\n    \"\"\"Check that a directory exists on the mounted filesystem.\n\n    Args:\n        dir (str): Path of directory\n        create_dir (bool): Whether to create directory if it does not exist\n\n    Returns:\n        bool\n\n    \"\"\"\n    dir = self._check_mounted_path(dir)\n    return super().dir_exists(dir, create_dir=create_dir)\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.mounted.MountedFilesystem.file_exists","title":"<code>file_exists(filepath)</code>","text":"<p>Check that a file exists on the mounted filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to file</p> required <p>Returns:</p> Type Description <code>bool</code> <p>bool</p> Source code in <code>lume_services/services/files/filesystems/mounted.py</code> <pre><code>def file_exists(self, filepath: str) -&gt; bool:\n    \"\"\"Check that a file exists on the mounted filesystem.\n\n    Args:\n        filepath (str): Path to file\n\n    Returns:\n        bool\n\n    \"\"\"\n\n    filepath = self._check_mounted_path(filepath)\n    return super().file_exists(filepath)\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.mounted.MountedFilesystem.read","title":"<code>read(filepath, serializer)</code>","text":"<p>Read file from the mounted filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path of file to read</p> required <code>serializer</code> <code>SerializerBase</code> <p>Implementation of lume-base SerializerBase abstract base class</p> required Source code in <code>lume_services/services/files/filesystems/mounted.py</code> <pre><code>def read(self, filepath: str, serializer: SerializerBase) -&gt; Any:\n    \"\"\"Read file from the mounted filesystem.\n\n    Args:\n        filepath (str): Path of file to read\n        serializer (SerializerBase): Implementation of lume-base SerializerBase\n            abstract base class\n\n    \"\"\"\n    filepath = self._check_mounted_path(filepath)\n    return super().read(filepath, serializer)\n</code></pre>"},{"location":"api/services/files/filesystems/#lume_services.services.files.filesystems.mounted.MountedFilesystem.write","title":"<code>write(filepath, object, serializer, create_dir=False)</code>","text":"<p>Write a file to the mounted filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> required <code>object</code> <code>Any</code> <p>Object to serialize</p> required <code>serializer</code> <code>SerializerBase</code> <p>Implementation of lume-base SerializerBase abstract base class</p> required <code>create_dir</code> <code>bool</code> <p>Whether to create directory in case not implemented</p> <code>False</code> Source code in <code>lume_services/services/files/filesystems/mounted.py</code> <pre><code>def write(\n    self,\n    filepath: str,\n    object: Any,\n    serializer: SerializerBase,\n    create_dir: bool = False,\n) -&gt; None:\n    \"\"\"Write a file to the mounted filesystem.\n\n    Args:\n        filepath (str):\n        object (Any): Object to serialize\n        serializer (SerializerBase): Implementation of lume-base SerializerBase\n            abstract base class\n        create_dir (bool): Whether to create directory in case not implemented\n\n    \"\"\"\n    filepath = self._check_mounted_path(filepath)\n\n    super().write(filepath, object, serializer, create_dir=create_dir)\n</code></pre>"},{"location":"api/services/models/db/","title":"Database","text":""},{"location":"api/services/models/db/#lume_services.services.models.db.db.ConnectionConfig","title":"<code>ConnectionConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for creating sqlalchemy engine.</p> <p>Parameters:</p> Name Type Description Default <code>pool_size</code> <code>int</code> <p>Number of connections to maintain in the connection pool. Establishing connections is expensive and maintaining multiple connections in a pool allows for availability.</p> required <code>pool_pre_ping</code> <code>bool</code> <p>Performs liveliness check and expires all existing connections if the database is unreachable.</p> required Source code in <code>lume_services/services/models/db/db.py</code> <pre><code>class ConnectionConfig(BaseModel):\n    \"\"\"Configuration for creating sqlalchemy engine.\n\n    Args:\n        pool_size (int): Number of connections to maintain in the connection pool.\n            Establishing connections is expensive and maintaining multiple connections\n            in a pool allows for availability.\n        pool_pre_ping (bool): Performs liveliness check and expires all existing\n            connections if the database is unreachable.\n\n    \"\"\"\n\n    pool_size: Optional[int]\n    pool_pre_ping: bool = True\n</code></pre>"},{"location":"api/services/models/db/#lume_services.services.models.db.db.ModelDB","title":"<code>ModelDB</code>","text":"<p>DBService client responsible for handling connections to the model database.</p> Source code in <code>lume_services/services/models/db/db.py</code> <pre><code>class ModelDB:\n    \"\"\"DBService client responsible for handling connections to the model database.\"\"\"\n\n    def __init__(self, config: ModelDBConfig):\n        \"\"\"Initialize client service.\n\n        Args:\n            config (ModelDBConfig): Connection configuration.\n\n        \"\"\"\n        self.config = config\n        self._create_engine()\n\n    def _create_engine(self) -&gt; None:\n        \"\"\"Create sqlalchemy engine using uri.\"\"\"\n        self._pid = os.getpid()\n\n        # since using a context manager, must have context-local managed vars\n        self._connection = ContextVar(\"connection\", default=None)\n\n        self.engine = create_engine(\n            f\"{self.config.dialect_str}://{self.config.user}:%s@{self.config.host}:\\\n                {self.config.port}/{self.config.database}\"\n            % quote_plus(self.config.password.get_secret_value()),\n            **self.config.connection.dict(exclude_none=True),\n        )\n\n        # sessionmaker for orm operations\n        self._sessionmaker = sessionmaker(bind=self.engine)\n\n    def _connect(self) -&gt; Connection:\n        \"\"\"Establish connection and set _connection.\"\"\"\n        cxn = self.engine.connect()\n        self._connection.set(cxn)\n\n        return cxn\n\n    def _check_mp(self) -&gt; None:\n        \"\"\"Check for multiprocessing. If PID is different that object PID, create new\n        engine connection.\n\n        \"\"\"\n\n        if os.getpid() != self._pid:\n            self._create_engine()\n\n    @property\n    def _currect_connection(self) -&gt; Connection:\n        \"\"\"Getter for current connection\"\"\"\n\n        return self._connection.get()\n\n    @contextmanager\n    def connection(self) -&gt; Connection:\n        \"\"\"Context manager for operations. Will clean up connections on exit of\n        scope.\n\n        \"\"\"\n\n        self._check_mp()\n\n        # get connection\n        cxn = self._connection.get()\n\n        if cxn is None:\n            cxn = self._connect()\n            cleanup = True\n\n        else:\n            cleanup = False\n\n        try:\n            yield cxn\n\n        finally:\n            if cleanup:\n                cxn = self._connection.get()\n\n                if cxn:\n                    cxn.close()\n                    self._connection.set(None)\n\n    def session(self) -&gt; Session:\n        \"\"\"Establishes Session with active connection.\n\n        Note: Setting expire_on_commit to False allows us to access objects\n        after session closing.\n\n        \"\"\"\n        logger.debug(\"ModelDB creating session.\")\n        with self.connection():\n            session = self._sessionmaker()\n            logger.debug(\"ModelDB session created.\")\n            session.expire_on_commit = False\n            return session\n\n    def execute(self, sql) -&gt; list:\n        \"\"\"Execute sql inside a managed session.\n\n        Args:\n            sql (sqlalchemy.sql.base.Executable): SQL query to execute.\n\n        Results:\n            list: Results of query operation\n\n        \"\"\"\n        logger.info(\"ModelDB executing: %s\", str(sql))\n        with self.session() as session:\n\n            res = session.execute(sql)\n            session.commit()\n\n        logger.info(\"ModelDB executed: %s\", str(sql))\n\n        return res\n\n    def select(self, sql: Select) -&gt; list:\n        \"\"\"Execute sql query inside a managed session.\n\n        Args:\n            sql (Select): Selection query to execute.\n\n        Results:\n            list: Results of selection operation\n\n        \"\"\"\n        logger.info(\"ModelDB selecting: %s\", str(sql))\n        with self.session() as session:\n\n            res = session.execute(sql).scalars().all()\n            session.commit()\n\n        return res\n\n    def insert(self, sql: Insert):\n        \"\"\"Execute and insert operation inside a managed session.\n\n        Args:\n            sql (Insert): Sqlalchemy insert operation\n\n        Returns:\n            Union[str, int]: primary key returned from insert operation\n\n        \"\"\"\n        logger.info(\"ModelDB inserting: %s\", str(sql))\n        with self.session() as session:\n\n            res = session.execute(sql)\n            session.commit()\n\n        logger.info(\"Sucessfully executed: %s\", str(sql))\n\n        return res.inserted_primary_key\n\n    def insert_many(self, sql: List[Insert]) -&gt; List[Union[str, int]]:\n        \"\"\"Execute many inserts within a managed session.\n\n        Args:\n            sql (List[Insert]): Execute a sqlalchemy insert operation\n\n        Returns:\n            List[Union[str, int]]: List of primary keys returned from insert operation\n\n        \"\"\"\n        logger.info(\"ModelDB inserting many: %s\", [str(statement) for statement in sql])\n        with self.session() as session:\n\n            results = []\n\n            for stmt in sql:\n                res = session.execute(stmt)\n                results.append(res)\n\n            session.commit()\n\n        logger.info(\"Sucessfully executed: %s\", [str(statement) for statement in sql])\n\n        return [res.inserted_primary_key for res in results]\n\n    @classmethod\n    def from_config_init(cls, **kwargs) -&gt; \"ModelDB\":\n        \"\"\"Initialize database handler from ModelDBConfig kwargs.\"\"\"\n        config = ModelDBConfig(**kwargs)\n        return cls(config=config)\n</code></pre>"},{"location":"api/services/models/db/#lume_services.services.models.db.db.ModelDB.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize client service.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelDBConfig</code> <p>Connection configuration.</p> required Source code in <code>lume_services/services/models/db/db.py</code> <pre><code>def __init__(self, config: ModelDBConfig):\n    \"\"\"Initialize client service.\n\n    Args:\n        config (ModelDBConfig): Connection configuration.\n\n    \"\"\"\n    self.config = config\n    self._create_engine()\n</code></pre>"},{"location":"api/services/models/db/#lume_services.services.models.db.db.ModelDB.connection","title":"<code>connection()</code>","text":"<p>Context manager for operations. Will clean up connections on exit of scope.</p> Source code in <code>lume_services/services/models/db/db.py</code> <pre><code>@contextmanager\ndef connection(self) -&gt; Connection:\n    \"\"\"Context manager for operations. Will clean up connections on exit of\n    scope.\n\n    \"\"\"\n\n    self._check_mp()\n\n    # get connection\n    cxn = self._connection.get()\n\n    if cxn is None:\n        cxn = self._connect()\n        cleanup = True\n\n    else:\n        cleanup = False\n\n    try:\n        yield cxn\n\n    finally:\n        if cleanup:\n            cxn = self._connection.get()\n\n            if cxn:\n                cxn.close()\n                self._connection.set(None)\n</code></pre>"},{"location":"api/services/models/db/#lume_services.services.models.db.db.ModelDB.execute","title":"<code>execute(sql)</code>","text":"<p>Execute sql inside a managed session.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>Executable</code> <p>SQL query to execute.</p> required Results <p>list: Results of query operation</p> Source code in <code>lume_services/services/models/db/db.py</code> <pre><code>def execute(self, sql) -&gt; list:\n    \"\"\"Execute sql inside a managed session.\n\n    Args:\n        sql (sqlalchemy.sql.base.Executable): SQL query to execute.\n\n    Results:\n        list: Results of query operation\n\n    \"\"\"\n    logger.info(\"ModelDB executing: %s\", str(sql))\n    with self.session() as session:\n\n        res = session.execute(sql)\n        session.commit()\n\n    logger.info(\"ModelDB executed: %s\", str(sql))\n\n    return res\n</code></pre>"},{"location":"api/services/models/db/#lume_services.services.models.db.db.ModelDB.from_config_init","title":"<code>from_config_init(**kwargs)</code>  <code>classmethod</code>","text":"<p>Initialize database handler from ModelDBConfig kwargs.</p> Source code in <code>lume_services/services/models/db/db.py</code> <pre><code>@classmethod\ndef from_config_init(cls, **kwargs) -&gt; \"ModelDB\":\n    \"\"\"Initialize database handler from ModelDBConfig kwargs.\"\"\"\n    config = ModelDBConfig(**kwargs)\n    return cls(config=config)\n</code></pre>"},{"location":"api/services/models/db/#lume_services.services.models.db.db.ModelDB.insert","title":"<code>insert(sql)</code>","text":"<p>Execute and insert operation inside a managed session.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>Insert</code> <p>Sqlalchemy insert operation</p> required <p>Returns:</p> Type Description <p>Union[str, int]: primary key returned from insert operation</p> Source code in <code>lume_services/services/models/db/db.py</code> <pre><code>def insert(self, sql: Insert):\n    \"\"\"Execute and insert operation inside a managed session.\n\n    Args:\n        sql (Insert): Sqlalchemy insert operation\n\n    Returns:\n        Union[str, int]: primary key returned from insert operation\n\n    \"\"\"\n    logger.info(\"ModelDB inserting: %s\", str(sql))\n    with self.session() as session:\n\n        res = session.execute(sql)\n        session.commit()\n\n    logger.info(\"Sucessfully executed: %s\", str(sql))\n\n    return res.inserted_primary_key\n</code></pre>"},{"location":"api/services/models/db/#lume_services.services.models.db.db.ModelDB.insert_many","title":"<code>insert_many(sql)</code>","text":"<p>Execute many inserts within a managed session.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>List[Insert]</code> <p>Execute a sqlalchemy insert operation</p> required <p>Returns:</p> Type Description <code>List[Union[str, int]]</code> <p>List[Union[str, int]]: List of primary keys returned from insert operation</p> Source code in <code>lume_services/services/models/db/db.py</code> <pre><code>def insert_many(self, sql: List[Insert]) -&gt; List[Union[str, int]]:\n    \"\"\"Execute many inserts within a managed session.\n\n    Args:\n        sql (List[Insert]): Execute a sqlalchemy insert operation\n\n    Returns:\n        List[Union[str, int]]: List of primary keys returned from insert operation\n\n    \"\"\"\n    logger.info(\"ModelDB inserting many: %s\", [str(statement) for statement in sql])\n    with self.session() as session:\n\n        results = []\n\n        for stmt in sql:\n            res = session.execute(stmt)\n            results.append(res)\n\n        session.commit()\n\n    logger.info(\"Sucessfully executed: %s\", [str(statement) for statement in sql])\n\n    return [res.inserted_primary_key for res in results]\n</code></pre>"},{"location":"api/services/models/db/#lume_services.services.models.db.db.ModelDB.select","title":"<code>select(sql)</code>","text":"<p>Execute sql query inside a managed session.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>Select</code> <p>Selection query to execute.</p> required Results <p>list: Results of selection operation</p> Source code in <code>lume_services/services/models/db/db.py</code> <pre><code>def select(self, sql: Select) -&gt; list:\n    \"\"\"Execute sql query inside a managed session.\n\n    Args:\n        sql (Select): Selection query to execute.\n\n    Results:\n        list: Results of selection operation\n\n    \"\"\"\n    logger.info(\"ModelDB selecting: %s\", str(sql))\n    with self.session() as session:\n\n        res = session.execute(sql).scalars().all()\n        session.commit()\n\n    return res\n</code></pre>"},{"location":"api/services/models/db/#lume_services.services.models.db.db.ModelDB.session","title":"<code>session()</code>","text":"<p>Establishes Session with active connection.</p> <p>Note: Setting expire_on_commit to False allows us to access objects after session closing.</p> Source code in <code>lume_services/services/models/db/db.py</code> <pre><code>def session(self) -&gt; Session:\n    \"\"\"Establishes Session with active connection.\n\n    Note: Setting expire_on_commit to False allows us to access objects\n    after session closing.\n\n    \"\"\"\n    logger.debug(\"ModelDB creating session.\")\n    with self.connection():\n        session = self._sessionmaker()\n        logger.debug(\"ModelDB session created.\")\n        session.expire_on_commit = False\n        return session\n</code></pre>"},{"location":"api/services/models/db/#lume_services.services.models.db.db.ModelDBConfig","title":"<code>ModelDBConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for SQL connection using sqlalchemy.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>Host of SQL server.</p> required <code>port</code> <code>str</code> <p>Port of SQL server on host.</p> required <code>user</code> <code>str</code> <p>User for connecting to SQL server.</p> required <code>password</code> <code>SecretStr</code> <p>Password for auth.</p> required <code>database</code> <code>str</code> <p>Name of database.</p> required <code>connection</code> <code>ConnectionConfig</code> <p>Configuration options for creating sqlalchemy engine.</p> required Source code in <code>lume_services/services/models/db/db.py</code> <pre><code>class ModelDBConfig(BaseModel):\n    \"\"\"Configuration for SQL connection using sqlalchemy.\n\n    Args:\n        host (str): Host of SQL server.\n        port (str): Port of SQL server on host.\n        user (str): User for connecting to SQL server.\n        password (SecretStr): Password for auth.\n        database (str): Name of database.\n        connection (ConnectionConfig): Configuration options for creating sqlalchemy\n            engine.\n\n    \"\"\"\n\n    host: str\n    port: int\n    user: str\n    password: SecretStr = Field(exclude=True)\n    database: str\n    connection: ConnectionConfig = ConnectionConfig()\n    dialect_str: str = \"mysql+pymysql\"\n</code></pre>"},{"location":"api/services/models/models/","title":"Model Database Service","text":""},{"location":"api/services/models/models/#lume_services.services.models.service.ModelDBService","title":"<code>ModelDBService</code>","text":"Source code in <code>lume_services/services/models/service.py</code> <pre><code>class ModelDBService:\n    def __init__(self, model_db: ModelDB):\n        self._model_db = model_db\n        self._model_registry = {}\n\n    @validate_kwargs_exist(Model)\n    def store_model(\n        self,\n        author: str,\n        laboratory: str,\n        facility: str,\n        beampath: str,\n        description: str,\n    ) -&gt; int:\n        \"\"\"Store a model.\n\n        Returns:\n            int: ID of inserted model\n        \"\"\"\n\n        # store in db\n        insert_stmt = insert(Model).values(\n            author=author,\n            laboratory=laboratory,\n            facility=facility,\n            beampath=beampath,\n            description=description,\n        )\n\n        result = self._model_db.insert(insert_stmt)\n\n        if len(result):\n            return result[0]\n\n        else:\n            return None\n\n    def store_deployment(\n        self,\n        model_id: int,\n        version: str,\n        source: str,\n        sha256: str,\n        image: str,\n        package_import_name: str,\n        is_live: bool = False,\n        asset_dir=None,\n    ) -&gt; int:\n        \"\"\"Store a deployment.\n\n        Args:\n            model_id (int): Associated model id.\n            version (str): Version of model.\n            source (str): Source of deployment code.\n            sha256 (str): Hash of source\n            image (str): Container image to be used with this deployment.\n            is_live (bool=False): Whether deployment is live.\n            asset_dir (str): Directory for assets stored on filesystem.\n            package_import_name (str): Name of package\n\n        Returns:\n            int: ID of inserted deployment id\n        \"\"\"\n\n        # store in db\n        insert_stmt = insert(Deployment).values(\n            model_id=model_id,\n            version=version,\n            source=source,\n            sha256=sha256,\n            image=image,\n            is_live=is_live,\n            asset_dir=asset_dir,\n            package_import_name=package_import_name,\n        )\n\n        result = self._model_db.insert(insert_stmt)\n\n        if len(result):\n            return result[0]\n\n        else:\n            return None\n\n    def store_project(self, project_name: str, description: str) -&gt; str:\n        \"\"\"Store a project.\n\n        Args:\n            project_name (str): Name of project (as created in Prefect).\n            decription (str): Short description of project.\n\n        Returns:\n            str: Inserted project name\n        \"\"\"\n\n        insert_stmt = insert(Project).values(\n            project_name=project_name, description=description\n        )\n\n        # store in db\n        result = self._model_db.insert(insert_stmt)\n\n        # Return inserted project name\n        if len(result):\n            return result[0]\n\n        else:\n            return None\n\n    def store_flow(\n        self, deployment_id: int, flow_id: str, flow_name: str, project_name: str\n    ) -&gt; str:\n        \"\"\"Store a flow in the model database.\n\n        Args:\n            deployment_id (int): ID of deployment associated with the flow.\n            flow_id (str): ID of flow (generated by Prefect).\n            flow_name (str): Name of flow (same as used to register with Prefect).\n            project_name (str): Name of project (same as used to register with Prefect).\n\n        Returns:\n            str: Inserted flow id\n        \"\"\"\n\n        insert_stmt = insert(Flow).values(\n            flow_id=flow_id,\n            deployment_id=deployment_id,\n            flow_name=flow_name,\n            project_name=project_name,\n        )\n\n        results = self._model_db.insert(insert_stmt)\n\n        # flow_id is result of first insert\n        if len(results):\n            return results[0]\n\n        else:\n            return None\n\n    @validate_kwargs_exist(Model)\n    def get_model(self, **kwargs) -&gt; Model:\n        \"\"\"Get a model from criteria\n\n        Returns:\n            Model\n        \"\"\"\n        # execute query\n        query = select(Model).filter_by(**kwargs)\n\n        result = self._model_db.select(query)\n\n        if len(result):\n            if len(result) &gt; 1:\n                # formatted this way to eventually move towards interpolated schema\n                logger.warning(\n                    \"Multiple models returned from query. get_model is returning the \\\n                        first result with %s %s\",\n                    \"model_id\",\n                    result[0].model_id,\n                )\n\n            return result[0]\n\n        else:\n            raise ModelNotFoundError(query)\n\n    @validate_kwargs_exist(Deployment)\n    def get_deployment(self, **kwargs) -&gt; Deployment:\n        \"\"\"Get a deployment based on criteria\n\n        Returns:\n            Deployment\n        \"\"\"\n\n        query = select(Deployment).filter_by(**kwargs)\n        result = self._model_db.select(query)\n\n        if len(result):\n            if len(result) &gt; 1:\n                # formatted this way to eventually move towards interpolated schema\n                logger.warning(\n                    \"Multiple deployments returned from query. get_deployment is \\\n                        returning the first result with %s %s\",\n                    \"deployment_id\",\n                    result[0].deployment_id,\n                )\n\n            return result[0]\n\n        else:\n            raise DeploymentNotFoundError(query)\n\n    def get_deployments(self, **kwargs) -&gt; List[Deployment]:\n        \"\"\"Get a set of deployments based on criteria\n\n        Returns:\n            List[Deployment]\n        \"\"\"\n\n        query = select(Deployment).filter_by(**kwargs)\n        result = self._model_db.select(query)\n\n        if len(result):\n            return result\n\n        else:\n            raise DeploymentNotFoundError(query)\n\n        ...\n\n    @validate_kwargs_exist(Deployment)\n    def get_latest_deployment(self, **kwargs) -&gt; Deployment:\n        \"\"\"Get the latest deployment\n\n        Returns:\n            Deployment\n\n        raises:\n            ValueError: Passed kwarg not in Project schema\n        \"\"\"\n\n        query = (\n            select(Deployment)\n            .filter_by(**kwargs)\n            .order_by(desc(Deployment.deploy_date))\n        )\n        result = self._model_db.select(query)\n\n        if len(result):\n            return result[0]\n\n        else:\n            raise DeploymentNotFoundError(query)\n\n    @validate_kwargs_exist(Project)\n    def get_project(self, **kwargs) -&gt; Project:\n        \"\"\"Get a single Project\n\n        Returns:\n            Project\n\n        raises:\n            ValueError: Passed kwarg not in Project schema\n        \"\"\"\n\n        # execute query\n        query = select(Project).filter_by(**kwargs)\n        result = self._model_db.select(query)\n\n        if len(result):\n            if len(result) &gt; 1:\n                # formatted this way to eventually move towards interpolated schema\n                logger.warning(\n                    \"Multiple projects returned from query. get_project is returning \\\n                        the first result with %s %s\",\n                    \"name\",\n                    result[0].project_name,\n                )\n\n            return result[0]\n\n        else:\n            raise ProjectNotFoundError(query)\n\n    @validate_kwargs_exist(Flow)\n    def get_flow(self, **kwargs) -&gt; Flow:\n        \"\"\"Get a flow from criteria\n\n        Returns:\n            Flow: Select a flow from the database.\n\n        raises:\n            ValueError: Passed kwarg not in Project schema\n        \"\"\"\n\n        query = select(Flow).filter_by(**kwargs)\n        result = self._model_db.select(query)\n\n        if len(result):\n            if len(result) &gt; 1:\n                # formatted this way to eventually move towards interpolated schema\n                logger.warning(\n                    \"Multiple flows returned from query. get_flow is returning the \\\n                        first result with %s %s\",\n                    \"flow_id\",\n                    result[0].flow_id,\n                )\n\n            return result[0]\n\n        else:\n            raise FlowNotFoundError(query)\n\n    @validate_kwargs_exist(FlowOfFlows)\n    def get_flow_of_flows(self, **kwargs) -&gt; Flow:\n        \"\"\"Get a flow from criteria\n\n        Returns:\n            FlowOfFlows: Select a flow of flows from the database.\n\n        raises:\n            ValueError: Passed kwarg not in Project schema\n        \"\"\"\n\n        query = select(FlowOfFlows).filter_by(**kwargs)\n        result = self._model_db.select(query)\n\n        if len(result):\n            return [res.flow for res in result]\n\n        else:\n            raise FlowOfFlowsNotFoundError(query)\n\n    def apply_schema(self) -&gt; None:\n        \"\"\"Applies database schema to connected service.\"\"\"\n\n        Base.metadata.create_all(self._model_db.engine)\n</code></pre>"},{"location":"api/services/models/models/#lume_services.services.models.service.ModelDBService.apply_schema","title":"<code>apply_schema()</code>","text":"<p>Applies database schema to connected service.</p> Source code in <code>lume_services/services/models/service.py</code> <pre><code>def apply_schema(self) -&gt; None:\n    \"\"\"Applies database schema to connected service.\"\"\"\n\n    Base.metadata.create_all(self._model_db.engine)\n</code></pre>"},{"location":"api/services/models/models/#lume_services.services.models.service.ModelDBService.get_deployment","title":"<code>get_deployment(**kwargs)</code>","text":"<p>Get a deployment based on criteria</p> <p>Returns:</p> Type Description <code>Deployment</code> <p>Deployment</p> Source code in <code>lume_services/services/models/service.py</code> <pre><code>@validate_kwargs_exist(Deployment)\ndef get_deployment(self, **kwargs) -&gt; Deployment:\n    \"\"\"Get a deployment based on criteria\n\n    Returns:\n        Deployment\n    \"\"\"\n\n    query = select(Deployment).filter_by(**kwargs)\n    result = self._model_db.select(query)\n\n    if len(result):\n        if len(result) &gt; 1:\n            # formatted this way to eventually move towards interpolated schema\n            logger.warning(\n                \"Multiple deployments returned from query. get_deployment is \\\n                    returning the first result with %s %s\",\n                \"deployment_id\",\n                result[0].deployment_id,\n            )\n\n        return result[0]\n\n    else:\n        raise DeploymentNotFoundError(query)\n</code></pre>"},{"location":"api/services/models/models/#lume_services.services.models.service.ModelDBService.get_deployments","title":"<code>get_deployments(**kwargs)</code>","text":"<p>Get a set of deployments based on criteria</p> <p>Returns:</p> Type Description <code>List[Deployment]</code> <p>List[Deployment]</p> Source code in <code>lume_services/services/models/service.py</code> <pre><code>def get_deployments(self, **kwargs) -&gt; List[Deployment]:\n    \"\"\"Get a set of deployments based on criteria\n\n    Returns:\n        List[Deployment]\n    \"\"\"\n\n    query = select(Deployment).filter_by(**kwargs)\n    result = self._model_db.select(query)\n\n    if len(result):\n        return result\n\n    else:\n        raise DeploymentNotFoundError(query)\n\n    ...\n</code></pre>"},{"location":"api/services/models/models/#lume_services.services.models.service.ModelDBService.get_flow","title":"<code>get_flow(**kwargs)</code>","text":"<p>Get a flow from criteria</p> <p>Returns:</p> Name Type Description <code>Flow</code> <code>Flow</code> <p>Select a flow from the database.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Passed kwarg not in Project schema</p> Source code in <code>lume_services/services/models/service.py</code> <pre><code>@validate_kwargs_exist(Flow)\ndef get_flow(self, **kwargs) -&gt; Flow:\n    \"\"\"Get a flow from criteria\n\n    Returns:\n        Flow: Select a flow from the database.\n\n    raises:\n        ValueError: Passed kwarg not in Project schema\n    \"\"\"\n\n    query = select(Flow).filter_by(**kwargs)\n    result = self._model_db.select(query)\n\n    if len(result):\n        if len(result) &gt; 1:\n            # formatted this way to eventually move towards interpolated schema\n            logger.warning(\n                \"Multiple flows returned from query. get_flow is returning the \\\n                    first result with %s %s\",\n                \"flow_id\",\n                result[0].flow_id,\n            )\n\n        return result[0]\n\n    else:\n        raise FlowNotFoundError(query)\n</code></pre>"},{"location":"api/services/models/models/#lume_services.services.models.service.ModelDBService.get_flow_of_flows","title":"<code>get_flow_of_flows(**kwargs)</code>","text":"<p>Get a flow from criteria</p> <p>Returns:</p> Name Type Description <code>FlowOfFlows</code> <code>Flow</code> <p>Select a flow of flows from the database.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Passed kwarg not in Project schema</p> Source code in <code>lume_services/services/models/service.py</code> <pre><code>@validate_kwargs_exist(FlowOfFlows)\ndef get_flow_of_flows(self, **kwargs) -&gt; Flow:\n    \"\"\"Get a flow from criteria\n\n    Returns:\n        FlowOfFlows: Select a flow of flows from the database.\n\n    raises:\n        ValueError: Passed kwarg not in Project schema\n    \"\"\"\n\n    query = select(FlowOfFlows).filter_by(**kwargs)\n    result = self._model_db.select(query)\n\n    if len(result):\n        return [res.flow for res in result]\n\n    else:\n        raise FlowOfFlowsNotFoundError(query)\n</code></pre>"},{"location":"api/services/models/models/#lume_services.services.models.service.ModelDBService.get_latest_deployment","title":"<code>get_latest_deployment(**kwargs)</code>","text":"<p>Get the latest deployment</p> <p>Returns:</p> Type Description <code>Deployment</code> <p>Deployment</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Passed kwarg not in Project schema</p> Source code in <code>lume_services/services/models/service.py</code> <pre><code>@validate_kwargs_exist(Deployment)\ndef get_latest_deployment(self, **kwargs) -&gt; Deployment:\n    \"\"\"Get the latest deployment\n\n    Returns:\n        Deployment\n\n    raises:\n        ValueError: Passed kwarg not in Project schema\n    \"\"\"\n\n    query = (\n        select(Deployment)\n        .filter_by(**kwargs)\n        .order_by(desc(Deployment.deploy_date))\n    )\n    result = self._model_db.select(query)\n\n    if len(result):\n        return result[0]\n\n    else:\n        raise DeploymentNotFoundError(query)\n</code></pre>"},{"location":"api/services/models/models/#lume_services.services.models.service.ModelDBService.get_model","title":"<code>get_model(**kwargs)</code>","text":"<p>Get a model from criteria</p> <p>Returns:</p> Type Description <code>Model</code> <p>Model</p> Source code in <code>lume_services/services/models/service.py</code> <pre><code>@validate_kwargs_exist(Model)\ndef get_model(self, **kwargs) -&gt; Model:\n    \"\"\"Get a model from criteria\n\n    Returns:\n        Model\n    \"\"\"\n    # execute query\n    query = select(Model).filter_by(**kwargs)\n\n    result = self._model_db.select(query)\n\n    if len(result):\n        if len(result) &gt; 1:\n            # formatted this way to eventually move towards interpolated schema\n            logger.warning(\n                \"Multiple models returned from query. get_model is returning the \\\n                    first result with %s %s\",\n                \"model_id\",\n                result[0].model_id,\n            )\n\n        return result[0]\n\n    else:\n        raise ModelNotFoundError(query)\n</code></pre>"},{"location":"api/services/models/models/#lume_services.services.models.service.ModelDBService.get_project","title":"<code>get_project(**kwargs)</code>","text":"<p>Get a single Project</p> <p>Returns:</p> Type Description <code>Project</code> <p>Project</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Passed kwarg not in Project schema</p> Source code in <code>lume_services/services/models/service.py</code> <pre><code>@validate_kwargs_exist(Project)\ndef get_project(self, **kwargs) -&gt; Project:\n    \"\"\"Get a single Project\n\n    Returns:\n        Project\n\n    raises:\n        ValueError: Passed kwarg not in Project schema\n    \"\"\"\n\n    # execute query\n    query = select(Project).filter_by(**kwargs)\n    result = self._model_db.select(query)\n\n    if len(result):\n        if len(result) &gt; 1:\n            # formatted this way to eventually move towards interpolated schema\n            logger.warning(\n                \"Multiple projects returned from query. get_project is returning \\\n                    the first result with %s %s\",\n                \"name\",\n                result[0].project_name,\n            )\n\n        return result[0]\n\n    else:\n        raise ProjectNotFoundError(query)\n</code></pre>"},{"location":"api/services/models/models/#lume_services.services.models.service.ModelDBService.store_deployment","title":"<code>store_deployment(model_id, version, source, sha256, image, package_import_name, is_live=False, asset_dir=None)</code>","text":"<p>Store a deployment.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>int</code> <p>Associated model id.</p> required <code>version</code> <code>str</code> <p>Version of model.</p> required <code>source</code> <code>str</code> <p>Source of deployment code.</p> required <code>sha256</code> <code>str</code> <p>Hash of source</p> required <code>image</code> <code>str</code> <p>Container image to be used with this deployment.</p> required <code>is_live</code> <code>bool=False</code> <p>Whether deployment is live.</p> <code>False</code> <code>asset_dir</code> <code>str</code> <p>Directory for assets stored on filesystem.</p> <code>None</code> <code>package_import_name</code> <code>str</code> <p>Name of package</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>ID of inserted deployment id</p> Source code in <code>lume_services/services/models/service.py</code> <pre><code>def store_deployment(\n    self,\n    model_id: int,\n    version: str,\n    source: str,\n    sha256: str,\n    image: str,\n    package_import_name: str,\n    is_live: bool = False,\n    asset_dir=None,\n) -&gt; int:\n    \"\"\"Store a deployment.\n\n    Args:\n        model_id (int): Associated model id.\n        version (str): Version of model.\n        source (str): Source of deployment code.\n        sha256 (str): Hash of source\n        image (str): Container image to be used with this deployment.\n        is_live (bool=False): Whether deployment is live.\n        asset_dir (str): Directory for assets stored on filesystem.\n        package_import_name (str): Name of package\n\n    Returns:\n        int: ID of inserted deployment id\n    \"\"\"\n\n    # store in db\n    insert_stmt = insert(Deployment).values(\n        model_id=model_id,\n        version=version,\n        source=source,\n        sha256=sha256,\n        image=image,\n        is_live=is_live,\n        asset_dir=asset_dir,\n        package_import_name=package_import_name,\n    )\n\n    result = self._model_db.insert(insert_stmt)\n\n    if len(result):\n        return result[0]\n\n    else:\n        return None\n</code></pre>"},{"location":"api/services/models/models/#lume_services.services.models.service.ModelDBService.store_flow","title":"<code>store_flow(deployment_id, flow_id, flow_name, project_name)</code>","text":"<p>Store a flow in the model database.</p> <p>Parameters:</p> Name Type Description Default <code>deployment_id</code> <code>int</code> <p>ID of deployment associated with the flow.</p> required <code>flow_id</code> <code>str</code> <p>ID of flow (generated by Prefect).</p> required <code>flow_name</code> <code>str</code> <p>Name of flow (same as used to register with Prefect).</p> required <code>project_name</code> <code>str</code> <p>Name of project (same as used to register with Prefect).</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Inserted flow id</p> Source code in <code>lume_services/services/models/service.py</code> <pre><code>def store_flow(\n    self, deployment_id: int, flow_id: str, flow_name: str, project_name: str\n) -&gt; str:\n    \"\"\"Store a flow in the model database.\n\n    Args:\n        deployment_id (int): ID of deployment associated with the flow.\n        flow_id (str): ID of flow (generated by Prefect).\n        flow_name (str): Name of flow (same as used to register with Prefect).\n        project_name (str): Name of project (same as used to register with Prefect).\n\n    Returns:\n        str: Inserted flow id\n    \"\"\"\n\n    insert_stmt = insert(Flow).values(\n        flow_id=flow_id,\n        deployment_id=deployment_id,\n        flow_name=flow_name,\n        project_name=project_name,\n    )\n\n    results = self._model_db.insert(insert_stmt)\n\n    # flow_id is result of first insert\n    if len(results):\n        return results[0]\n\n    else:\n        return None\n</code></pre>"},{"location":"api/services/models/models/#lume_services.services.models.service.ModelDBService.store_model","title":"<code>store_model(author, laboratory, facility, beampath, description)</code>","text":"<p>Store a model.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>ID of inserted model</p> Source code in <code>lume_services/services/models/service.py</code> <pre><code>@validate_kwargs_exist(Model)\ndef store_model(\n    self,\n    author: str,\n    laboratory: str,\n    facility: str,\n    beampath: str,\n    description: str,\n) -&gt; int:\n    \"\"\"Store a model.\n\n    Returns:\n        int: ID of inserted model\n    \"\"\"\n\n    # store in db\n    insert_stmt = insert(Model).values(\n        author=author,\n        laboratory=laboratory,\n        facility=facility,\n        beampath=beampath,\n        description=description,\n    )\n\n    result = self._model_db.insert(insert_stmt)\n\n    if len(result):\n        return result[0]\n\n    else:\n        return None\n</code></pre>"},{"location":"api/services/models/models/#lume_services.services.models.service.ModelDBService.store_project","title":"<code>store_project(project_name, description)</code>","text":"<p>Store a project.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Name of project (as created in Prefect).</p> required <code>decription</code> <code>str</code> <p>Short description of project.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Inserted project name</p> Source code in <code>lume_services/services/models/service.py</code> <pre><code>def store_project(self, project_name: str, description: str) -&gt; str:\n    \"\"\"Store a project.\n\n    Args:\n        project_name (str): Name of project (as created in Prefect).\n        decription (str): Short description of project.\n\n    Returns:\n        str: Inserted project name\n    \"\"\"\n\n    insert_stmt = insert(Project).values(\n        project_name=project_name, description=description\n    )\n\n    # store in db\n    result = self._model_db.insert(insert_stmt)\n\n    # Return inserted project name\n    if len(result):\n        return result[0]\n\n    else:\n        return None\n</code></pre>"},{"location":"api/services/results/db/","title":"Database","text":""},{"location":"api/services/results/db/#lume_services.services.results.db.ResultsDB","title":"<code>ResultsDB</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Implementation of the database.</p> Source code in <code>lume_services/services/results/db.py</code> <pre><code>class ResultsDB(ABC):\n    \"\"\"Implementation of the database.\"\"\"\n\n    @abstractmethod\n    def __init__(self, db_config: ResultsDBConfig):\n        ...\n\n    @abstractmethod\n    def insert_one(self, item: dict, **kwargs) -&gt; str:\n        \"\"\"Insert document into the database.\n\n        Args:\n            item (dict): Dictionary representation of item\n\n        Returns:\n            str: Inserted item id\n\n        \"\"\"\n\n    @abstractmethod\n    def insert_many(self, items: List[dict], **kwargs) -&gt; List[str]:\n        \"\"\"Insert many documents into the database.\n\n        Args:\n            items (List[dict]): List of dictionary representations of items\n\n        Returns:\n            List[str]: List of interted ids\n\n        \"\"\"\n\n    @abstractmethod\n    def find(self, *, query: dict, fields: List[str] = None, **kwargs) -&gt; List[dict]:\n        \"\"\"Find a document based on a query.\n\n        Args:\n            query (dict): fields to query on\n            fields (List[str]): List of fields to return if any\n            **kwargs (dict): DB implementation specific fields\n\n        Returns:\n            List[dict]: List of dict reps of found items.\n\n        \"\"\"\n\n    @abstractmethod\n    def find_all(self, **kwargs) -&gt; List[dict]:\n        \"\"\"Find all documents for a collection\n\n        Returns:\n            List[dict]: List of result items represented as dict.\n        \"\"\"\n\n    @abstractmethod\n    def configure(self, **kwargs) -&gt; None:\n        \"\"\"Configure the results db service.\"\"\"\n</code></pre>"},{"location":"api/services/results/db/#lume_services.services.results.db.ResultsDB.configure","title":"<code>configure(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Configure the results db service.</p> Source code in <code>lume_services/services/results/db.py</code> <pre><code>@abstractmethod\ndef configure(self, **kwargs) -&gt; None:\n    \"\"\"Configure the results db service.\"\"\"\n</code></pre>"},{"location":"api/services/results/db/#lume_services.services.results.db.ResultsDB.find","title":"<code>find(*, query, fields=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Find a document based on a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>dict</code> <p>fields to query on</p> required <code>fields</code> <code>List[str]</code> <p>List of fields to return if any</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>DB implementation specific fields</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of dict reps of found items.</p> Source code in <code>lume_services/services/results/db.py</code> <pre><code>@abstractmethod\ndef find(self, *, query: dict, fields: List[str] = None, **kwargs) -&gt; List[dict]:\n    \"\"\"Find a document based on a query.\n\n    Args:\n        query (dict): fields to query on\n        fields (List[str]): List of fields to return if any\n        **kwargs (dict): DB implementation specific fields\n\n    Returns:\n        List[dict]: List of dict reps of found items.\n\n    \"\"\"\n</code></pre>"},{"location":"api/services/results/db/#lume_services.services.results.db.ResultsDB.find_all","title":"<code>find_all(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Find all documents for a collection</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of result items represented as dict.</p> Source code in <code>lume_services/services/results/db.py</code> <pre><code>@abstractmethod\ndef find_all(self, **kwargs) -&gt; List[dict]:\n    \"\"\"Find all documents for a collection\n\n    Returns:\n        List[dict]: List of result items represented as dict.\n    \"\"\"\n</code></pre>"},{"location":"api/services/results/db/#lume_services.services.results.db.ResultsDB.insert_many","title":"<code>insert_many(items, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Insert many documents into the database.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[dict]</code> <p>List of dictionary representations of items</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of interted ids</p> Source code in <code>lume_services/services/results/db.py</code> <pre><code>@abstractmethod\ndef insert_many(self, items: List[dict], **kwargs) -&gt; List[str]:\n    \"\"\"Insert many documents into the database.\n\n    Args:\n        items (List[dict]): List of dictionary representations of items\n\n    Returns:\n        List[str]: List of interted ids\n\n    \"\"\"\n</code></pre>"},{"location":"api/services/results/db/#lume_services.services.results.db.ResultsDB.insert_one","title":"<code>insert_one(item, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Insert document into the database.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>dict</code> <p>Dictionary representation of item</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Inserted item id</p> Source code in <code>lume_services/services/results/db.py</code> <pre><code>@abstractmethod\ndef insert_one(self, item: dict, **kwargs) -&gt; str:\n    \"\"\"Insert document into the database.\n\n    Args:\n        item (dict): Dictionary representation of item\n\n    Returns:\n        str: Inserted item id\n\n    \"\"\"\n</code></pre>"},{"location":"api/services/results/db/#lume_services.services.results.mongodb.MongodbResultsDB","title":"<code>MongodbResultsDB</code>","text":"<p>               Bases: <code>ResultsDB</code></p> Source code in <code>lume_services/services/results/mongodb.py</code> <pre><code>class MongodbResultsDB(ResultsDB):\n    # Note: pymongo is threadsafe\n\n    def __init__(self, db_config: MongodbResultsDBConfig):\n        self.config = db_config\n\n        # track pid to make multiprocessing safe\n        self._pid = os.getpid()\n        self._client = ContextVar(\"client\", default=None)\n        self._collections = ContextVar(\"collections\", default={})\n\n    def _connect(self) -&gt; MongoClient:\n        \"\"\"Establish connection and set _client.\"\"\"\n\n        client = MongoClient(\n            **self.config.dict(exclude_none=True, by_alias=True),\n            password=self.config.password.get_secret_value(),\n            **self.config.options\n        )\n\n        self._client.set(client)\n        db = client[self.config.database]\n\n        collections = {}\n\n        for collection_name in db.list_collection_names():\n            collection = db[collection_name]\n            index_info = collection.index_information()\n            collections[collection_name] = MongodbCollection(\n                database=self.config.database, name=collection_name, indices=index_info\n            )\n\n        self._collections.set(collections)\n\n        return client\n\n    def _check_mp(self) -&gt; None:\n        \"\"\"Check for multiprocessing. If PID is different that object PID, create new\n        engine connection.\n\n        \"\"\"\n\n        if os.getpid() != self._pid:\n            self._connect()\n\n    @property\n    def _currect_connection(self) -&gt; MongoClient:\n        \"\"\"Getter for current connection\"\"\"\n\n        return self._client.get()\n\n    def _disconnect(self):\n        \"\"\"Disconnect mongodb connection.\"\"\"\n        client = self._client.get()\n        if client is not None:\n            client.close()\n        self._client.set(None)\n        self._collections.set(None)\n\n    def disconnect(self):\n        \"\"\"Disconnect mongodb connection.\"\"\"\n        self._disconnect()\n\n    @contextmanager\n    def client(self) -&gt; MongoClient:\n        \"\"\"Context manager for mongoclient. Will check for multiprocessing and restart\n        accordingly.\n\n        \"\"\"\n        self._check_mp()\n\n        # get connection\n        client = self._client.get()\n\n        if client is None:\n            client = self._connect()\n            cleanup = True\n\n        else:\n            cleanup = False\n\n        try:\n            yield client\n\n        finally:\n            if cleanup:\n                client = self._client.get()\n\n                if client:\n                    self._disconnect()\n                    self._client.set(None)\n\n    def insert_one(self, collection: str, **kwargs) -&gt; str:\n        \"\"\"Insert one document into the database.\n\n        Args:\n            collection (str): Name of collection for saving document\n            **kwargs: Kwargs contain representation of document\n\n        Returns:\n            str: saved document id\n\n        \"\"\"\n        # conver to bson\n        with self.client() as client:\n            db = client[self.config.database]\n            db_collection = db[collection]\n            inserted_id = db_collection.insert_one(kwargs).inserted_id\n\n            if inserted_id is None:\n                raise ValueError(\n                    \"Unable to insert database object %s into collection %s\",\n                    kwargs,\n                    collection,\n                )\n\n        return inserted_id\n\n    def insert_many(self, collection: str, items: List[dict]) -&gt; List[str]:\n        \"\"\"Insert many documents into the database.\n\n        Args:\n            collection (str): Document type to query\n            items (List[dict]): List of dictionary reps of documents to save to database\n\n        Returns:\n            List[str]: List of saved document ids.\n\n\n        \"\"\"\n        # make items bsonable\n        with self.client() as client:\n            db = client[self.config.database]\n            db_collection = db[collection]\n            inserted_ids = db_collection.insert_many(items).inserted_ids\n\n        return [inserted_id.str for inserted_id in inserted_ids]\n\n    def find(\n        self, collection: str, query: dict = None, fields: List[str] = None\n    ) -&gt; List[dict]:\n        \"\"\"Find a document based on a query.\n\n        Args:\n            collection (str): Document type to query\n            query (dict): Query in dictionary form mapping fields to values\n            fields (List[str]): List of fields for filtering result\n\n        Returns:\n            List[dict]: List of of saved document ids.\n\n        \"\"\"\n\n        with self.client() as client:\n            db = client[self.config.database]\n            if fields is None:\n                results = db[collection].find(query)\n\n            else:\n                results = db[collection].find(query, projection=fields)\n\n            results = list(results)\n\n        # convert types to python\n\n        return results\n\n    def find_all(self, collection: str) -&gt; List[dict]:\n        \"\"\"Find all documents for a collection\n\n        Args:\n            collection (str): Collection name to query\n\n        Returns:\n            List[dict]: List of result documents.\n\n        \"\"\"\n        return self.find(collection=collection)\n\n    def configure(self, collections: Dict[str, List[str]]) -&gt; None:\n        \"\"\"Configure the results database from collections and their indices.\n\n        Args:\n            collections (Dict[str, List[str]]): Dictionary mapping collection to\n                index rep.\n\n        \"\"\"\n\n        collection_indices = {}\n\n        with self.client() as client:\n            db = client[self.config.database]\n\n            for collection_name, index in collections.items():\n\n                formatted_index = [(idx, DESCENDING) for idx in index]\n                db[collection_name].create_index(formatted_index, unique=True)\n\n                index_info = db[collection_name].index_information()\n\n                collection_indices[collection_name] = MongodbCollection(\n                    database=self.config.database,\n                    name=collection_name,\n                    indices=index_info,\n                )\n\n        self._collections.set(collection_indices)\n</code></pre>"},{"location":"api/services/results/db/#lume_services.services.results.mongodb.MongodbResultsDB.client","title":"<code>client()</code>","text":"<p>Context manager for mongoclient. Will check for multiprocessing and restart accordingly.</p> Source code in <code>lume_services/services/results/mongodb.py</code> <pre><code>@contextmanager\ndef client(self) -&gt; MongoClient:\n    \"\"\"Context manager for mongoclient. Will check for multiprocessing and restart\n    accordingly.\n\n    \"\"\"\n    self._check_mp()\n\n    # get connection\n    client = self._client.get()\n\n    if client is None:\n        client = self._connect()\n        cleanup = True\n\n    else:\n        cleanup = False\n\n    try:\n        yield client\n\n    finally:\n        if cleanup:\n            client = self._client.get()\n\n            if client:\n                self._disconnect()\n                self._client.set(None)\n</code></pre>"},{"location":"api/services/results/db/#lume_services.services.results.mongodb.MongodbResultsDB.configure","title":"<code>configure(collections)</code>","text":"<p>Configure the results database from collections and their indices.</p> <p>Parameters:</p> Name Type Description Default <code>collections</code> <code>Dict[str, List[str]]</code> <p>Dictionary mapping collection to index rep.</p> required Source code in <code>lume_services/services/results/mongodb.py</code> <pre><code>def configure(self, collections: Dict[str, List[str]]) -&gt; None:\n    \"\"\"Configure the results database from collections and their indices.\n\n    Args:\n        collections (Dict[str, List[str]]): Dictionary mapping collection to\n            index rep.\n\n    \"\"\"\n\n    collection_indices = {}\n\n    with self.client() as client:\n        db = client[self.config.database]\n\n        for collection_name, index in collections.items():\n\n            formatted_index = [(idx, DESCENDING) for idx in index]\n            db[collection_name].create_index(formatted_index, unique=True)\n\n            index_info = db[collection_name].index_information()\n\n            collection_indices[collection_name] = MongodbCollection(\n                database=self.config.database,\n                name=collection_name,\n                indices=index_info,\n            )\n\n    self._collections.set(collection_indices)\n</code></pre>"},{"location":"api/services/results/db/#lume_services.services.results.mongodb.MongodbResultsDB.disconnect","title":"<code>disconnect()</code>","text":"<p>Disconnect mongodb connection.</p> Source code in <code>lume_services/services/results/mongodb.py</code> <pre><code>def disconnect(self):\n    \"\"\"Disconnect mongodb connection.\"\"\"\n    self._disconnect()\n</code></pre>"},{"location":"api/services/results/db/#lume_services.services.results.mongodb.MongodbResultsDB.find","title":"<code>find(collection, query=None, fields=None)</code>","text":"<p>Find a document based on a query.</p> <p>Parameters:</p> Name Type Description Default <code>collection</code> <code>str</code> <p>Document type to query</p> required <code>query</code> <code>dict</code> <p>Query in dictionary form mapping fields to values</p> <code>None</code> <code>fields</code> <code>List[str]</code> <p>List of fields for filtering result</p> <code>None</code> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of of saved document ids.</p> Source code in <code>lume_services/services/results/mongodb.py</code> <pre><code>def find(\n    self, collection: str, query: dict = None, fields: List[str] = None\n) -&gt; List[dict]:\n    \"\"\"Find a document based on a query.\n\n    Args:\n        collection (str): Document type to query\n        query (dict): Query in dictionary form mapping fields to values\n        fields (List[str]): List of fields for filtering result\n\n    Returns:\n        List[dict]: List of of saved document ids.\n\n    \"\"\"\n\n    with self.client() as client:\n        db = client[self.config.database]\n        if fields is None:\n            results = db[collection].find(query)\n\n        else:\n            results = db[collection].find(query, projection=fields)\n\n        results = list(results)\n\n    # convert types to python\n\n    return results\n</code></pre>"},{"location":"api/services/results/db/#lume_services.services.results.mongodb.MongodbResultsDB.find_all","title":"<code>find_all(collection)</code>","text":"<p>Find all documents for a collection</p> <p>Parameters:</p> Name Type Description Default <code>collection</code> <code>str</code> <p>Collection name to query</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of result documents.</p> Source code in <code>lume_services/services/results/mongodb.py</code> <pre><code>def find_all(self, collection: str) -&gt; List[dict]:\n    \"\"\"Find all documents for a collection\n\n    Args:\n        collection (str): Collection name to query\n\n    Returns:\n        List[dict]: List of result documents.\n\n    \"\"\"\n    return self.find(collection=collection)\n</code></pre>"},{"location":"api/services/results/db/#lume_services.services.results.mongodb.MongodbResultsDB.insert_many","title":"<code>insert_many(collection, items)</code>","text":"<p>Insert many documents into the database.</p> <p>Parameters:</p> Name Type Description Default <code>collection</code> <code>str</code> <p>Document type to query</p> required <code>items</code> <code>List[dict]</code> <p>List of dictionary reps of documents to save to database</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of saved document ids.</p> Source code in <code>lume_services/services/results/mongodb.py</code> <pre><code>def insert_many(self, collection: str, items: List[dict]) -&gt; List[str]:\n    \"\"\"Insert many documents into the database.\n\n    Args:\n        collection (str): Document type to query\n        items (List[dict]): List of dictionary reps of documents to save to database\n\n    Returns:\n        List[str]: List of saved document ids.\n\n\n    \"\"\"\n    # make items bsonable\n    with self.client() as client:\n        db = client[self.config.database]\n        db_collection = db[collection]\n        inserted_ids = db_collection.insert_many(items).inserted_ids\n\n    return [inserted_id.str for inserted_id in inserted_ids]\n</code></pre>"},{"location":"api/services/results/db/#lume_services.services.results.mongodb.MongodbResultsDB.insert_one","title":"<code>insert_one(collection, **kwargs)</code>","text":"<p>Insert one document into the database.</p> <p>Parameters:</p> Name Type Description Default <code>collection</code> <code>str</code> <p>Name of collection for saving document</p> required <code>**kwargs</code> <p>Kwargs contain representation of document</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>saved document id</p> Source code in <code>lume_services/services/results/mongodb.py</code> <pre><code>def insert_one(self, collection: str, **kwargs) -&gt; str:\n    \"\"\"Insert one document into the database.\n\n    Args:\n        collection (str): Name of collection for saving document\n        **kwargs: Kwargs contain representation of document\n\n    Returns:\n        str: saved document id\n\n    \"\"\"\n    # conver to bson\n    with self.client() as client:\n        db = client[self.config.database]\n        db_collection = db[collection]\n        inserted_id = db_collection.insert_one(kwargs).inserted_id\n\n        if inserted_id is None:\n            raise ValueError(\n                \"Unable to insert database object %s into collection %s\",\n                kwargs,\n                collection,\n            )\n\n    return inserted_id\n</code></pre>"},{"location":"api/services/results/db/#lume_services.services.results.mongodb.MongodbResultsDBConfig","title":"<code>MongodbResultsDBConfig</code>","text":"<p>               Bases: <code>ResultsDBConfig</code></p> <p>Configuration for connecting to Mongodb using the PyMongo driver.</p> Attr <p>database (Optional[str]): Database name used for storing results. host (str): Host name of mongodb service. username (str): Username string. password (SecretStr): Password stored as a Pydantic secret string. https://pydantic-docs.helpmanual.io/usage/types/#secret-types port (int): Host port of mongodb service endpoint. authMechanism (str): Auth mechanism supported by PyMongo driver. See https://pymongo.readthedocs.io/en/stable/api/pymongo/database.html#pymongo.auth.MECHANISMS. options (dict): Dictionary of additional connection options for MongoClient. https://pymongo.readthedocs.io/en/stable/api/pymongo/mongo_client.html#pymongo.mongo_client.MongoClient</p> Source code in <code>lume_services/services/results/mongodb.py</code> <pre><code>class MongodbResultsDBConfig(ResultsDBConfig):\n    \"\"\"Configuration for connecting to Mongodb using the PyMongo driver.\n\n    Attr:\n        database (Optional[str]): Database name used for storing results.\n        host (str): Host name of mongodb service.\n        username (str): Username string.\n        password (SecretStr): Password stored as a Pydantic secret string. https://pydantic-docs.helpmanual.io/usage/types/#secret-types\n        port (int): Host port of mongodb service endpoint.\n        authMechanism (str): Auth mechanism supported by PyMongo driver. See https://pymongo.readthedocs.io/en/stable/api/pymongo/database.html#pymongo.auth.MECHANISMS.\n        options (dict): Dictionary of additional connection options for MongoClient. https://pymongo.readthedocs.io/en/stable/api/pymongo/mongo_client.html#pymongo.mongo_client.MongoClient\n\n    \"\"\"  # noqa\n\n    # excluded in serialization bc not used to initialize cxn\n    database: Optional[str] = Field(exclude=True)\n    username: str\n    host: str\n    password: SecretStr = Field(exclude=True)\n    port: int\n    authMechanism: str = \"DEFAULT\"\n    # Pydantic literal parsing from env has issue with literals...\n    # Literal[\"DEFAULT\", 'GSSAPI', 'MONGODB-AWS', 'MONGODB-CR', 'MONGODB-X509',\n    # 'PLAIN', 'SCRAM-SHA-1', 'SCRAM-SHA-256'] = \"DEFAULT\"\n    options: dict = Field({}, exclude=True)\n\n    class Config:\n        allow_population_by_field_name = True\n</code></pre>"},{"location":"api/services/results/results/","title":"Results Database Service","text":""},{"location":"api/services/results/results/#lume_services.services.results.service.ResultsDBService","title":"<code>ResultsDBService</code>","text":"<p>Results database for use with NoSQL database service</p> Source code in <code>lume_services/services/results/service.py</code> <pre><code>class ResultsDBService:\n    \"\"\"Results database for use with NoSQL database service\"\"\"\n\n    def __init__(self, results_db: ResultsDB):\n        \"\"\"Initialize Results DB Service interface\n        Args:\n            results_db (DBService): DB Connection service\n        \"\"\"\n        self._results_db = results_db\n\n    def insert_one(self, item: dict, **kwargs) -&gt; str:\n        \"\"\"Store model data.\n        Args:\n            model_type (str): Must correspond to models listed in model_docs enum\n                provided during construction.\n            **kwargs: Initialization arguments for document construction covering\n                minimal data required by model.\n        Returns:\n            bool: Success of storage operation\n        \"\"\"\n\n        return self._results_db.insert_one(**item, **kwargs)\n\n    def insert_many(self, items: List[dict], **kwargs) -&gt; List[str]:\n        \"\"\"Insert many documents into the database.\n\n        Args:\n            items (List[dict]): List of dictionary representations of items\n\n        Returns:\n            List[str]: List of interted ids\n\n        \"\"\"\n        return self._results_db.insert_many(items, **kwargs)\n\n    def find(self, *, query: dict, fields: List[str] = None, **kwargs) -&gt; List[dict]:\n        \"\"\"Find a document based on a query.\n\n        Args:\n            query (dict): fields to query on\n            fields (List[str]): List of fields to return if any\n            **kwargs (dict): DB implementation specific fields\n\n        Returns:\n            List[dict]: List of dict reps of found items.\n\n        \"\"\"\n        query = get_jsonable_dict(query)\n        return self._results_db.find(query=query, fields=fields, **kwargs)\n\n    def find_all(self, **kwargs) -&gt; List[dict]:\n        \"\"\"Find all documents for a collection\n\n        Returns:\n            List[dict]: List of result items represented as dict.\n        \"\"\"\n        return self._results_db.find_all(**kwargs)\n</code></pre>"},{"location":"api/services/results/results/#lume_services.services.results.service.ResultsDBService.__init__","title":"<code>__init__(results_db)</code>","text":"<p>Initialize Results DB Service interface Args:     results_db (DBService): DB Connection service</p> Source code in <code>lume_services/services/results/service.py</code> <pre><code>def __init__(self, results_db: ResultsDB):\n    \"\"\"Initialize Results DB Service interface\n    Args:\n        results_db (DBService): DB Connection service\n    \"\"\"\n    self._results_db = results_db\n</code></pre>"},{"location":"api/services/results/results/#lume_services.services.results.service.ResultsDBService.find","title":"<code>find(*, query, fields=None, **kwargs)</code>","text":"<p>Find a document based on a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>dict</code> <p>fields to query on</p> required <code>fields</code> <code>List[str]</code> <p>List of fields to return if any</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>DB implementation specific fields</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of dict reps of found items.</p> Source code in <code>lume_services/services/results/service.py</code> <pre><code>def find(self, *, query: dict, fields: List[str] = None, **kwargs) -&gt; List[dict]:\n    \"\"\"Find a document based on a query.\n\n    Args:\n        query (dict): fields to query on\n        fields (List[str]): List of fields to return if any\n        **kwargs (dict): DB implementation specific fields\n\n    Returns:\n        List[dict]: List of dict reps of found items.\n\n    \"\"\"\n    query = get_jsonable_dict(query)\n    return self._results_db.find(query=query, fields=fields, **kwargs)\n</code></pre>"},{"location":"api/services/results/results/#lume_services.services.results.service.ResultsDBService.find_all","title":"<code>find_all(**kwargs)</code>","text":"<p>Find all documents for a collection</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of result items represented as dict.</p> Source code in <code>lume_services/services/results/service.py</code> <pre><code>def find_all(self, **kwargs) -&gt; List[dict]:\n    \"\"\"Find all documents for a collection\n\n    Returns:\n        List[dict]: List of result items represented as dict.\n    \"\"\"\n    return self._results_db.find_all(**kwargs)\n</code></pre>"},{"location":"api/services/results/results/#lume_services.services.results.service.ResultsDBService.insert_many","title":"<code>insert_many(items, **kwargs)</code>","text":"<p>Insert many documents into the database.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[dict]</code> <p>List of dictionary representations of items</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of interted ids</p> Source code in <code>lume_services/services/results/service.py</code> <pre><code>def insert_many(self, items: List[dict], **kwargs) -&gt; List[str]:\n    \"\"\"Insert many documents into the database.\n\n    Args:\n        items (List[dict]): List of dictionary representations of items\n\n    Returns:\n        List[str]: List of interted ids\n\n    \"\"\"\n    return self._results_db.insert_many(items, **kwargs)\n</code></pre>"},{"location":"api/services/results/results/#lume_services.services.results.service.ResultsDBService.insert_one","title":"<code>insert_one(item, **kwargs)</code>","text":"<p>Store model data. Args:     model_type (str): Must correspond to models listed in model_docs enum         provided during construction.     **kwargs: Initialization arguments for document construction covering         minimal data required by model. Returns:     bool: Success of storage operation</p> Source code in <code>lume_services/services/results/service.py</code> <pre><code>def insert_one(self, item: dict, **kwargs) -&gt; str:\n    \"\"\"Store model data.\n    Args:\n        model_type (str): Must correspond to models listed in model_docs enum\n            provided during construction.\n        **kwargs: Initialization arguments for document construction covering\n            minimal data required by model.\n    Returns:\n        bool: Success of storage operation\n    \"\"\"\n\n    return self._results_db.insert_one(**item, **kwargs)\n</code></pre>"},{"location":"api/services/scheduling/backends/","title":"Backends","text":""},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.backend.Backend","title":"<code>Backend</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Abstract base class for Prefect backends. Backends handle Prefect interactions including running of flows, result handling, and flow registration with server backends.</p> Source code in <code>lume_services/services/scheduling/backends/backend.py</code> <pre><code>class Backend(BaseModel, ABC):\n    \"\"\"Abstract base class for Prefect backends. Backends handle Prefect interactions\n    including running of flows, result handling, and flow registration with server\n    backends.\n\n    \"\"\"\n\n    @abstractmethod\n    def create_project(self, project_name: str) -&gt; None:\n        \"\"\"Create a Prefect project. Backend implementations without server connecton\n        should raise errors when this method is called.\n\n        Args:\n            project_name (str): Create a named Prefect project.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def register_flow(\n        self,\n        flow: Flow,\n        project_name: str,\n        image: Optional[str],\n    ) -&gt; str:\n        \"\"\"Register a flow with Prefect. Backend implementations without server connecton\n        should raise errors when this method is called.\n\n        Args:\n            flow (Flow): Prefect flow to register.\n            project_name (str): Name of project to register flow to.\n            image (str): Name of Docker image to run flow inside.\n\n        Returns:\n            str: ID of registered flow.\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def load_flow(self, flow_name: str, project_name: str) -&gt; dict:\n        \"\"\"Load a Prefect flow object. Backend implementations without server connecton\n        should raise errors when this method is called.\n\n        Args:\n            flow_name (str): Name of flow.\n            project_name (str): Name of project flow is registered with.\n\n        Returns:\n            dict: Dictionary with keys \"flow_id\" and \"flow\"\n\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def run(\n        self,\n        parameters: Optional[Dict[str, Any]],\n        run_config: Optional[RunConfig],\n        **kwargs\n    ) -&gt; Union[str, None]:\n        \"\"\"Run a flow. Does not return result. Implementations should cover instantiation\n        of run_config from kwargs as well as backend-specific kwargs.\n\n        Args:\n            parameters (Optional[Dict[str, Any]]): Dictionary mapping flow parameter\n                name to value\n            run_config (Optional[RunConfig]): RunConfig object to configure flow fun.\n            **kwargs: Keyword arguments for RunConfig init and backend-specific\n                execution.\n\n        Returns:\n            Union[str, None]: Return run_id in case of server backend, None in the case\n                of local execution.\n\n        Raises:\n            pydantic.ValidationError: Error validating run configuration.\n            ValueError: Value error on flow run\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def run_and_return(\n        self,\n        parameters: Optional[Dict[str, Any]],\n        run_config: Optional[RunConfig],\n        task_name: Optional[str],\n        **kwargs\n    ) -&gt; Any:\n        \"\"\"Run a flow and return result. Implementations should cover instantiation of\n        run_config from kwargs as well as backend-specific kwargs.\n\n        Args:\n            parameters (Optional[Dict[str, Any]]): Dictionary mapping flow parameter\n                name to value\n            run_config (Optional[RunConfig]): RunConfig object to configure flow fun.\n            task_name (Optional[str]): Name of task to return result. If no task slug\n                is passed, will return the flow result.\n            **kwargs: Keyword arguments for RunConfig init and backend-specific\n                execution.\n\n        Returns:\n            Any: Result of flow run.\n\n        Raises:\n            lume_services.errors.EmptyResultError: No result is associated with the\n                flow.\n            pydantic.ValidationError: Error validating run configuration.\n            ValueError: Value error on flow run\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.backend.Backend.create_project","title":"<code>create_project(project_name)</code>  <code>abstractmethod</code>","text":"<p>Create a Prefect project. Backend implementations without server connecton should raise errors when this method is called.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Create a named Prefect project.</p> required Source code in <code>lume_services/services/scheduling/backends/backend.py</code> <pre><code>@abstractmethod\ndef create_project(self, project_name: str) -&gt; None:\n    \"\"\"Create a Prefect project. Backend implementations without server connecton\n    should raise errors when this method is called.\n\n    Args:\n        project_name (str): Create a named Prefect project.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.backend.Backend.load_flow","title":"<code>load_flow(flow_name, project_name)</code>  <code>abstractmethod</code>","text":"<p>Load a Prefect flow object. Backend implementations without server connecton should raise errors when this method is called.</p> <p>Parameters:</p> Name Type Description Default <code>flow_name</code> <code>str</code> <p>Name of flow.</p> required <code>project_name</code> <code>str</code> <p>Name of project flow is registered with.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary with keys \"flow_id\" and \"flow\"</p> Source code in <code>lume_services/services/scheduling/backends/backend.py</code> <pre><code>@abstractmethod\ndef load_flow(self, flow_name: str, project_name: str) -&gt; dict:\n    \"\"\"Load a Prefect flow object. Backend implementations without server connecton\n    should raise errors when this method is called.\n\n    Args:\n        flow_name (str): Name of flow.\n        project_name (str): Name of project flow is registered with.\n\n    Returns:\n        dict: Dictionary with keys \"flow_id\" and \"flow\"\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.backend.Backend.register_flow","title":"<code>register_flow(flow, project_name, image)</code>  <code>abstractmethod</code>","text":"<p>Register a flow with Prefect. Backend implementations without server connecton should raise errors when this method is called.</p> <p>Parameters:</p> Name Type Description Default <code>flow</code> <code>Flow</code> <p>Prefect flow to register.</p> required <code>project_name</code> <code>str</code> <p>Name of project to register flow to.</p> required <code>image</code> <code>str</code> <p>Name of Docker image to run flow inside.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>ID of registered flow.</p> Source code in <code>lume_services/services/scheduling/backends/backend.py</code> <pre><code>@abstractmethod\ndef register_flow(\n    self,\n    flow: Flow,\n    project_name: str,\n    image: Optional[str],\n) -&gt; str:\n    \"\"\"Register a flow with Prefect. Backend implementations without server connecton\n    should raise errors when this method is called.\n\n    Args:\n        flow (Flow): Prefect flow to register.\n        project_name (str): Name of project to register flow to.\n        image (str): Name of Docker image to run flow inside.\n\n    Returns:\n        str: ID of registered flow.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.backend.Backend.run","title":"<code>run(parameters, run_config, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Run a flow. Does not return result. Implementations should cover instantiation of run_config from kwargs as well as backend-specific kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary mapping flow parameter name to value</p> required <code>run_config</code> <code>Optional[RunConfig]</code> <p>RunConfig object to configure flow fun.</p> required <code>**kwargs</code> <p>Keyword arguments for RunConfig init and backend-specific execution.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>Union[str, None]: Return run_id in case of server backend, None in the case of local execution.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>Error validating run configuration.</p> <code>ValueError</code> <p>Value error on flow run</p> Source code in <code>lume_services/services/scheduling/backends/backend.py</code> <pre><code>@abstractmethod\ndef run(\n    self,\n    parameters: Optional[Dict[str, Any]],\n    run_config: Optional[RunConfig],\n    **kwargs\n) -&gt; Union[str, None]:\n    \"\"\"Run a flow. Does not return result. Implementations should cover instantiation\n    of run_config from kwargs as well as backend-specific kwargs.\n\n    Args:\n        parameters (Optional[Dict[str, Any]]): Dictionary mapping flow parameter\n            name to value\n        run_config (Optional[RunConfig]): RunConfig object to configure flow fun.\n        **kwargs: Keyword arguments for RunConfig init and backend-specific\n            execution.\n\n    Returns:\n        Union[str, None]: Return run_id in case of server backend, None in the case\n            of local execution.\n\n    Raises:\n        pydantic.ValidationError: Error validating run configuration.\n        ValueError: Value error on flow run\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.backend.Backend.run_and_return","title":"<code>run_and_return(parameters, run_config, task_name, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Run a flow and return result. Implementations should cover instantiation of run_config from kwargs as well as backend-specific kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary mapping flow parameter name to value</p> required <code>run_config</code> <code>Optional[RunConfig]</code> <p>RunConfig object to configure flow fun.</p> required <code>task_name</code> <code>Optional[str]</code> <p>Name of task to return result. If no task slug is passed, will return the flow result.</p> required <code>**kwargs</code> <p>Keyword arguments for RunConfig init and backend-specific execution.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of flow run.</p> <p>Raises:</p> Type Description <code>EmptyResultError</code> <p>No result is associated with the flow.</p> <code>ValidationError</code> <p>Error validating run configuration.</p> <code>ValueError</code> <p>Value error on flow run</p> Source code in <code>lume_services/services/scheduling/backends/backend.py</code> <pre><code>@abstractmethod\ndef run_and_return(\n    self,\n    parameters: Optional[Dict[str, Any]],\n    run_config: Optional[RunConfig],\n    task_name: Optional[str],\n    **kwargs\n) -&gt; Any:\n    \"\"\"Run a flow and return result. Implementations should cover instantiation of\n    run_config from kwargs as well as backend-specific kwargs.\n\n    Args:\n        parameters (Optional[Dict[str, Any]]): Dictionary mapping flow parameter\n            name to value\n        run_config (Optional[RunConfig]): RunConfig object to configure flow fun.\n        task_name (Optional[str]): Name of task to return result. If no task slug\n            is passed, will return the flow result.\n        **kwargs: Keyword arguments for RunConfig init and backend-specific\n            execution.\n\n    Returns:\n        Any: Result of flow run.\n\n    Raises:\n        lume_services.errors.EmptyResultError: No result is associated with the\n            flow.\n        pydantic.ValidationError: Error validating run configuration.\n        ValueError: Value error on flow run\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.backend.RunConfig","title":"<code>RunConfig</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Pydantic representation of Prefect UniversalRunConfig: https://docs.prefect.io/api/latest/run_configs.html#universalrun</p> <p>Attributes:</p> Name Type Description <code>labels</code> <code>Optional[List[str]]</code> <p>an list of labels to apply to this run config. Labels are string identifiers used by Prefect Agents for selecting valid flow runs when polling for work</p> <code>env</code> <code>Optional[dict]</code> <p>Additional environment variables to set on the job</p> Source code in <code>lume_services/services/scheduling/backends/backend.py</code> <pre><code>class RunConfig(BaseModel, ABC):\n    \"\"\"Pydantic representation of Prefect UniversalRunConfig:\n    https://docs.prefect.io/api/latest/run_configs.html#universalrun\n\n\n    Attributes:\n        labels (Optional[List[str]]): an list of labels to apply to this run\n            config. Labels are string identifiers used by Prefect Agents for selecting\n            valid flow runs when polling for work\n        env (Optional[dict]): Additional environment variables to set on the job\n\n    \"\"\"\n\n    labels: List[str] = [\"lume-services\"]\n    env: Optional[dict]\n\n    @abstractmethod\n    def build(self) -&gt; PrefectRunConfig:\n        \"\"\"Method for converting object to Prefect RunConfig type.\n\n        Returns:\n            PrefectRunConfig\n\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.backend.RunConfig.build","title":"<code>build()</code>  <code>abstractmethod</code>","text":"<p>Method for converting object to Prefect RunConfig type.</p> <p>Returns:</p> Type Description <code>RunConfig</code> <p>PrefectRunConfig</p> Source code in <code>lume_services/services/scheduling/backends/backend.py</code> <pre><code>@abstractmethod\ndef build(self) -&gt; PrefectRunConfig:\n    \"\"\"Method for converting object to Prefect RunConfig type.\n\n    Returns:\n        PrefectRunConfig\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.local.LocalBackend","title":"<code>LocalBackend</code>","text":"<p>               Bases: <code>Backend</code></p> <p>Backend used for local execution. This backend will raise errors on any function calls requiring registration with the Prefect server.</p> <p>Attributes:</p> Name Type Description <code>run_config</code> <code>Optional[LocalRunConfig]</code> <p>Default configuration object for a given run.</p> Source code in <code>lume_services/services/scheduling/backends/local.py</code> <pre><code>class LocalBackend(Backend):\n    \"\"\"Backend used for local execution. This backend will raise errors on any function\n    calls requiring registration with the Prefect server.\n\n    Attributes:\n        run_config (Optional[LocalRunConfig]): Default configuration object for a given\n            run.\n\n    \"\"\"\n\n    def run(\n        self,\n        data: Dict[str, Any],\n        run_config: LocalRunConfig = None,\n        *,\n        flow: Flow,\n        **kwargs\n    ) -&gt; None:\n        \"\"\"Run flow execution. Does not return result.\n\n        Args:\n            labels (Optional[List[str]]): an list of labels to apply to this run\n                config. Labels are string identifiers used by Prefect Agents for\n                selecting valid flow runs when polling for work.\n            env (Optional[dict]): Additional environment variables to set on the job\n            data (Optional[Dict[str, Any]]): Dictionary mapping flow parameter name to\n                value.\n            run_config (Optional[LocalRunConfig]): LocalRunConfig object to configure\n                flow fun.\n            flow (Flow): Prefect flow to execute.\n            **kwargs: Keyword arguments to intantiate the LocalRunConfig.\n\n        Raises:\n            pydantic.ValidationError: Error validating run configuration.\n\n        \"\"\"\n\n        if run_config is not None and len(kwargs):\n            warnings.warn(\n                \"Both run_config and kwargs passed to LocalBackend.run. Flow\\\n                will execute using passed run_config.\"\n            )\n\n        if run_config is None:\n            run_config = LocalRunConfig(**kwargs)\n\n        # convert to Prefect LocalRun\n        prefect_run_config = run_config.build()\n\n        # apply run config\n        flow.run_config = prefect_run_config\n        flow.run(parameters=data)\n\n    def run_and_return(\n        self,\n        data: Dict[str, Any],\n        run_config: LocalRunConfig = None,\n        task_name: str = None,\n        *,\n        flow: Flow,\n        **kwargs\n    ) -&gt; Any:\n        \"\"\"Run flow execution and return result.\n\n        Args:\n            data (Optional[Dict[str, Any]]): Dictionary mapping flow parameter name to\n                value.\n            run_config (Optional[LocalRunConfig]): LocalRunConfig object to configure\n                flow fun.\n            task_name (Optional[str]): Name of task to return result. If no task slug\n                is passed, will return the flow result.\n            flow (Flow): Prefect flow to execute.\n            **kwargs: Keyword arguments to intantiate the LocalRunConfig.\n\n        Raises:\n            pydantic.ValidationError: Error validating run configuration.\n            EmptyResultError: No result is associated with the flow.\n            TaskNotCompletedError: Result reference task was not completed.\n            TaskNotInFlowError: Provided task slug not in flow.\n\n        \"\"\"\n        if run_config is not None and len(kwargs):\n            warnings.warn(\n                \"Both run_config and kwargs passed to LocalBackend.run. Flow\\\n                will execute using passed run_config.\"\n            )\n\n        if run_config is None:\n            run_config = LocalRunConfig(**kwargs)\n\n        # convert to Prefect LocalRun\n        prefect_run_config = run_config.build()\n\n        # apply run config\n        flow.run_config = prefect_run_config\n\n        try:\n            flow_run = flow.run(parameters=data)\n            if flow_run.is_failed():\n                logger.exception(flow_run.message)\n                raise FlowFailedError(\n                    flow_id=\"local_flow\",\n                    flow_run_id=\"local_flow_run\",\n                    exception_message=flow_run.message,\n                )\n\n        except Exception as e:\n            logger.exception(e.message)\n            raise FlowFailedError(\n                flow_id=\"local_flow\",\n                flow_run_id=\"local_flow_run\",\n                exception_message=e.message,\n            )\n\n        result = flow_run.result\n\n        if result is None:\n            raise EmptyResultError\n\n        task_to_slug_map = {task: slug for task, slug in flow.slugs.items()}\n        # slug_to_task_map = {slug: task for task, slug in flow.slugs.items()}\n\n        # account for task slug\n        if task_name is not None:\n            # get tasks\n            tasks = flow.get_tasks(name=task_name)\n            if not len(tasks):\n                raise TaskNotInFlowError(\n                    flow_name=flow.name, project_name=\"local\", task_name=task_name\n                )\n\n            results = []\n            for task in tasks:\n                slug = task_to_slug_map.get(task)\n                state = result[task]\n                if not state.is_successful():\n                    raise TaskNotCompletedError(\n                        slug, flow_id=\"local_flow\", flow_run_id=\"local_flow_run\"\n                    )\n\n                res = state.result\n                if res is None:\n                    raise EmptyResultError(\"local_flow\", \"local_flow_run\", slug)\n\n                results.append(state.result)\n\n            if len(tasks) == 1:\n                return results[0]\n\n            else:\n                return results\n\n        # else return dict of task slug to value\n        else:\n            return {\n                slug: result[task].result for task, slug in task_to_slug_map.items()\n            }\n\n    def create_project(self, *args, **kwargs) -&gt; None:\n        \"\"\"Raise LocalBackendError for calls to register_flow server-type method.\n\n        Raises:\n            LocalBackendError: Indicates that a server-backend operation has been\n                executed against the LocalBackend. Server-backend operations include\n                flow registration and remote execution.\n\n        \"\"\"\n        raise LocalBackendError()\n\n    def register_flow(self, *args, **kwargs) -&gt; None:\n        \"\"\"Raise LocalBackendError for calls to register_flow server-type method.\n\n        Raises:\n            LocalBackendError: Indicates that a server-backend operation has been\n                executed against the LocalBackend. Server-backend operations include\n                flow registration and remote execution.\n\n\n        \"\"\"\n        raise LocalBackendError()\n\n    def load_flow(self, *args, **kwargs) -&gt; None:\n        \"\"\"Raise LocalBackendError for calls to load_flow server-type method.\n\n        Raises:\n            LocalBackendError: Indicates that a server-backend operation has been\n                executed against the LocalBackend. Server-backend operations include\n                flow registration and remote execution.\n\n        \"\"\"\n        raise LocalBackendError()\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.local.LocalBackend.create_project","title":"<code>create_project(*args, **kwargs)</code>","text":"<p>Raise LocalBackendError for calls to register_flow server-type method.</p> <p>Raises:</p> Type Description <code>LocalBackendError</code> <p>Indicates that a server-backend operation has been executed against the LocalBackend. Server-backend operations include flow registration and remote execution.</p> Source code in <code>lume_services/services/scheduling/backends/local.py</code> <pre><code>def create_project(self, *args, **kwargs) -&gt; None:\n    \"\"\"Raise LocalBackendError for calls to register_flow server-type method.\n\n    Raises:\n        LocalBackendError: Indicates that a server-backend operation has been\n            executed against the LocalBackend. Server-backend operations include\n            flow registration and remote execution.\n\n    \"\"\"\n    raise LocalBackendError()\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.local.LocalBackend.load_flow","title":"<code>load_flow(*args, **kwargs)</code>","text":"<p>Raise LocalBackendError for calls to load_flow server-type method.</p> <p>Raises:</p> Type Description <code>LocalBackendError</code> <p>Indicates that a server-backend operation has been executed against the LocalBackend. Server-backend operations include flow registration and remote execution.</p> Source code in <code>lume_services/services/scheduling/backends/local.py</code> <pre><code>def load_flow(self, *args, **kwargs) -&gt; None:\n    \"\"\"Raise LocalBackendError for calls to load_flow server-type method.\n\n    Raises:\n        LocalBackendError: Indicates that a server-backend operation has been\n            executed against the LocalBackend. Server-backend operations include\n            flow registration and remote execution.\n\n    \"\"\"\n    raise LocalBackendError()\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.local.LocalBackend.register_flow","title":"<code>register_flow(*args, **kwargs)</code>","text":"<p>Raise LocalBackendError for calls to register_flow server-type method.</p> <p>Raises:</p> Type Description <code>LocalBackendError</code> <p>Indicates that a server-backend operation has been executed against the LocalBackend. Server-backend operations include flow registration and remote execution.</p> Source code in <code>lume_services/services/scheduling/backends/local.py</code> <pre><code>def register_flow(self, *args, **kwargs) -&gt; None:\n    \"\"\"Raise LocalBackendError for calls to register_flow server-type method.\n\n    Raises:\n        LocalBackendError: Indicates that a server-backend operation has been\n            executed against the LocalBackend. Server-backend operations include\n            flow registration and remote execution.\n\n\n    \"\"\"\n    raise LocalBackendError()\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.local.LocalBackend.run","title":"<code>run(data, run_config=None, *, flow, **kwargs)</code>","text":"<p>Run flow execution. Does not return result.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Optional[List[str]]</code> <p>an list of labels to apply to this run config. Labels are string identifiers used by Prefect Agents for selecting valid flow runs when polling for work.</p> required <code>env</code> <code>Optional[dict]</code> <p>Additional environment variables to set on the job</p> required <code>data</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary mapping flow parameter name to value.</p> required <code>run_config</code> <code>Optional[LocalRunConfig]</code> <p>LocalRunConfig object to configure flow fun.</p> <code>None</code> <code>flow</code> <code>Flow</code> <p>Prefect flow to execute.</p> required <code>**kwargs</code> <p>Keyword arguments to intantiate the LocalRunConfig.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValidationError</code> <p>Error validating run configuration.</p> Source code in <code>lume_services/services/scheduling/backends/local.py</code> <pre><code>def run(\n    self,\n    data: Dict[str, Any],\n    run_config: LocalRunConfig = None,\n    *,\n    flow: Flow,\n    **kwargs\n) -&gt; None:\n    \"\"\"Run flow execution. Does not return result.\n\n    Args:\n        labels (Optional[List[str]]): an list of labels to apply to this run\n            config. Labels are string identifiers used by Prefect Agents for\n            selecting valid flow runs when polling for work.\n        env (Optional[dict]): Additional environment variables to set on the job\n        data (Optional[Dict[str, Any]]): Dictionary mapping flow parameter name to\n            value.\n        run_config (Optional[LocalRunConfig]): LocalRunConfig object to configure\n            flow fun.\n        flow (Flow): Prefect flow to execute.\n        **kwargs: Keyword arguments to intantiate the LocalRunConfig.\n\n    Raises:\n        pydantic.ValidationError: Error validating run configuration.\n\n    \"\"\"\n\n    if run_config is not None and len(kwargs):\n        warnings.warn(\n            \"Both run_config and kwargs passed to LocalBackend.run. Flow\\\n            will execute using passed run_config.\"\n        )\n\n    if run_config is None:\n        run_config = LocalRunConfig(**kwargs)\n\n    # convert to Prefect LocalRun\n    prefect_run_config = run_config.build()\n\n    # apply run config\n    flow.run_config = prefect_run_config\n    flow.run(parameters=data)\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.local.LocalBackend.run_and_return","title":"<code>run_and_return(data, run_config=None, task_name=None, *, flow, **kwargs)</code>","text":"<p>Run flow execution and return result.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary mapping flow parameter name to value.</p> required <code>run_config</code> <code>Optional[LocalRunConfig]</code> <p>LocalRunConfig object to configure flow fun.</p> <code>None</code> <code>task_name</code> <code>Optional[str]</code> <p>Name of task to return result. If no task slug is passed, will return the flow result.</p> <code>None</code> <code>flow</code> <code>Flow</code> <p>Prefect flow to execute.</p> required <code>**kwargs</code> <p>Keyword arguments to intantiate the LocalRunConfig.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValidationError</code> <p>Error validating run configuration.</p> <code>EmptyResultError</code> <p>No result is associated with the flow.</p> <code>TaskNotCompletedError</code> <p>Result reference task was not completed.</p> <code>TaskNotInFlowError</code> <p>Provided task slug not in flow.</p> Source code in <code>lume_services/services/scheduling/backends/local.py</code> <pre><code>def run_and_return(\n    self,\n    data: Dict[str, Any],\n    run_config: LocalRunConfig = None,\n    task_name: str = None,\n    *,\n    flow: Flow,\n    **kwargs\n) -&gt; Any:\n    \"\"\"Run flow execution and return result.\n\n    Args:\n        data (Optional[Dict[str, Any]]): Dictionary mapping flow parameter name to\n            value.\n        run_config (Optional[LocalRunConfig]): LocalRunConfig object to configure\n            flow fun.\n        task_name (Optional[str]): Name of task to return result. If no task slug\n            is passed, will return the flow result.\n        flow (Flow): Prefect flow to execute.\n        **kwargs: Keyword arguments to intantiate the LocalRunConfig.\n\n    Raises:\n        pydantic.ValidationError: Error validating run configuration.\n        EmptyResultError: No result is associated with the flow.\n        TaskNotCompletedError: Result reference task was not completed.\n        TaskNotInFlowError: Provided task slug not in flow.\n\n    \"\"\"\n    if run_config is not None and len(kwargs):\n        warnings.warn(\n            \"Both run_config and kwargs passed to LocalBackend.run. Flow\\\n            will execute using passed run_config.\"\n        )\n\n    if run_config is None:\n        run_config = LocalRunConfig(**kwargs)\n\n    # convert to Prefect LocalRun\n    prefect_run_config = run_config.build()\n\n    # apply run config\n    flow.run_config = prefect_run_config\n\n    try:\n        flow_run = flow.run(parameters=data)\n        if flow_run.is_failed():\n            logger.exception(flow_run.message)\n            raise FlowFailedError(\n                flow_id=\"local_flow\",\n                flow_run_id=\"local_flow_run\",\n                exception_message=flow_run.message,\n            )\n\n    except Exception as e:\n        logger.exception(e.message)\n        raise FlowFailedError(\n            flow_id=\"local_flow\",\n            flow_run_id=\"local_flow_run\",\n            exception_message=e.message,\n        )\n\n    result = flow_run.result\n\n    if result is None:\n        raise EmptyResultError\n\n    task_to_slug_map = {task: slug for task, slug in flow.slugs.items()}\n    # slug_to_task_map = {slug: task for task, slug in flow.slugs.items()}\n\n    # account for task slug\n    if task_name is not None:\n        # get tasks\n        tasks = flow.get_tasks(name=task_name)\n        if not len(tasks):\n            raise TaskNotInFlowError(\n                flow_name=flow.name, project_name=\"local\", task_name=task_name\n            )\n\n        results = []\n        for task in tasks:\n            slug = task_to_slug_map.get(task)\n            state = result[task]\n            if not state.is_successful():\n                raise TaskNotCompletedError(\n                    slug, flow_id=\"local_flow\", flow_run_id=\"local_flow_run\"\n                )\n\n            res = state.result\n            if res is None:\n                raise EmptyResultError(\"local_flow\", \"local_flow_run\", slug)\n\n            results.append(state.result)\n\n        if len(tasks) == 1:\n            return results[0]\n\n        else:\n            return results\n\n    # else return dict of task slug to value\n    else:\n        return {\n            slug: result[task].result for task, slug in task_to_slug_map.items()\n        }\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.local.LocalRunConfig","title":"<code>LocalRunConfig</code>","text":"<p>               Bases: <code>RunConfig</code></p> <p>Local run configuration. If no directory is found at the filepath passed as working_dir, an error will be raised.</p> <p>Attributes:</p> Name Type Description <code>labels</code> <code>Optional[List[str]]</code> <p>an list of labels to apply to this run config. Labels are string identifiers used by Prefect Agents for selecting valid flow runs when polling for work</p> <code>env</code> <code>Optional[Dict[str, str]]</code> <p>Dictionary of environment variables to use for run</p> <code>working_dir</code> <code>Optional[str]</code> <p>Working directory</p> Source code in <code>lume_services/services/scheduling/backends/local.py</code> <pre><code>class LocalRunConfig(RunConfig):\n    \"\"\"Local run configuration. If no directory is found at the filepath passed as\n    working_dir, an error will be raised.\n\n    Attributes:\n        labels (Optional[List[str]]): an list of labels to apply to this run\n            config. Labels are string identifiers used by Prefect Agents for selecting\n            valid flow runs when polling for work\n        env (Optional[Dict[str, str]]): Dictionary of environment variables to use for\n            run\n        working_dir (Optional[str]): Working directory\n\n    \"\"\"\n\n    env: Optional[Dict[str, str]]\n    working_dir: Optional[str] = str(os.getcwd())\n\n    @validator(\"working_dir\", pre=True)\n    def validate(cls, v):\n        \"\"\"Pydantic validator checking working directory existence\"\"\"\n        if not os.path.isdir(v):\n            raise FileNotFoundError(\"No directory found at %s\", v)\n\n        return v\n\n    def build(self) -&gt; LocalRun:\n        \"\"\"Method for converting to Prefect RunConfig type LocalRun.\n\n        Returns:\n            LocalRun\n\n        \"\"\"\n        return LocalRun(**self.dict(exclude_none=True))\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.local.LocalRunConfig.build","title":"<code>build()</code>","text":"<p>Method for converting to Prefect RunConfig type LocalRun.</p> <p>Returns:</p> Type Description <code>LocalRun</code> <p>LocalRun</p> Source code in <code>lume_services/services/scheduling/backends/local.py</code> <pre><code>def build(self) -&gt; LocalRun:\n    \"\"\"Method for converting to Prefect RunConfig type LocalRun.\n\n    Returns:\n        LocalRun\n\n    \"\"\"\n    return LocalRun(**self.dict(exclude_none=True))\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.local.LocalRunConfig.validate","title":"<code>validate(v)</code>","text":"<p>Pydantic validator checking working directory existence</p> Source code in <code>lume_services/services/scheduling/backends/local.py</code> <pre><code>@validator(\"working_dir\", pre=True)\ndef validate(cls, v):\n    \"\"\"Pydantic validator checking working directory existence\"\"\"\n    if not os.path.isdir(v):\n        raise FileNotFoundError(\"No directory found at %s\", v)\n\n    return v\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.server.ServerBackend","title":"<code>ServerBackend</code>","text":"<p>               Bases: <code>Backend</code></p> <p>Abstract backend used for connecting to a Prefect server.</p> <p>Prefect manages its own contexts for the purpose of registering flow objects etc. This introduced issues with management of clients, namely that even after setting the prefect configuration in the PrefectConfig.apply method, the original cloud context was still being used to construct the client. For this reason, all clients are constructed inside a context constructed from the backend configuration.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>PrefectConfig</code> <p>Instantiated PrefectConfig object describing connection to Prefect server.</p> <code>default_image</code> <code>str</code> <p>Default image used for registering flow storage.</p> Source code in <code>lume_services/services/scheduling/backends/server.py</code> <pre><code>class ServerBackend(Backend):\n    \"\"\"Abstract backend used for connecting to a Prefect server.\n\n    Prefect manages its own contexts for the purpose of registering flow objects\n    etc. This introduced issues with management of clients, namely that even after\n    setting the prefect configuration in the PrefectConfig.apply method, the\n    original cloud context was still being used to construct the client. For this\n    reason, all clients are constructed inside a context constructed from the backend\n    configuration.\n\n\n    Attributes:\n        config (PrefectConfig): Instantiated PrefectConfig object describing connection\n            to Prefect server.\n        default_image (str): Default image used for registering flow storage.\n\n    \"\"\"\n\n    config: PrefectConfig\n\n    class Config:\n        underscore_attrs_are_private = True\n\n    @abstractproperty\n    def run_config_type(self) -&gt; PrefectRunConfig:\n        \"\"\"Abstract property that must return the Prefect RunConfig type pertinent to\n        the Backend implementation.\n\n        \"\"\"\n        ...\n\n    def create_project(self, project_name: str) -&gt; None:\n        \"\"\"Create a Prefect project.\n\n        Args:\n            project_name (str): Create a named Prefect project.\n\n        Raises:\n            prefect.errors.ClientError: if the GraphQL query is bad for any reason\n\n        \"\"\"\n        with prefect.context(config=self.config.apply()):\n            client = Client()\n            client.create_project(project_name=project_name)\n\n    def register_flow(\n        self,\n        flow: Flow,\n        project_name: str,\n        image: str = None,\n        labels: List[str] = None,\n        idempotency_key: str = None,\n        version_group_id: str = None,\n        build: bool = True,\n        no_url: bool = False,\n        set_schedule_active: bool = True,\n    ) -&gt; str:\n        \"\"\"Register a flow with Prefect.\n\n        Args:\n            flow (Flow): Prefect flow to register\n            project_name (str): Name of project to register flow to\n            image (str): Name of Docker image to run flow inside. If not specified,\n                this will use the default image packaged with this repository.\n            build (bool): Whether the flows storage should be built prior to\n                serialization. By default lume-services flows use the same\n                image for execution with additional packages passed for installation\n                configured at runtime.\n            labels (Optional[List[str]]): A list of labels to add to this Flow.\n            idempotency_key (Optional[str]): a key that, if matching the most recent\n                registration call for this flow group, will prevent the creation of\n                another flow version and return the existing flow id instead.\n            version_group_id (Optional[str]): The UUID version group ID to use for\n                versioning this Flow in Cloud. If not provided, the version group ID\n                associated with this Flow's project and name will be used.\n            no_url (Optional[bool]): If True, the stdout from this function will not\n                contain the URL link to the newly-registered flow in the UI\n            set_schedule_active (Optional[bool]): If False, will set the schedule to\n                inactive in the database to prevent auto-scheduling runs (if the Flow\n                has a schedule)\n\n        Returns:\n            str: ID of registered flow\n\n        Notes:\n            prefect registration idempotency key omitted and version group...\n\n        Raises:\n            prefect.errors.ClientError: if the GraphQL query is bad for any reason\n\n        \"\"\"\n        if not image:\n            image = self.default_image\n\n        # configure run config for backend\n        run_config = self.run_config_type(image=image)\n        flow.run_config = run_config.build()\n        if labels is not None:\n            logger.info(\n                \"Flow run config is not empty. Clearing existing labels and assigning \\\n                    new.\"\n            )\n            flow.run_config.labels = set(labels)\n\n        flow.run_config.image_tag = image\n\n        with prefect.context(config=self.config.apply()):\n            flow_id = flow.register(\n                project_name=project_name,\n                build=build,\n                set_schedule_active=set_schedule_active,\n                version_group_id=version_group_id,\n                no_url=no_url,\n                idempotency_key=idempotency_key,\n            )\n\n        return flow_id\n\n    def load_flow(self, flow_name: str, project_name: str) -&gt; dict:\n        \"\"\"Load a Prefect flow object.\n\n        Args:\n            flow_name (str): Name of flow.\n            project_name (str): Name of project flow is registered with.\n\n        Returns:\n            dict: Dictionary with keys \"flow_id\" and \"flow\"\n\n        Raises:\n            prefect.errors.ClientError: if the GraphQL query is bad for any reason\n\n        \"\"\"\n\n        flow_view = FlowView.from_flow_name(\n            flow_name, project_name=project_name, last_updated=True\n        )\n        with prefect.context(config=self.config.apply()):\n            flow_view = FlowView.from_flow_name(\n                flow_name, project_name=project_name, last_updated=True\n            )\n            return {\"flow_id\": flow_view.flow_id, \"flow\": flow_view.flow}\n\n    def run(\n        self,\n        parameters: Dict[str, Any] = None,\n        run_config: RunConfig = None,\n        *,\n        flow_id: str,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"Create a flow run for a flow.\n\n        Args:\n            flow_id (str): Flow identifier\n            parameters (Optional[Dict[str, Any]]): Dictionary mapping flow parameter\n                name to value\n            run_config (Optional[RunConfig]): RunConfig object to configure flow fun.\n            **kwargs: Keyword arguments to intantiate the RunConfig.\n\n        Returns:\n            str: ID of flow run\n\n        Raises:\n            prefect.errors.ClientError: if the GraphQL query is bad for any reason\n            docker.errors.DockerException: Run configuration error for docker api.\n            pydantic.ValidationError: Error validating run configuration.\n            ValueError: Value error on flow run\n        \"\"\"\n        if run_config is not None and len(kwargs):\n            warnings.warn(\n                \"Both run_config and kwargs passed to Backend.run. Flow\\\n                will execute using passed run_config.\"\n            )\n\n        with prefect.context(config=self.config.apply()):\n            client = Client()\n\n            flow_view = FlowView.from_flow_id(flow_id)\n\n            # convert LUME-services run config to appropriate Prefect RunConfig object\n            if run_config is None:\n                run_config = self.run_config_type(\n                    env={\"PREFECT__CONTEXT__PROJECT_NAME\": flow_view.project_name},\n                    **kwargs,\n                )\n\n            prefect_run_config = run_config.build()\n\n            flow_run_id = client.create_flow_run(\n                flow_id=flow_id, parameters=parameters, run_config=prefect_run_config\n            )\n\n        return flow_run_id\n\n    def run_and_return(\n        self,\n        parameters: Dict[str, Any] = None,\n        run_config: RunConfig = None,\n        task_name: str = None,\n        *,\n        flow_id: str,\n        timeout: timedelta = timedelta(minutes=1),\n        cancel_on_timeout: bool = True,\n        **kwargs,\n    ):\n        \"\"\"Create a flow run for a flow and return the result.\n\n        Args:\n            parameters (Optional[Dict[str, Any]]): Dictionary mapping flow parameter\n                name to value\n            run_config (Optional[RunConfig]): RunConfig object to configure flow fun.\n            task_name (Optional[str]): Name of task to return result. If no task slug\n                is passed, will return the flow result.\n            flow_id (str): ID of flow to run.\n            timeout (timedelta): Time before stopping flow execution.\n            cancel_on_timeout (bool): Whether to cancel execution on timeout\n                error.\n            **kwargs: Keyword arguments to intantiate the RunConfig.\n\n        Raises:\n            EmptyResultError: No result is associated with the flow.\n            TaskNotCompletedError: Result reference task was not completed.\n            RuntimeError: Flow did not complete within given timeout.\n            prefect.errors.ClientError: if the GraphQL query is bad for any reason\n            docker.errors.DockerException: Run configuration error for docker api.\n            pydantic.ValidationError: Error validating run configuration.\n            TaskNotInFlowError: Provided task slug not in flow.\n            ValueError: Value error on flow run\n        \"\"\"\n        if run_config is not None and len(kwargs):\n            warnings.warn(\n                \"Both run_config and kwargs passed to Backend.run. Flow\\\n                will execute using passed run_config.\"\n            )\n\n        with prefect.context(config=self.config.apply()):\n            client = Client()\n\n            flow_view = FlowView.from_flow_id(flow_id)\n\n            # convert LUME-services run config to appropriate Prefect RunConfig object\n            if run_config is None:\n                run_config = self.run_config_type(\n                    env={\"PREFECT__CONTEXT__PROJECT_NAME\": flow_view.project_name},\n                    **kwargs,\n                )\n\n            logger.info(\n                \"Creating Prefect flow run for %s with parameters %s and run_config %s\",\n                flow_id,\n                parameters,\n                run_config.json(),\n            )\n\n            prefect_run_config = run_config.build()\n\n            flow_run_id = client.create_flow_run(\n                flow_id=flow_id, parameters=parameters, run_config=prefect_run_config\n            )\n\n            # watch flow run and stream logs until timeout\n            try:\n                for log in watch_flow_run(\n                    flow_run_id,\n                    stream_states=True,\n                    stream_logs=True,\n                    max_duration=timeout,\n                ):\n                    logger.info(log)\n            except RuntimeError as err:\n                if cancel_on_timeout:\n                    client.cancel_flow_run(flow_run_id=flow_run_id)\n                raise err\n\n            logger.debug(\"Watched flow completed.\")\n            flow_run = FlowRunView.from_flow_run_id(flow_run_id)\n\n            # check state\n            if flow_run.state.is_failed():\n                logger.exception(flow_run.state.message)\n                raise FlowFailedError(\n                    flow_id=flow_run.flow_id,\n                    flow_run_id=flow_run.flow_run_id,\n                    exception_message=flow_run.state.message,\n                )\n\n            task_runs = flow_run.get_all_task_runs()\n\n            # populate tasks\n            results = {}\n            for task_run in task_runs:\n                slug = task_run.task_slug\n                if not task_run.state.is_successful():\n                    raise TaskNotCompletedError(slug, flow_id, flow_run_id)\n\n                try:\n                    res = task_run.get_result()\n                # location is not set, no result\n                except ValueError:\n                    res = None\n\n                results[slug] = res\n\n        # get task run\n        if task_name is not None:\n            # filter tasks based on name\n            task_runs = {\n                slug: res for slug, res in results.items() if task_name in slug\n            }\n            logger.debug(task_runs)\n\n            if not len(task_runs):\n                raise TaskNotInFlowError(\n                    flow_name=flow_view.name,\n                    project_name=flow_view.project_name,\n                    task_name=task_name,\n                )\n\n            if len(task_runs) == 1:\n                res = list(task_runs.values())[0]\n                if res is None:\n                    raise EmptyResultError(flow_id, flow_run_id, slug)\n\n                return res\n\n            else:\n                return task_runs\n\n        # assume flow result, return all results\n        else:\n            return results\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.server.ServerBackend.create_project","title":"<code>create_project(project_name)</code>","text":"<p>Create a Prefect project.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Create a named Prefect project.</p> required <p>Raises:</p> Type Description <code>ClientError</code> <p>if the GraphQL query is bad for any reason</p> Source code in <code>lume_services/services/scheduling/backends/server.py</code> <pre><code>def create_project(self, project_name: str) -&gt; None:\n    \"\"\"Create a Prefect project.\n\n    Args:\n        project_name (str): Create a named Prefect project.\n\n    Raises:\n        prefect.errors.ClientError: if the GraphQL query is bad for any reason\n\n    \"\"\"\n    with prefect.context(config=self.config.apply()):\n        client = Client()\n        client.create_project(project_name=project_name)\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.server.ServerBackend.load_flow","title":"<code>load_flow(flow_name, project_name)</code>","text":"<p>Load a Prefect flow object.</p> <p>Parameters:</p> Name Type Description Default <code>flow_name</code> <code>str</code> <p>Name of flow.</p> required <code>project_name</code> <code>str</code> <p>Name of project flow is registered with.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary with keys \"flow_id\" and \"flow\"</p> <p>Raises:</p> Type Description <code>ClientError</code> <p>if the GraphQL query is bad for any reason</p> Source code in <code>lume_services/services/scheduling/backends/server.py</code> <pre><code>def load_flow(self, flow_name: str, project_name: str) -&gt; dict:\n    \"\"\"Load a Prefect flow object.\n\n    Args:\n        flow_name (str): Name of flow.\n        project_name (str): Name of project flow is registered with.\n\n    Returns:\n        dict: Dictionary with keys \"flow_id\" and \"flow\"\n\n    Raises:\n        prefect.errors.ClientError: if the GraphQL query is bad for any reason\n\n    \"\"\"\n\n    flow_view = FlowView.from_flow_name(\n        flow_name, project_name=project_name, last_updated=True\n    )\n    with prefect.context(config=self.config.apply()):\n        flow_view = FlowView.from_flow_name(\n            flow_name, project_name=project_name, last_updated=True\n        )\n        return {\"flow_id\": flow_view.flow_id, \"flow\": flow_view.flow}\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.server.ServerBackend.register_flow","title":"<code>register_flow(flow, project_name, image=None, labels=None, idempotency_key=None, version_group_id=None, build=True, no_url=False, set_schedule_active=True)</code>","text":"<p>Register a flow with Prefect.</p> <p>Parameters:</p> Name Type Description Default <code>flow</code> <code>Flow</code> <p>Prefect flow to register</p> required <code>project_name</code> <code>str</code> <p>Name of project to register flow to</p> required <code>image</code> <code>str</code> <p>Name of Docker image to run flow inside. If not specified, this will use the default image packaged with this repository.</p> <code>None</code> <code>build</code> <code>bool</code> <p>Whether the flows storage should be built prior to serialization. By default lume-services flows use the same image for execution with additional packages passed for installation configured at runtime.</p> <code>True</code> <code>labels</code> <code>Optional[List[str]]</code> <p>A list of labels to add to this Flow.</p> <code>None</code> <code>idempotency_key</code> <code>Optional[str]</code> <p>a key that, if matching the most recent registration call for this flow group, will prevent the creation of another flow version and return the existing flow id instead.</p> <code>None</code> <code>version_group_id</code> <code>Optional[str]</code> <p>The UUID version group ID to use for versioning this Flow in Cloud. If not provided, the version group ID associated with this Flow's project and name will be used.</p> <code>None</code> <code>no_url</code> <code>Optional[bool]</code> <p>If True, the stdout from this function will not contain the URL link to the newly-registered flow in the UI</p> <code>False</code> <code>set_schedule_active</code> <code>Optional[bool]</code> <p>If False, will set the schedule to inactive in the database to prevent auto-scheduling runs (if the Flow has a schedule)</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>ID of registered flow</p> Notes <p>prefect registration idempotency key omitted and version group...</p> <p>Raises:</p> Type Description <code>ClientError</code> <p>if the GraphQL query is bad for any reason</p> Source code in <code>lume_services/services/scheduling/backends/server.py</code> <pre><code>def register_flow(\n    self,\n    flow: Flow,\n    project_name: str,\n    image: str = None,\n    labels: List[str] = None,\n    idempotency_key: str = None,\n    version_group_id: str = None,\n    build: bool = True,\n    no_url: bool = False,\n    set_schedule_active: bool = True,\n) -&gt; str:\n    \"\"\"Register a flow with Prefect.\n\n    Args:\n        flow (Flow): Prefect flow to register\n        project_name (str): Name of project to register flow to\n        image (str): Name of Docker image to run flow inside. If not specified,\n            this will use the default image packaged with this repository.\n        build (bool): Whether the flows storage should be built prior to\n            serialization. By default lume-services flows use the same\n            image for execution with additional packages passed for installation\n            configured at runtime.\n        labels (Optional[List[str]]): A list of labels to add to this Flow.\n        idempotency_key (Optional[str]): a key that, if matching the most recent\n            registration call for this flow group, will prevent the creation of\n            another flow version and return the existing flow id instead.\n        version_group_id (Optional[str]): The UUID version group ID to use for\n            versioning this Flow in Cloud. If not provided, the version group ID\n            associated with this Flow's project and name will be used.\n        no_url (Optional[bool]): If True, the stdout from this function will not\n            contain the URL link to the newly-registered flow in the UI\n        set_schedule_active (Optional[bool]): If False, will set the schedule to\n            inactive in the database to prevent auto-scheduling runs (if the Flow\n            has a schedule)\n\n    Returns:\n        str: ID of registered flow\n\n    Notes:\n        prefect registration idempotency key omitted and version group...\n\n    Raises:\n        prefect.errors.ClientError: if the GraphQL query is bad for any reason\n\n    \"\"\"\n    if not image:\n        image = self.default_image\n\n    # configure run config for backend\n    run_config = self.run_config_type(image=image)\n    flow.run_config = run_config.build()\n    if labels is not None:\n        logger.info(\n            \"Flow run config is not empty. Clearing existing labels and assigning \\\n                new.\"\n        )\n        flow.run_config.labels = set(labels)\n\n    flow.run_config.image_tag = image\n\n    with prefect.context(config=self.config.apply()):\n        flow_id = flow.register(\n            project_name=project_name,\n            build=build,\n            set_schedule_active=set_schedule_active,\n            version_group_id=version_group_id,\n            no_url=no_url,\n            idempotency_key=idempotency_key,\n        )\n\n    return flow_id\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.server.ServerBackend.run","title":"<code>run(parameters=None, run_config=None, *, flow_id, **kwargs)</code>","text":"<p>Create a flow run for a flow.</p> <p>Parameters:</p> Name Type Description Default <code>flow_id</code> <code>str</code> <p>Flow identifier</p> required <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary mapping flow parameter name to value</p> <code>None</code> <code>run_config</code> <code>Optional[RunConfig]</code> <p>RunConfig object to configure flow fun.</p> <code>None</code> <code>**kwargs</code> <p>Keyword arguments to intantiate the RunConfig.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>ID of flow run</p> <p>Raises:</p> Type Description <code>ClientError</code> <p>if the GraphQL query is bad for any reason</p> <code>DockerException</code> <p>Run configuration error for docker api.</p> <code>ValidationError</code> <p>Error validating run configuration.</p> <code>ValueError</code> <p>Value error on flow run</p> Source code in <code>lume_services/services/scheduling/backends/server.py</code> <pre><code>def run(\n    self,\n    parameters: Dict[str, Any] = None,\n    run_config: RunConfig = None,\n    *,\n    flow_id: str,\n    **kwargs,\n) -&gt; str:\n    \"\"\"Create a flow run for a flow.\n\n    Args:\n        flow_id (str): Flow identifier\n        parameters (Optional[Dict[str, Any]]): Dictionary mapping flow parameter\n            name to value\n        run_config (Optional[RunConfig]): RunConfig object to configure flow fun.\n        **kwargs: Keyword arguments to intantiate the RunConfig.\n\n    Returns:\n        str: ID of flow run\n\n    Raises:\n        prefect.errors.ClientError: if the GraphQL query is bad for any reason\n        docker.errors.DockerException: Run configuration error for docker api.\n        pydantic.ValidationError: Error validating run configuration.\n        ValueError: Value error on flow run\n    \"\"\"\n    if run_config is not None and len(kwargs):\n        warnings.warn(\n            \"Both run_config and kwargs passed to Backend.run. Flow\\\n            will execute using passed run_config.\"\n        )\n\n    with prefect.context(config=self.config.apply()):\n        client = Client()\n\n        flow_view = FlowView.from_flow_id(flow_id)\n\n        # convert LUME-services run config to appropriate Prefect RunConfig object\n        if run_config is None:\n            run_config = self.run_config_type(\n                env={\"PREFECT__CONTEXT__PROJECT_NAME\": flow_view.project_name},\n                **kwargs,\n            )\n\n        prefect_run_config = run_config.build()\n\n        flow_run_id = client.create_flow_run(\n            flow_id=flow_id, parameters=parameters, run_config=prefect_run_config\n        )\n\n    return flow_run_id\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.server.ServerBackend.run_and_return","title":"<code>run_and_return(parameters=None, run_config=None, task_name=None, *, flow_id, timeout=timedelta(minutes=1), cancel_on_timeout=True, **kwargs)</code>","text":"<p>Create a flow run for a flow and return the result.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary mapping flow parameter name to value</p> <code>None</code> <code>run_config</code> <code>Optional[RunConfig]</code> <p>RunConfig object to configure flow fun.</p> <code>None</code> <code>task_name</code> <code>Optional[str]</code> <p>Name of task to return result. If no task slug is passed, will return the flow result.</p> <code>None</code> <code>flow_id</code> <code>str</code> <p>ID of flow to run.</p> required <code>timeout</code> <code>timedelta</code> <p>Time before stopping flow execution.</p> <code>timedelta(minutes=1)</code> <code>cancel_on_timeout</code> <code>bool</code> <p>Whether to cancel execution on timeout error.</p> <code>True</code> <code>**kwargs</code> <p>Keyword arguments to intantiate the RunConfig.</p> <code>{}</code> <p>Raises:</p> Type Description <code>EmptyResultError</code> <p>No result is associated with the flow.</p> <code>TaskNotCompletedError</code> <p>Result reference task was not completed.</p> <code>RuntimeError</code> <p>Flow did not complete within given timeout.</p> <code>ClientError</code> <p>if the GraphQL query is bad for any reason</p> <code>DockerException</code> <p>Run configuration error for docker api.</p> <code>ValidationError</code> <p>Error validating run configuration.</p> <code>TaskNotInFlowError</code> <p>Provided task slug not in flow.</p> <code>ValueError</code> <p>Value error on flow run</p> Source code in <code>lume_services/services/scheduling/backends/server.py</code> <pre><code>def run_and_return(\n    self,\n    parameters: Dict[str, Any] = None,\n    run_config: RunConfig = None,\n    task_name: str = None,\n    *,\n    flow_id: str,\n    timeout: timedelta = timedelta(minutes=1),\n    cancel_on_timeout: bool = True,\n    **kwargs,\n):\n    \"\"\"Create a flow run for a flow and return the result.\n\n    Args:\n        parameters (Optional[Dict[str, Any]]): Dictionary mapping flow parameter\n            name to value\n        run_config (Optional[RunConfig]): RunConfig object to configure flow fun.\n        task_name (Optional[str]): Name of task to return result. If no task slug\n            is passed, will return the flow result.\n        flow_id (str): ID of flow to run.\n        timeout (timedelta): Time before stopping flow execution.\n        cancel_on_timeout (bool): Whether to cancel execution on timeout\n            error.\n        **kwargs: Keyword arguments to intantiate the RunConfig.\n\n    Raises:\n        EmptyResultError: No result is associated with the flow.\n        TaskNotCompletedError: Result reference task was not completed.\n        RuntimeError: Flow did not complete within given timeout.\n        prefect.errors.ClientError: if the GraphQL query is bad for any reason\n        docker.errors.DockerException: Run configuration error for docker api.\n        pydantic.ValidationError: Error validating run configuration.\n        TaskNotInFlowError: Provided task slug not in flow.\n        ValueError: Value error on flow run\n    \"\"\"\n    if run_config is not None and len(kwargs):\n        warnings.warn(\n            \"Both run_config and kwargs passed to Backend.run. Flow\\\n            will execute using passed run_config.\"\n        )\n\n    with prefect.context(config=self.config.apply()):\n        client = Client()\n\n        flow_view = FlowView.from_flow_id(flow_id)\n\n        # convert LUME-services run config to appropriate Prefect RunConfig object\n        if run_config is None:\n            run_config = self.run_config_type(\n                env={\"PREFECT__CONTEXT__PROJECT_NAME\": flow_view.project_name},\n                **kwargs,\n            )\n\n        logger.info(\n            \"Creating Prefect flow run for %s with parameters %s and run_config %s\",\n            flow_id,\n            parameters,\n            run_config.json(),\n        )\n\n        prefect_run_config = run_config.build()\n\n        flow_run_id = client.create_flow_run(\n            flow_id=flow_id, parameters=parameters, run_config=prefect_run_config\n        )\n\n        # watch flow run and stream logs until timeout\n        try:\n            for log in watch_flow_run(\n                flow_run_id,\n                stream_states=True,\n                stream_logs=True,\n                max_duration=timeout,\n            ):\n                logger.info(log)\n        except RuntimeError as err:\n            if cancel_on_timeout:\n                client.cancel_flow_run(flow_run_id=flow_run_id)\n            raise err\n\n        logger.debug(\"Watched flow completed.\")\n        flow_run = FlowRunView.from_flow_run_id(flow_run_id)\n\n        # check state\n        if flow_run.state.is_failed():\n            logger.exception(flow_run.state.message)\n            raise FlowFailedError(\n                flow_id=flow_run.flow_id,\n                flow_run_id=flow_run.flow_run_id,\n                exception_message=flow_run.state.message,\n            )\n\n        task_runs = flow_run.get_all_task_runs()\n\n        # populate tasks\n        results = {}\n        for task_run in task_runs:\n            slug = task_run.task_slug\n            if not task_run.state.is_successful():\n                raise TaskNotCompletedError(slug, flow_id, flow_run_id)\n\n            try:\n                res = task_run.get_result()\n            # location is not set, no result\n            except ValueError:\n                res = None\n\n            results[slug] = res\n\n    # get task run\n    if task_name is not None:\n        # filter tasks based on name\n        task_runs = {\n            slug: res for slug, res in results.items() if task_name in slug\n        }\n        logger.debug(task_runs)\n\n        if not len(task_runs):\n            raise TaskNotInFlowError(\n                flow_name=flow_view.name,\n                project_name=flow_view.project_name,\n                task_name=task_name,\n            )\n\n        if len(task_runs) == 1:\n            res = list(task_runs.values())[0]\n            if res is None:\n                raise EmptyResultError(flow_id, flow_run_id, slug)\n\n            return res\n\n        else:\n            return task_runs\n\n    # assume flow result, return all results\n    else:\n        return results\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.server.ServerBackend.run_config_type","title":"<code>run_config_type()</code>","text":"<p>Abstract property that must return the Prefect RunConfig type pertinent to the Backend implementation.</p> Source code in <code>lume_services/services/scheduling/backends/server.py</code> <pre><code>@abstractproperty\ndef run_config_type(self) -&gt; PrefectRunConfig:\n    \"\"\"Abstract property that must return the Prefect RunConfig type pertinent to\n    the Backend implementation.\n\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.docker.DockerBackend","title":"<code>DockerBackend</code>","text":"<p>               Bases: <code>ServerBackend</code></p> <p>Implementation of Backend used for interacting with prefect deployed in cluster of Docker containers, as with docker-compose.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>PrefectConfig</code> <p>Instantiated PrefectConfig object describing connection to Prefect server.</p> <code>_client</code> <code>Client</code> <p>Prefect client connection created on instantiation.</p> <code>_run_config_type</code> <code>type</code> <p>Type used to compose Prefect run configuration.</p> Source code in <code>lume_services/services/scheduling/backends/docker.py</code> <pre><code>class DockerBackend(ServerBackend):\n    \"\"\"Implementation of Backend used for interacting with prefect deployed in\n    cluster of Docker containers, as with docker-compose.\n\n    Attributes:\n        config (PrefectConfig): Instantiated PrefectConfig object describing connection\n            to Prefect server.\n        _client (Client): Prefect client connection created on instantiation.\n        _run_config_type (type): Type used to compose Prefect run configuration.\n\n    \"\"\"\n\n    _run_config_type: type = DockerRunConfig\n\n    @property\n    def run_config_type(self):\n        return self._run_config_type\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.docker.DockerRunConfig","title":"<code>DockerRunConfig</code>","text":"<p>               Bases: <code>RunConfig</code></p> <p>Pydantic representation of a Docker Prefect run configuration: https://docs.prefect.io/api/latest/run_configs.html#dockerrun</p> <p>Attributes:</p> Name Type Description <code>labels</code> <code>Optional[List[str]]</code> <p>an list of labels to apply to this run config. Labels are string identifiers used by Prefect Agents for selecting valid flow runs when polling for work</p> <code>env</code> <code>Optional[dict]</code> <p>Additional environment variables to set on the job</p> <code>image</code> <code>str</code> <p>Tag of image in which flow should run.</p> <code>host_config</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary representing runtime args to be passed to Docker agent. Full documentation of args can be found here: https://docker-py.readthedocs.io/en/stable/api.html#docker.api.container.ContainerApiMixin.create_host_config</p> <code>ports</code> <code>Optional[List[int]]</code> <p>An list of ports numbers to expose on container.</p> Source code in <code>lume_services/services/scheduling/backends/docker.py</code> <pre><code>class DockerRunConfig(RunConfig):\n    \"\"\"Pydantic representation of a Docker Prefect run configuration:\n    https://docs.prefect.io/api/latest/run_configs.html#dockerrun\n\n    Attributes:\n        labels (Optional[List[str]]): an list of labels to apply to this run\n            config. Labels are string identifiers used by Prefect Agents for selecting\n            valid flow runs when polling for work\n        env (Optional[dict]): Additional environment variables to set on the job\n        image (str): Tag of image in which flow should run.\n        host_config (Optional[Dict[str, Any]]): Dictionary representing runtime args\n            to be passed to Docker agent. Full documentation of args can be found here:\n            https://docker-py.readthedocs.io/en/stable/api.html#docker.api.container.ContainerApiMixin.create_host_config\n        ports (Optional[List[int]]): An list of ports numbers to expose on\n            container.\n\n    \"\"\"  # noqa\n\n    image: str\n    host_config: Dict[str, Any] = None\n    ports: Optional[List[int]]\n\n    @validator(\"host_config\", pre=True)\n    def validate_host_config(cls, v):\n        \"\"\"Composes a model for the Docker host configuration and applies any passed\n        values.\n\n        \"\"\"\n        if isinstance(v, (dict,)):\n            # test host config composition using api version\n            try:\n                HostConfig(version=docker_api_version(), **v)\n            except Exception as e:\n                logger.exception(e)\n                raise e\n\n        return v\n\n    def build(self) -&gt; DockerRun:\n        \"\"\"Method for converting to Prefect RunConfig type DockerRun.\n\n        Returns:\n            DockerRun\n\n        \"\"\"\n        return DockerRun(**self.dict(exclude_none=True))\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.docker.DockerRunConfig.build","title":"<code>build()</code>","text":"<p>Method for converting to Prefect RunConfig type DockerRun.</p> <p>Returns:</p> Type Description <code>DockerRun</code> <p>DockerRun</p> Source code in <code>lume_services/services/scheduling/backends/docker.py</code> <pre><code>def build(self) -&gt; DockerRun:\n    \"\"\"Method for converting to Prefect RunConfig type DockerRun.\n\n    Returns:\n        DockerRun\n\n    \"\"\"\n    return DockerRun(**self.dict(exclude_none=True))\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.docker.DockerRunConfig.validate_host_config","title":"<code>validate_host_config(v)</code>","text":"<p>Composes a model for the Docker host configuration and applies any passed values.</p> Source code in <code>lume_services/services/scheduling/backends/docker.py</code> <pre><code>@validator(\"host_config\", pre=True)\ndef validate_host_config(cls, v):\n    \"\"\"Composes a model for the Docker host configuration and applies any passed\n    values.\n\n    \"\"\"\n    if isinstance(v, (dict,)):\n        # test host config composition using api version\n        try:\n            HostConfig(version=docker_api_version(), **v)\n        except Exception as e:\n            logger.exception(e)\n            raise e\n\n    return v\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.kubernetes.KubernetesBackend","title":"<code>KubernetesBackend</code>","text":"<p>               Bases: <code>ServerBackend</code></p> <p>Implementation of Backend used for interacting with Prefect deployed in K8 cluster.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>PrefectConfig</code> <p>Instantiated PrefectConfig object describing connection to Prefect server.</p> <code>_client</code> <code>Client</code> <p>Prefect client connection created on instantiation.</p> <code>_run_config_type</code> <code>type</code> <p>Type used to compose run configuration.</p> Source code in <code>lume_services/services/scheduling/backends/kubernetes.py</code> <pre><code>class KubernetesBackend(ServerBackend):\n    \"\"\"Implementation of Backend used for interacting with Prefect deployed in\n    K8 cluster.\n\n    Attributes:\n        config (PrefectConfig): Instantiated PrefectConfig object describing connection\n            to Prefect server.\n        _client (Client): Prefect client connection created on instantiation.\n        _run_config_type (type): Type used to compose run configuration.\n\n    \"\"\"\n\n    _run_config_type: type = KubernetesRunConfig\n\n    @property\n    def run_config_type(self):\n        return self._run_config_type\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.kubernetes.KubernetesRunConfig","title":"<code>KubernetesRunConfig</code>","text":"<p>               Bases: <code>RunConfig</code></p> <p>Pydantic representation of args to: https://docs.prefect.io/api/latest/run_configs.html#kubernetesrun https://kubernetes.io/docs/concepts/configuration/overview/#container-images</p> <p>Attributes:</p> Name Type Description <code>labels</code> <code>Optional[List[str]]</code> <p>an list of labels to apply to this run config. Labels are string identifiers used by Prefect Agents for selecting valid flow runs when polling for work</p> <code>env</code> <code>Optional[dict]</code> <p>Additional environment variables to set on the job</p> <code>image</code> <code>Optional[str]</code> <p>The image to use. Can also be specified via job template.</p> <code>job_template_path</code> <code>Optional[str]</code> <p>Path to a job template to use. If a local path (no file scheme, or a file/local scheme), the job template will be loaded on initialization and stored on the KubernetesRun object as the job_template field. Otherwise the job template will be loaded at runtime on the agent. Supported runtime file schemes include (s3, gcs, and agent (for paths local to the runtime agent)).</p> <code>job_template</code> <code>Optional[str]</code> <p>An in-memory job template to use.</p> <code>cpu_limit</code> <code>Union[float, str]</code> <p>The CPU limit to use for the job</p> <code>cpu_request</code> <code>Union[float, str]</code> <p>The CPU request to use for the job</p> <code>memory_limit</code> <code>Optional[str]</code> <p>The memory limit to use for the job</p> <code>memory_request</code> <code>Optional[str]</code> <p>The memory request to use for the job</p> <code>service_account_name</code> <code>Optional[str]</code> <p>A service account name to use for this job. If present, overrides any service account configured on the agent or in the job template.</p> <code>image_pull_secrets</code> <code>Optional[list]</code> <p>A list of image pull secrets to use for this job. If present, overrides any image pull secrets configured on the agent or in the job template.</p> <code>image_pull_policy</code> <code>Optional[str]</code> <p>The imagePullPolicy to use for the job.</p> Source code in <code>lume_services/services/scheduling/backends/kubernetes.py</code> <pre><code>class KubernetesRunConfig(RunConfig):\n    \"\"\"Pydantic representation of args to:\n    https://docs.prefect.io/api/latest/run_configs.html#kubernetesrun\n    https://kubernetes.io/docs/concepts/configuration/overview/#container-images\n\n    Attributes:\n        labels (Optional[List[str]]): an list of labels to apply to this run\n            config. Labels are string identifiers used by Prefect Agents for selecting\n            valid flow runs when polling for work\n        env (Optional[dict]): Additional environment variables to set on the job\n        image (Optional[str]): The image to use. Can also be specified via job\n            template.\n        job_template_path (Optional[str]): Path to a job template to use. If a local\n            path (no file scheme, or a file/local scheme), the job template will be\n            loaded on initialization and stored on the KubernetesRun object as the\n            job_template field. Otherwise the job template will be loaded at runtime\n            on the agent. Supported runtime file schemes include (s3, gcs, and agent\n            (for paths local to the runtime agent)).\n        job_template (Optional[str]): An in-memory job template to use.\n        cpu_limit (Union[float, str]): The CPU limit to use for the job\n        cpu_request (Union[float, str]): The CPU request to use for the job\n        memory_limit (Optional[str]): The memory limit to use for the job\n        memory_request (Optional[str]): The memory request to use for the job\n        service_account_name (Optional[str]): A service account name to use for this\n            job. If present, overrides any service account configured on the agent or\n            in the job template.\n        image_pull_secrets (Optional[list]): A list of image pull secrets to use for\n            this job. If present, overrides any image pull secrets configured on the\n            agent or in the job template.\n        image_pull_policy (Optional[str]): The imagePullPolicy to use for the job.\n\n    \"\"\"\n\n    image: Optional[str]\n    image_pull_secrets: Optional[List[str]]\n    job_template: Optional[dict]\n    job_template_path: Optional[str]\n    service_account_name: Optional[str]\n    image_pull_policy: Literal[\"Always\", \"IfNotPresent\", \"Never\"] = \"IfNotPresent\"\n    cpu_limit: Union[float, str] = 1.0\n    cpu_request: Union[float, str] = 0.5\n    memory_limit: Union[str, int] = None\n    memory_request: Union[str, int] = None\n\n    @validator(\"memory_limit\", \"memory_request\")\n    def validate_memory(cls, v):\n        \"\"\"Validate w.r.t. Kubernetes resource formats: int, fixed-point number using\n        quantity suffixes: E, P, T, G, M, k or power-of-two equivalents: Ei, Pi,\n        Ti, Gi, Mi, Ki\n\n        \"\"\"\n\n        if isinstance(v, (int,)):\n            return v\n\n        elif isinstance(v, (str,)):\n\n            acceptable = False\n\n            # check substrings\n            inclusions = [\n                substring for substring in KUBERNETES_REQUEST_SUFFIXES if substring in v\n            ]\n\n            if len(inclusions):\n\n                for inclusion in inclusions:\n\n                    try:\n                        stripped = v.replace(inclusion, \"\")\n                        _ = int(stripped)\n                        acceptable = True\n\n                    except ValueError:\n                        pass\n\n            if not acceptable:\n                logger.error(\"Kubernetes resource request invalid: %s\", v)\n                raise ValueError(f\"Kubernetes resource request invalid: {v}\")\n\n        else:\n            raise ValueError(\"Must provide string or int to request\")\n\n        return v\n\n    def build(self) -&gt; KubernetesRun:\n        \"\"\"Method for converting to Prefect RunConfig type KubernetesRun.\n\n        Returns:\n            KubernetesRun\n\n        \"\"\"\n        # if job template and job template path missing, use packaged template\n        if self.job_template is None and self.job_template_path is None:\n            self.job_template = KUBERNETES_JOB_TEMPLATE\n\n        return KubernetesRun(**self.dict(exclude_none=True))\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.kubernetes.KubernetesRunConfig.build","title":"<code>build()</code>","text":"<p>Method for converting to Prefect RunConfig type KubernetesRun.</p> <p>Returns:</p> Type Description <code>KubernetesRun</code> <p>KubernetesRun</p> Source code in <code>lume_services/services/scheduling/backends/kubernetes.py</code> <pre><code>def build(self) -&gt; KubernetesRun:\n    \"\"\"Method for converting to Prefect RunConfig type KubernetesRun.\n\n    Returns:\n        KubernetesRun\n\n    \"\"\"\n    # if job template and job template path missing, use packaged template\n    if self.job_template is None and self.job_template_path is None:\n        self.job_template = KUBERNETES_JOB_TEMPLATE\n\n    return KubernetesRun(**self.dict(exclude_none=True))\n</code></pre>"},{"location":"api/services/scheduling/backends/#lume_services.services.scheduling.backends.kubernetes.KubernetesRunConfig.validate_memory","title":"<code>validate_memory(v)</code>","text":"<p>Validate w.r.t. Kubernetes resource formats: int, fixed-point number using quantity suffixes: E, P, T, G, M, k or power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki</p> Source code in <code>lume_services/services/scheduling/backends/kubernetes.py</code> <pre><code>@validator(\"memory_limit\", \"memory_request\")\ndef validate_memory(cls, v):\n    \"\"\"Validate w.r.t. Kubernetes resource formats: int, fixed-point number using\n    quantity suffixes: E, P, T, G, M, k or power-of-two equivalents: Ei, Pi,\n    Ti, Gi, Mi, Ki\n\n    \"\"\"\n\n    if isinstance(v, (int,)):\n        return v\n\n    elif isinstance(v, (str,)):\n\n        acceptable = False\n\n        # check substrings\n        inclusions = [\n            substring for substring in KUBERNETES_REQUEST_SUFFIXES if substring in v\n        ]\n\n        if len(inclusions):\n\n            for inclusion in inclusions:\n\n                try:\n                    stripped = v.replace(inclusion, \"\")\n                    _ = int(stripped)\n                    acceptable = True\n\n                except ValueError:\n                    pass\n\n        if not acceptable:\n            logger.error(\"Kubernetes resource request invalid: %s\", v)\n            raise ValueError(f\"Kubernetes resource request invalid: {v}\")\n\n    else:\n        raise ValueError(\"Must provide string or int to request\")\n\n    return v\n</code></pre>"},{"location":"api/services/scheduling/scheduling/","title":"Scheduling Service","text":""},{"location":"api/services/scheduling/scheduling/#lume_services.services.scheduling.service.SchedulingService","title":"<code>SchedulingService</code>","text":"<p>Scheduler handling job submission with Prefect.</p> Source code in <code>lume_services/services/scheduling/service.py</code> <pre><code>class SchedulingService:\n    \"\"\"Scheduler handling job submission with Prefect.\"\"\"\n\n    def __init__(self, backend: Backend):\n        \"\"\"Initialize PrefectScheduler using configuration\n\n        Args:\n            backend (Backend): Scheduling service client configuration\n\n        \"\"\"\n\n        self.backend = backend\n\n    def create_project(self, project_name: str) -&gt; None:\n        \"\"\"Create a Prefect project.\n\n        Args:\n            project_name (str): Create a named Prefect project.\n\n        Raises:\n            prefect.errors.ClientError: if the GraphQL query is bad for any reason\n            lume_services.errors.LocalBackendError: Using local run configuration,\n                no server backend methods permitted.\n\n        \"\"\"\n        self.backend.create_project(project_name=project_name)\n\n    def register_flow(\n        self,\n        flow: Flow,\n        project_name: str,\n        image: Optional[str],\n        labels: Optional[List[str]],\n    ) -&gt; str:\n        \"\"\"Register a flow with Prefect. Backend implementations without server\n        connecton should raise errors when this method is called.\n\n        Args:\n            flow (Flow): Prefect flow to register.\n            project_name (str): Name of project to register flow to.\n            image (str): Name of Docker image to run flow inside.\n\n        Returns:\n            str: ID of registered flow.\n\n        Raises:\n            prefect.errors.ClientError: if the GraphQL query is bad for any reason\n            lume_services.errors.LocalBackendError: Using local run configuration, no\n                server backend methods permitted.\n\n        \"\"\"\n        return self.backend.register_flow(flow, project_name, image, labels=labels)\n\n    def load_flow(self, flow_name: str, project_name: str) -&gt; dict:\n        \"\"\"Load a Prefect flow object. Backend implementations without server connecton\n        should raise errors when this method is called.\n\n        Args:\n            flow_name (str): Name of flow.\n            project_name (str): Name of project flow is registered with.\n\n        Returns:\n            dict: Dictionary with keys \"flow_id\" and \"flow\"\n\n        Raises:\n            prefect.errors.ClientError: if the GraphQL query is bad for any reason\n            lume_services.errors.LocalBackendError: Using local run configuration, no\n                server backend methods permitted.\n\n        \"\"\"\n        return self.backend.load_flow(flow_name, project_name)\n\n    def run(\n        self, parameters: Dict[str, Any], run_config=None, **kwargs\n    ) -&gt; Union[str, None]:\n        \"\"\"Run a flow. Does not return result. Implementations should cover\n        instantiation of run_config from kwargs as well as backend-specific kwargs.\n\n        Args:\n            parameters (Dict[str, Any]): Dictionary mapping flow parameter\n                name to value\n            **kwargs: Keyword arguments for RunConfig init and backend-specific\n                execution.\n\n        Returns:\n            Union[str, None]: Return run_id in case of server backend, None in the case\n                of local execution.\n\n        Raises:\n            docker.errors.DockerException: Run configuration error for docker api.\n            pydantic.ValidationError: Error validating run configuration.\n            prefect.errors.ClientError: if the GraphQL query is bad for any reason\n            ValueError: Value error on flow run\n\n        \"\"\"\n        return self.backend.run(parameters, run_config=run_config, **kwargs)\n\n    def run_and_return(\n        self,\n        parameters: Optional[Dict[str, Any]],\n        run_config: Optional[RunConfig] = None,\n        task_name: Optional[str] = None,\n        **kwargs\n    ) -&gt; Any:\n        \"\"\"Run a flow and return result. Implementations should cover instantiation of\n        run_config from kwargs as well as backend-specific kwargs.\n\n        Args:\n            parameters (Optional[Dict[str, Any]]): Dictionary mapping flow parameter\n                name to value\n            run_config (Optional[RunConfig]): RunConfig object to configure flow fun.\n            task_name (Optional[str]): Name of task to return result. If no task slug\n                is passed, will return the flow result.\n            **kwargs: Keyword arguments for RunConfig init and backend-specific\n                execution.\n\n        Returns:\n            Any: Result of flow run.\n\n        Raises:\n            lume_services.errors.EmptyResultError: No result is associated with the\n                flow.\n            docker.errors.DockerException: Run configuration error for docker api.\n            pydantic.ValidationError: Error validating run configuration.\n            prefect.errors.ClientError: if the GraphQL query is bad for any reason\n            lume_services.errors.TaskNotInFlowError: Task slug does not exist in flow.\n            lume_services.errors.TaskNotCompletedError: Result reference task was not\n                completed.\n            RuntimeError: Flow did not complete within given timeout.\n            ValueError: Value error on flow run\n\n        \"\"\"\n        return self.backend.run_and_return(parameters, run_config, task_name, **kwargs)\n</code></pre>"},{"location":"api/services/scheduling/scheduling/#lume_services.services.scheduling.service.SchedulingService.__init__","title":"<code>__init__(backend)</code>","text":"<p>Initialize PrefectScheduler using configuration</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>Backend</code> <p>Scheduling service client configuration</p> required Source code in <code>lume_services/services/scheduling/service.py</code> <pre><code>def __init__(self, backend: Backend):\n    \"\"\"Initialize PrefectScheduler using configuration\n\n    Args:\n        backend (Backend): Scheduling service client configuration\n\n    \"\"\"\n\n    self.backend = backend\n</code></pre>"},{"location":"api/services/scheduling/scheduling/#lume_services.services.scheduling.service.SchedulingService.create_project","title":"<code>create_project(project_name)</code>","text":"<p>Create a Prefect project.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Create a named Prefect project.</p> required <p>Raises:</p> Type Description <code>ClientError</code> <p>if the GraphQL query is bad for any reason</p> <code>LocalBackendError</code> <p>Using local run configuration, no server backend methods permitted.</p> Source code in <code>lume_services/services/scheduling/service.py</code> <pre><code>def create_project(self, project_name: str) -&gt; None:\n    \"\"\"Create a Prefect project.\n\n    Args:\n        project_name (str): Create a named Prefect project.\n\n    Raises:\n        prefect.errors.ClientError: if the GraphQL query is bad for any reason\n        lume_services.errors.LocalBackendError: Using local run configuration,\n            no server backend methods permitted.\n\n    \"\"\"\n    self.backend.create_project(project_name=project_name)\n</code></pre>"},{"location":"api/services/scheduling/scheduling/#lume_services.services.scheduling.service.SchedulingService.load_flow","title":"<code>load_flow(flow_name, project_name)</code>","text":"<p>Load a Prefect flow object. Backend implementations without server connecton should raise errors when this method is called.</p> <p>Parameters:</p> Name Type Description Default <code>flow_name</code> <code>str</code> <p>Name of flow.</p> required <code>project_name</code> <code>str</code> <p>Name of project flow is registered with.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary with keys \"flow_id\" and \"flow\"</p> <p>Raises:</p> Type Description <code>ClientError</code> <p>if the GraphQL query is bad for any reason</p> <code>LocalBackendError</code> <p>Using local run configuration, no server backend methods permitted.</p> Source code in <code>lume_services/services/scheduling/service.py</code> <pre><code>def load_flow(self, flow_name: str, project_name: str) -&gt; dict:\n    \"\"\"Load a Prefect flow object. Backend implementations without server connecton\n    should raise errors when this method is called.\n\n    Args:\n        flow_name (str): Name of flow.\n        project_name (str): Name of project flow is registered with.\n\n    Returns:\n        dict: Dictionary with keys \"flow_id\" and \"flow\"\n\n    Raises:\n        prefect.errors.ClientError: if the GraphQL query is bad for any reason\n        lume_services.errors.LocalBackendError: Using local run configuration, no\n            server backend methods permitted.\n\n    \"\"\"\n    return self.backend.load_flow(flow_name, project_name)\n</code></pre>"},{"location":"api/services/scheduling/scheduling/#lume_services.services.scheduling.service.SchedulingService.register_flow","title":"<code>register_flow(flow, project_name, image, labels)</code>","text":"<p>Register a flow with Prefect. Backend implementations without server connecton should raise errors when this method is called.</p> <p>Parameters:</p> Name Type Description Default <code>flow</code> <code>Flow</code> <p>Prefect flow to register.</p> required <code>project_name</code> <code>str</code> <p>Name of project to register flow to.</p> required <code>image</code> <code>str</code> <p>Name of Docker image to run flow inside.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>ID of registered flow.</p> <p>Raises:</p> Type Description <code>ClientError</code> <p>if the GraphQL query is bad for any reason</p> <code>LocalBackendError</code> <p>Using local run configuration, no server backend methods permitted.</p> Source code in <code>lume_services/services/scheduling/service.py</code> <pre><code>def register_flow(\n    self,\n    flow: Flow,\n    project_name: str,\n    image: Optional[str],\n    labels: Optional[List[str]],\n) -&gt; str:\n    \"\"\"Register a flow with Prefect. Backend implementations without server\n    connecton should raise errors when this method is called.\n\n    Args:\n        flow (Flow): Prefect flow to register.\n        project_name (str): Name of project to register flow to.\n        image (str): Name of Docker image to run flow inside.\n\n    Returns:\n        str: ID of registered flow.\n\n    Raises:\n        prefect.errors.ClientError: if the GraphQL query is bad for any reason\n        lume_services.errors.LocalBackendError: Using local run configuration, no\n            server backend methods permitted.\n\n    \"\"\"\n    return self.backend.register_flow(flow, project_name, image, labels=labels)\n</code></pre>"},{"location":"api/services/scheduling/scheduling/#lume_services.services.scheduling.service.SchedulingService.run","title":"<code>run(parameters, run_config=None, **kwargs)</code>","text":"<p>Run a flow. Does not return result. Implementations should cover instantiation of run_config from kwargs as well as backend-specific kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, Any]</code> <p>Dictionary mapping flow parameter name to value</p> required <code>**kwargs</code> <p>Keyword arguments for RunConfig init and backend-specific execution.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>Union[str, None]: Return run_id in case of server backend, None in the case of local execution.</p> <p>Raises:</p> Type Description <code>DockerException</code> <p>Run configuration error for docker api.</p> <code>ValidationError</code> <p>Error validating run configuration.</p> <code>ClientError</code> <p>if the GraphQL query is bad for any reason</p> <code>ValueError</code> <p>Value error on flow run</p> Source code in <code>lume_services/services/scheduling/service.py</code> <pre><code>def run(\n    self, parameters: Dict[str, Any], run_config=None, **kwargs\n) -&gt; Union[str, None]:\n    \"\"\"Run a flow. Does not return result. Implementations should cover\n    instantiation of run_config from kwargs as well as backend-specific kwargs.\n\n    Args:\n        parameters (Dict[str, Any]): Dictionary mapping flow parameter\n            name to value\n        **kwargs: Keyword arguments for RunConfig init and backend-specific\n            execution.\n\n    Returns:\n        Union[str, None]: Return run_id in case of server backend, None in the case\n            of local execution.\n\n    Raises:\n        docker.errors.DockerException: Run configuration error for docker api.\n        pydantic.ValidationError: Error validating run configuration.\n        prefect.errors.ClientError: if the GraphQL query is bad for any reason\n        ValueError: Value error on flow run\n\n    \"\"\"\n    return self.backend.run(parameters, run_config=run_config, **kwargs)\n</code></pre>"},{"location":"api/services/scheduling/scheduling/#lume_services.services.scheduling.service.SchedulingService.run_and_return","title":"<code>run_and_return(parameters, run_config=None, task_name=None, **kwargs)</code>","text":"<p>Run a flow and return result. Implementations should cover instantiation of run_config from kwargs as well as backend-specific kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary mapping flow parameter name to value</p> required <code>run_config</code> <code>Optional[RunConfig]</code> <p>RunConfig object to configure flow fun.</p> <code>None</code> <code>task_name</code> <code>Optional[str]</code> <p>Name of task to return result. If no task slug is passed, will return the flow result.</p> <code>None</code> <code>**kwargs</code> <p>Keyword arguments for RunConfig init and backend-specific execution.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Result of flow run.</p> <p>Raises:</p> Type Description <code>EmptyResultError</code> <p>No result is associated with the flow.</p> <code>DockerException</code> <p>Run configuration error for docker api.</p> <code>ValidationError</code> <p>Error validating run configuration.</p> <code>ClientError</code> <p>if the GraphQL query is bad for any reason</p> <code>TaskNotInFlowError</code> <p>Task slug does not exist in flow.</p> <code>TaskNotCompletedError</code> <p>Result reference task was not completed.</p> <code>RuntimeError</code> <p>Flow did not complete within given timeout.</p> <code>ValueError</code> <p>Value error on flow run</p> Source code in <code>lume_services/services/scheduling/service.py</code> <pre><code>def run_and_return(\n    self,\n    parameters: Optional[Dict[str, Any]],\n    run_config: Optional[RunConfig] = None,\n    task_name: Optional[str] = None,\n    **kwargs\n) -&gt; Any:\n    \"\"\"Run a flow and return result. Implementations should cover instantiation of\n    run_config from kwargs as well as backend-specific kwargs.\n\n    Args:\n        parameters (Optional[Dict[str, Any]]): Dictionary mapping flow parameter\n            name to value\n        run_config (Optional[RunConfig]): RunConfig object to configure flow fun.\n        task_name (Optional[str]): Name of task to return result. If no task slug\n            is passed, will return the flow result.\n        **kwargs: Keyword arguments for RunConfig init and backend-specific\n            execution.\n\n    Returns:\n        Any: Result of flow run.\n\n    Raises:\n        lume_services.errors.EmptyResultError: No result is associated with the\n            flow.\n        docker.errors.DockerException: Run configuration error for docker api.\n        pydantic.ValidationError: Error validating run configuration.\n        prefect.errors.ClientError: if the GraphQL query is bad for any reason\n        lume_services.errors.TaskNotInFlowError: Task slug does not exist in flow.\n        lume_services.errors.TaskNotCompletedError: Result reference task was not\n            completed.\n        RuntimeError: Flow did not complete within given timeout.\n        ValueError: Value error on flow run\n\n    \"\"\"\n    return self.backend.run_and_return(parameters, run_config, task_name, **kwargs)\n</code></pre>"},{"location":"developer/cluster/","title":"Cluster architecture","text":""},{"location":"developer/cluster/#docker","title":"Docker","text":""},{"location":"developer/cluster/#kubernetes","title":"Kubernetes","text":""},{"location":"developer/configuration/","title":"Configuration","text":"<p>LUME-services is entirely configurable by environment variables. The environment variables are collected from a pydantic BaseSettings object, <code>LUMEServicesSettings</code>, in <code>lume_services.config</code>.</p> <p>This object gathers all configurations used by LUME-services and exposes them through a single interface.</p> <p>Command line tools call the configuration here:</p> <p>So on any attempts to run a command line tool, the LUMEServicesSettings are inferred from the environment variables.</p> <p>In <code>LUMEServicesSettings.Config</code> we set the options for constructing the environment variable: <pre><code>class LUMEServicesSettings(BaseSettings):\n    \"\"\"Settings describing configuration for default LUME-services provider objects.\"\"\"\n\n    model_db: Optional[ModelDBConfig]\n    results_db: Optional[MongodbResultsDBConfig]\n    prefect: Optional[PrefectConfig]\n    mounted_filesystem: Optional[MountedFilesystem]\n    backend: str = \"local\"\n    # something wrong with pydantic literal parsing?\n    # Literal[\"kubernetes\", \"local\", \"docker\"] = \"local\"\n\n    class Config:\n        # env_file = '.env'\n        # env_file_encoding = 'utf-8'\n        validate_assignment = True\n        env_prefix = \"LUME_\"\n        env_nested_delimiter = \"__\"\n</code></pre> This means that out environment variables will be prefixed with \"LUME_\" and sub-configurations will be prefixed by \"__\", following the first level configuration options. These variables are not case sentsitive. So, to set the host_port of the model database configuration, we can use the environment variable <code>LUME_MODEL_DB__PORT</code> and to set the host port for the prefect server we use <code>LUME_PREFECT__SERVER__HOST_PORT</code>.</p>"},{"location":"developer/configuration/#injection","title":"Injection","text":"<p>LUME-services uses dependency injection for service management. High level APIs use abstracted service APIs instead of the services themselves. This design facilitates portable code with services configurable at runtime by environment variables. Abstracted base classes provide functional patterns for implementing custom services compatable with LUME-services code.</p> <p>The service APIs and their respective injected base classes are listed below:</p> Service API Injected Service Base Class <p>The base classes provide </p> <p>The injected services are used to initialize the service API during runtime with </p> <p>by constructing a Python dependency_injector <code>DeclarativeContainer</code> in <code>lume_services.config</code>. This container is referenced </p> <p>Using classes other than the defaults requres implementation of a custom <code>configure</code> method...</p> <p>For example:</p> <pre><code>\n</code></pre> <p>The container provides </p> <p>Once configured, the injection container will provide the configured services to LUME-services objects. For example, the signature for _ looks like _:</p> <p>...</p> <p>This means that _ can be run using manual instantiation, or by using injection by precipating the code with:</p> <pre><code>from lume_services.config import configure\n\nconfigure()\n...\n</code></pre> <p>Applications can take advantage of this feature by... WIRING...</p>"},{"location":"developer/configuration/#propogation-of-environment-to-jobs","title":"Propogation of environment to jobs","text":"<p>In order for deployed jobs to appropriately configure access to services (results database, filesystems, etc.), appropriate environment variables must be propogated to the jobs. Prefect Agents are responsible for starting and monitoring the results of flows.</p> <p>Agent in docker-compose is started with env flags- passes these env vars to all jobs kicked off In the <code>docker-compose.yml</code> packaged in <code>lume_services.docker.files</code>:</p> <p><pre><code>  agent:\n    image: prefecthq/prefect:1.2.3-python3.10\n    command: &gt;\n      bash -c \"prefect server create-tenant --name default --slug default &amp;&gt;/dev/null ;\n      prefect agent docker start --label lume-services --agent-address http://localhost:5000/ --show-flow-logs --log-level DEBUG --network service-net  --env LUME_BACKEND=$LUME_BACKEND --env LUME_MOUNTED_FILESYSTEM__IDENTIFIER=$LUME_MOUNTED_FILESYSTEM__IDENTIFIER --env LUME_MOUNTED_FILESYSTEM__MOUNT_PATH=$LUME_MOUNTED_FILESYSTEM__MOUNT_PATH --env LUME_MOUNTED_FILESYSTEM__MOUNT_ALIAS=$LUME_MOUNTED_FILESYSTEM__MOUNT_ALIAS --env LUME_MOUNTED_FILESYSTEM__MOUNT_TYPE=$LUME_MOUNTED_FILESYSTEM__MOUNT_TYPE --env LUME_ENVIRONMENT__LOCAL_PIP_REPOSITORY=$LUME_ENVIRONMENT__LOCAL_PIP_REPOSITORY --env LUME_ENVIRONMENT__LOCAL_CONDA_CHANNEL_DIRECTORY=$LUME_ENVIRONMENT__LOCAL_CONDA_CHANNEL_DIRECTORY --env LUME_MODEL_DB__HOST=$LUME_MODEL_DB__HOST --env LUME_MODEL_DB__PORT=$LUME_MODEL_DB__PORT --env LUME_MODEL_DB__USER=$LUME_MODEL_DB__USER --env LUME_MODEL_DB__PASSWORD=$LUME_MODEL_DB__PASSWORD --env LUME_RESULTS_DB__USERNAME=$LUME_RESULTS_DB__USERNAME --env LUME_RESULTS_DB__PASSWORD=$LUME_RESULTS_DB__PASSWORD --env LUME_RESULTS_DB__PORT=$LUME_RESULTS_DB__PORT:-27017 --env LUME_RESULTS_DB__HOST=$LUME_RESULTS_DB__HOST --env  LUME_PREFECT__SERVER__HOST=$LUME_PREFECT__SERVER__HOST --env  LUME_PREFECT__SERVER__HOST_PORT=$LUME_PREFECT__SERVER__HOST_PORT --env  LUME_PREFECT__HOME_DIR=$LUME_PREFECT__HOME_DIR --env LUME_PREFECT__DEBUG=$LUME_PREFECT__DEBUG --env LUME_PREFECT__BACKEND=$LUME_PREFECT__BACKEND\"\n    environment:\n      PREFECT__LOGGING__LEVEL: DEBUG\n      LUME_MODEL_DB_PORT:\n      LUME_MOUNTED_FILESYSTEM__IDENTIFIER: mounted\n      LUME_MOUNTED_FILESYSTEM__MOUNT_PATH: ${PWD}\n      LUME_MOUNTED_FILESYSTEM__MOUNT_ALIAS: /working_directory\n      LUME_MOUNTED_FILESYSTEM__MOUNT_TYPE: DirectoryOrCreate\n      LOCAL_CHANNEL_ONLY: ${LUME_PREFECT__ISOLATED:-False}\n      LUME_ENVIRONMENT__LOCAL_PIP_REPOSITORY: ${LUME_EVIRONMENT__LOCAL_PIP_REPOSITORY}\n      LUME_ENVIRONMENT__LOCAL_CONDA_CHANNEL_DIRECTORY: ${LUME_EVIRONMENT__LOCAL_CONDA_CHANNEL_DIRECTORY}\n      LUME_MODEL_DB__HOST: http://mysql\n      LUME_MODEL_DB__PORT: ${LUME_MODEL_DB__PORT:-3306}\n      LUME_MODEL_DB__USER: ${LUME_MODEL_DB__USER:-root}\n      LUME_MODEL_DB__PASSWORD: ${LUME_MODEL_DB__PASSWORD:-password}\n      LUME_MODEL_DB__DATABASE: ${LUME_MODEL_DB__DATABASE:-lume_services_models}\n      LUME_RESULTS_DB__DATABASE: ${LUME_MODEL_DB__DATABASE:-results}\n      LUME_RESULTS_DB__USERNAME : ${LUME_RESULTS_DB__USERNAME:-root}\n      LUME_RESULTS_DB__HOST: http://mongodb\n      LUME_RESULTS_DB__PASSWORD: ${LUME_RESULTS_DB__PASSWORD:-password}\n      LUME_RESULTS_DB__PORT: 27017\n      LUME_PREFECT__SERVER__HOST: apollo\n      LUME_PREFECT__SERVER__HOST_PORT: 42001\n      LUME_PREFECT__HOME_DIR: ~/.prefect\n      LUME_PREFECT__DEBUG: ${LUME_PREFECT__DEBUG:-false}\n      LUME_PREFECT__BACKEND: server\n</code></pre> The command references the assigned environment variables on the agent. In order to be compatible with LUME-services tooling, all agents must be launched using this environment variable propogation. </p>"},{"location":"developer/configuration/#pydantic","title":"Pydantic","text":""},{"location":"developer/configuration/#pydantic-docker-secrets","title":"Pydantic docker secrets:","text":"<p>https://pydantic-docs.helpmanual.io/usage/settings/ Secret strings used for passwords</p>"},{"location":"developer/docker_image/","title":"Docker Image","text":"<p>The lume-services repository is packaged with a Dockerfile for composing the image used to execute individual workflows. The file was modeled after that packaged with Prefect with the addition of conda for environment resolution and already-installed <code>lume-services</code>.</p> <p>When a job is submitted to the scheduling service, the agent creates a container using this image and executes a workflow using a module path (all flows are required to use module storage).</p>"},{"location":"developer/docker_image/#notes","title":"Notes:","text":"<p>This project might be helpful for future development: https://github.com/conda-forge/conda-docker-feedstock</p>"},{"location":"developer/documentation/","title":"Documentation","text":"<p>Documentation for this project</p>"},{"location":"developer/documentation/#mkdocs","title":"MkDocs","text":"<p>For documentation, we use mkdocs to automatically generate our GitHub documentation using the mkdocs.yml packaged in the root of the project directory.</p> <p><code>docs/api</code> contains the API documentation compiled from docstrings.</p> <p>This doesn't happen automatically -add page to hierarchy in mkdocs.yml</p> <p>Auto generated diagrams: - schema - class diagrams... -</p>"},{"location":"developer/documentation/#plugins","title":"Plugins","text":"<p>We use the <code>mkdocstrings</code> plugin for building the [API documentation]((../api). This introspects the docstrings on each of our python objects and assembles documentation based on the typed arguments, attributes, returns, etc.</p>"},{"location":"developer/documentation/#badges","title":"Badges","text":"<p>This repo uses https://shields.io/ for creating badges packaged with the documentation. Most of these are automatically inferred from repo metadata with the exception of the coverage badge. The coverage badge is generated in the <code>coverage</code> job in the <code>.github/workflows/test.yml</code> workflow. The coverage job formats the coverage report an creates a comment on the commit being tested. That comment is then used by the last step to dynamically format the badge by storing a json representation of badge data in a Github gist.</p> <p>In order for this to happen, the repository secret <code>PYTEST_COVERAGE_COMMENT</code> points to a personal auth token. The maintainer can create a new gist and update the <code>PYTEST_COVERAGE_COMMENT</code> to a new auth string.</p> <p> The comment is created by the subaction: https://github.com/marketplace/actions/python-coverage-comment  The dynamic badge is formatted by the subaction:  https://github.com/marketplace/actions/dynamic-badges</p>"},{"location":"developer/setup/","title":"Setup developer tools","text":"<p>First, create the development environment with conda. <pre><code>conda env create -f dev-environment.yml\nconda activate lume-services-dev\npip install -e .\n</code></pre> The repository is packaged using pre-commit hooks for code format consistency. In order to install, run: <pre><code>pre-commit install\n</code></pre></p>"},{"location":"developer/setup/#requirements-management","title":"Requirements management","text":"<p>Requirements are tracked in a number of places and must be kept in sync. In the repository root we have:  - <code>requirements.txt</code>: Run requirements for the package.  - <code>dev-requirements.txt</code>: Requirements for development. Includes packages for testing and formatting hooks.  - <code>docs-requirements.txt</code>: Requirements for building package documentation.  - <code>dev-environment.yml</code>: Developer environment, which should be kept in sync with the above.</p> <p>There is an additional <code>environment.yml</code> packaged in <code>lume_services.docker.files</code> which must be consistent with requirements defined in the repository root. This file is used for composing the Docker image inside which workflows will run in a distributed configuration.</p>"},{"location":"developer/setup/#github-actions-build","title":"GitHub actions build","text":"<p>Upon push of a tag to the repository, the GitHub action defined in <code>.github/workflows/build_sdist.yml</code> will publish the source distribution to the GitHub release artifact. Users installing LUME-services using pip should point to the sdist file.</p>"},{"location":"developer/todo/","title":"TODO:","text":""},{"location":"developer/todo/#repo-setup","title":"Repo setup","text":"<ul> <li>TODO:<ul> <li> Persist volumes for databases</li> <li> Docs for injection and container</li> <li> Docs!</li> <li> Set up all loggers</li> <li> Move environments, flows, tasks, and results under model directory</li> <li> Add tests for templated repo</li> <li> Fix docker cred handling in workflows</li> <li> Set up with conda</li> <li> Could test the templated flows directly with the services by using the lume-services pytest fixtures </li> </ul> </li> <li>Done:<ul> <li> Versioneer</li> <li> Action for testing package install &amp; running pytests</li> <li> Tests for injection and container 7/5</li> <li> Environment variables with extensible prefix 7/5</li> <li> License should be passed or something generic. SLAC license shouldn't be in LUME 4/12</li> <li> Add versioneer config. Versioneer install needs to happen on init. 4/12</li> <li> Transition from pytest-mysql to remove pip dependency 4/15</li> <li> Basics of injection and container 4/15</li> <li> Action for building docs</li> <li> Automatically generate diagram from .sql schema? 5/6s</li> <li> Move any services in init files to a service.py</li> <li> Fix prefect context bug during pytest</li> <li> Fix image issue... Build a designated image in deployment using a lume-services base image</li> <li> Remove tests in pip installation</li> </ul> </li> </ul>"},{"location":"developer/todo/#cli","title":"CLI","text":"<ul> <li>TODO:<ul> <li> Integrate template with LUME-services CLI tooling. Once the template is tracked as a git submodule, a command can be implemented for creating the template directly using the lume-services entrypoint.</li> <li> Implement CLI tools for registering model deployments. At present, you need to call the in-code API as demonstrated in </li> <li> Add CLI tool for listing available models and deployments</li> </ul> </li> </ul>"},{"location":"developer/todo/#template","title":"Template","text":"<ul> <li>TODO:<ul> <li> Use git submodule to track template with version</li> <li> Use relative path for cookiecutter generation</li> </ul> </li> <li>Done:<ul> <li> Action for testing package install &amp; running pytests</li> <li> Upload docker image to repo artifacts 5/3</li> <li> Docker build action for templated repos 4/12</li> <li> Instructions for templating tool and configuration</li> <li> Templated README</li> <li> Handling of input/output variables at outset</li> <li> Raise not implemented for to-be-replaced template functions</li> </ul> </li> </ul>"},{"location":"developer/todo/#databases","title":"Databases","text":"<ul> <li>TODO:<ul> <li> Test database failure cases</li> <li> Dashboard image field in mongodb for impact</li> <li> Docs</li> </ul> </li> <li>Done:<ul> <li> Initial tests for mysql model db</li> <li> Abstraction of database schema and auth config from query execution</li> <li> Add flow_to_deployments into db</li> <li> Change model_versions to deployments in schema</li> <li> Make mongodb multiprocessing safe</li> <li> Add uniqueness to results</li> <li> Fix test connection with mysql in-package plugin. \"Connection refused\" 4/13</li> <li> Rename base classes for DBService</li> </ul> </li> </ul>"},{"location":"developer/todo/#backlog","title":"Backlog","text":"<ul> <li>TODO:<ul> <li> Synchronous snapshot service (-&gt; SLAC-services)</li> <li> HPC interface</li> <li> Slurm interface (Can we just mount the binary?)</li> <li> Output service (this probably belongs in LUME-EPICS)</li> </ul> </li> <li>Done:<ul> <li> Finish util testing</li> </ul> </li> </ul>"},{"location":"developer/todo/#scheduler","title":"Scheduler","text":"<ul> <li> <p>TODO:</p> <ul> <li> Kubernetes backend tests</li> <li> Docs</li> <li> Authentication for private repositories</li> </ul> </li> <li> <p>Done:</p> <ul> <li> Refactor scheduler 7/13</li> <li> Create prefect flow image 6/26</li> <li> Remove redundant flow storage during build in pytest 7/6</li> <li> Pin docker-py version 7/15</li> <li> Constrain KubernetesBackend image pull policy to existing options</li> <li> Drop prefect subfolder 6/24</li> <li> Create docker backend 6/27</li> <li> Tesing infrastructure for prefect w/ docker compose 6/27</li> <li> Result tasks</li> <li> Add scheduler to context</li> <li> Drop all but apollo from config 8/3</li> <li> Improve service wait_until_responsive checks and move into docker 8/9</li> <li> Test flow of flows composition</li> <li> Add requirements table</li> <li> Environment solving for containerized jobs</li> <li> Interace to model</li> <li> How do we handle submission of environment variables to the scheduler? For example, how do we communicate the aliasing of services in a docker-compose app?</li> </ul> </li> </ul>"},{"location":"developer/todo/#misc","title":"Misc","text":"<ul> <li>TODO:<ul> <li> Use environment variable fixture in tests instead of modifying env</li> </ul> </li> <li>Done:<ul> <li> Rename file.systems to file.filesystems and all files names service.py</li> <li> Move fixtures from conftest to designated files under tests/fixtures</li> <li> Change LUME-model SurrogateModel to BaseModel for generalizability</li> </ul> </li> </ul>"},{"location":"developer/todo/#files","title":"Files","text":"<ul> <li>TODO:<ul> <li> Do we need file locks for the file handlers?</li> </ul> </li> <li>Done:<ul> <li> Implement local file handler 5/2</li> <li> Implement mounted filesystem handler 5/2</li> </ul> </li> </ul>"},{"location":"developer/todo/#models","title":"Models","text":"<ul> <li>TODO:<ul> <li> Create model interface with injected services</li> <li> Add utility for loading flow objects</li> </ul> </li> </ul>"},{"location":"developer/notebooks/CommonSetup/","title":"LUME-services","text":"In\u00a0[1]: Copied! <pre>from lume_services.services.models.db import ModelDBConfig\nfrom lume_services.services.results.mongodb import MongodbResultsDBConfig\nfrom lume_services.services.scheduling.backends.server import (\n    PrefectAgentConfig,\n    PrefectConfig,\n    PrefectServerConfig,\n)\n\nfrom lume_services.services.files.filesystems import (\n    LocalFilesystem,\n    MountedFilesystem,\n)\nfrom lume_services.config import LUMEServicesSettings\n\n\nmodel_db_config = ModelDBConfig(\n        host=\"127.0.0.1\",\n        port=\"3306\",\n        user=\"root\",\n        password=\"test\",\n        database=\"model_db\",\n)\n\nresults_db_config = MongodbResultsDBConfig(\n    port=\"27017\",\n    host=\"127.0.0.1\",\n    username=\"root\",\n    database=\"test\",\n    password=\"password\",\n)\n\nprefect_config = PrefectConfig(\n    server=PrefectServerConfig(\n        host=\"http://localhost\", host_port=\"4200\", tag=\"core-1.2.4\"\n    ),\n    agent=PrefectAgentConfig(host=\"http://localhost\", host_port=\"5000\"),\n    backend=\"server\",\n)\n\n\nsettings = LUMEServicesSettings(\n    model_db=model_db_config,\n    results_db=results_db_config,\n    prefect=prefect_config,\n    backend=\"docker\",\n    mounted_filesystem=None,\n)\n</pre> from lume_services.services.models.db import ModelDBConfig from lume_services.services.results.mongodb import MongodbResultsDBConfig from lume_services.services.scheduling.backends.server import (     PrefectAgentConfig,     PrefectConfig,     PrefectServerConfig, )  from lume_services.services.files.filesystems import (     LocalFilesystem,     MountedFilesystem, ) from lume_services.config import LUMEServicesSettings   model_db_config = ModelDBConfig(         host=\"127.0.0.1\",         port=\"3306\",         user=\"root\",         password=\"test\",         database=\"model_db\", )  results_db_config = MongodbResultsDBConfig(     port=\"27017\",     host=\"127.0.0.1\",     username=\"root\",     database=\"test\",     password=\"password\", )  prefect_config = PrefectConfig(     server=PrefectServerConfig(         host=\"http://localhost\", host_port=\"4200\", tag=\"core-1.2.4\"     ),     agent=PrefectAgentConfig(host=\"http://localhost\", host_port=\"5000\"),     backend=\"server\", )   settings = LUMEServicesSettings(     model_db=model_db_config,     results_db=results_db_config,     prefect=prefect_config,     backend=\"docker\",     mounted_filesystem=None, ) In\u00a0[2]: Copied! <pre>from lume_services import config\nconfig.configure(settings)\n\n# initialize singletons\nmodel_db_service = config.context.model_db_service()\nresults_db_service = config.context.results_db_service()\n</pre> from lume_services import config config.configure(settings)  # initialize singletons model_db_service = config.context.model_db_service() results_db_service = config.context.results_db_service() In\u00a0[4]: Copied! <pre># model\nauthor = \"Jackie Garrahan\"\nlaboratory = \"slac\"\nfacility = \"lcls\"\nbeampath = \"cu_hxr\"\ndescription = \"test_model\"\n\n# deployment\nversion = \"v0.0\"\nsha256 = \"placeholder\"\nasset_dir = None  # opt\nsource = \"my source\"\nis_live = 1\nimage = \"placeholder\"\n\n# project\nproject = \"my_project\"\nproject_description = \"placeholder\"\n\n# flow\nflow_id_placeholder = \"test\"\nflow_name = \"my_test_flow\"\n\nmodel_id = model_db_service.store_model(\n    author=author,\n    laboratory=laboratory,\n    facility=facility,\n    beampath=beampath,\n    description=description,\n)\n</pre> # model author = \"Jackie Garrahan\" laboratory = \"slac\" facility = \"lcls\" beampath = \"cu_hxr\" description = \"test_model\"  # deployment version = \"v0.0\" sha256 = \"placeholder\" asset_dir = None  # opt source = \"my source\" is_live = 1 image = \"placeholder\"  # project project = \"my_project\" project_description = \"placeholder\"  # flow flow_id_placeholder = \"test\" flow_name = \"my_test_flow\"  model_id = model_db_service.store_model(     author=author,     laboratory=laboratory,     facility=facility,     beampath=beampath,     description=description, ) In\u00a0[5]: Copied! <pre>deployment_id = model_db_service.store_deployment(\n    model_id=model_id,\n    version=version,\n    asset_dir=asset_dir,\n    sha256=sha256,\n    source=source,\n    is_live=is_live,\n    image=image,\n)\n\n\ndeployment = model_db_service.get_deployment(deployment_id=deployment_id)\n</pre> deployment_id = model_db_service.store_deployment(     model_id=model_id,     version=version,     asset_dir=asset_dir,     sha256=sha256,     source=source,     is_live=is_live,     image=image, )   deployment = model_db_service.get_deployment(deployment_id=deployment_id) In\u00a0[9]: Copied! <pre>project_name = model_db_service.store_project(\n    project_name=project, description=project_description\n)\n</pre> project_name = model_db_service.store_project(     project_name=project, description=project_description )  <pre>\n---------------------------------------------------------------------------\nIntegrityError                            Traceback (most recent call last)\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1819, in Connection._execute_context(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\n   1818     if not evt_handled:\n-&gt; 1819         self.dialect.do_execute(\n   1820             cursor, statement, parameters, context\n   1821         )\n   1823 if self._has_events or self.engine._has_events:\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/default.py:732, in DefaultDialect.do_execute(self, cursor, statement, parameters, context)\n    731 def do_execute(self, cursor, statement, parameters, context=None):\n--&gt; 732     cursor.execute(statement, parameters)\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/cursors.py:148, in Cursor.execute(self, query, args)\n    146 query = self.mogrify(query, args)\n--&gt; 148 result = self._query(query)\n    149 self._executed = query\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/cursors.py:310, in Cursor._query(self, q)\n    309 self._clear_result()\n--&gt; 310 conn.query(q)\n    311 self._do_get_result()\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:548, in Connection.query(self, sql, unbuffered)\n    547 self._execute_command(COMMAND.COM_QUERY, sql)\n--&gt; 548 self._affected_rows = self._read_query_result(unbuffered=unbuffered)\n    549 return self._affected_rows\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:775, in Connection._read_query_result(self, unbuffered)\n    774     result = MySQLResult(self)\n--&gt; 775     result.read()\n    776 self._result = result\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:1156, in MySQLResult.read(self)\n   1155 try:\n-&gt; 1156     first_packet = self.connection._read_packet()\n   1158     if first_packet.is_ok_packet():\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:725, in Connection._read_packet(self, packet_type)\n    724         self._result.unbuffered_active = False\n--&gt; 725     packet.raise_for_error()\n    726 return packet\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/protocol.py:221, in MysqlPacket.raise_for_error(self)\n    220     print(\"errno =\", errno)\n--&gt; 221 err.raise_mysql_exception(self._data)\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/err.py:143, in raise_mysql_exception(data)\n    142     errorclass = InternalError if errno &lt; 1000 else OperationalError\n--&gt; 143 raise errorclass(errno, errval)\n\nIntegrityError: (1062, \"Duplicate entry 'my_project' for key 'project.PRIMARY'\")\n\nThe above exception was the direct cause of the following exception:\n\nIntegrityError                            Traceback (most recent call last)\nInput In [9], in &lt;cell line: 1&gt;()\n----&gt; 1 project_name = model_db_service.store_project(\n      2     project_name=project, description=project_description\n      3 )\n\nFile ~/sandbox/lume-services/lume_services/services/models/service.py:124, in ModelDBService.store_project(self, project_name, description)\n    119 insert_stmt = insert(Project).values(\n    120     project_name=project_name, description=description\n    121 )\n    123 # store in db\n--&gt; 124 result = self._model_db.insert(insert_stmt)\n    126 # Return inserted project name\n    127 if len(result):\n\nFile ~/sandbox/lume-services/lume_services/services/models/db/db.py:206, in ModelDB.insert(self, sql)\n    203 logger.info(\"ModelDB inserting: %s\", str(sql))\n    204 with self.session() as session:\n--&gt; 206     res = session.execute(sql)\n    207     session.commit()\n    209 logger.info(\"Sucessfully executed: %s\", str(sql))\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/orm/session.py:1712, in Session.execute(self, statement, params, execution_options, bind_arguments, _parent_execute_state, _add_event, **kw)\n   1710 else:\n   1711     conn = self._connection_for_bind(bind)\n-&gt; 1712 result = conn._execute_20(statement, params or {}, execution_options)\n   1714 if compile_state_cls:\n   1715     result = compile_state_cls.orm_setup_cursor_result(\n   1716         self,\n   1717         statement,\n   (...)\n   1721         result,\n   1722     )\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1631, in Connection._execute_20(self, statement, parameters, execution_options)\n   1627     util.raise_(\n   1628         exc.ObjectNotExecutableError(statement), replace_context=err\n   1629     )\n   1630 else:\n-&gt; 1631     return meth(self, args_10style, kwargs_10style, execution_options)\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/sql/elements.py:332, in ClauseElement._execute_on_connection(self, connection, multiparams, params, execution_options, _force)\n    328 def _execute_on_connection(\n    329     self, connection, multiparams, params, execution_options, _force=False\n    330 ):\n    331     if _force or self.supports_execution:\n--&gt; 332         return connection._execute_clauseelement(\n    333             self, multiparams, params, execution_options\n    334         )\n    335     else:\n    336         raise exc.ObjectNotExecutableError(self)\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1498, in Connection._execute_clauseelement(self, elem, multiparams, params, execution_options)\n   1486 compiled_cache = execution_options.get(\n   1487     \"compiled_cache\", self.engine._compiled_cache\n   1488 )\n   1490 compiled_sql, extracted_params, cache_hit = elem._compile_w_cache(\n   1491     dialect=dialect,\n   1492     compiled_cache=compiled_cache,\n   (...)\n   1496     linting=self.dialect.compiler_linting | compiler.WARN_LINTING,\n   1497 )\n-&gt; 1498 ret = self._execute_context(\n   1499     dialect,\n   1500     dialect.execution_ctx_cls._init_compiled,\n   1501     compiled_sql,\n   1502     distilled_params,\n   1503     execution_options,\n   1504     compiled_sql,\n   1505     distilled_params,\n   1506     elem,\n   1507     extracted_params,\n   1508     cache_hit=cache_hit,\n   1509 )\n   1510 if has_events:\n   1511     self.dispatch.after_execute(\n   1512         self,\n   1513         elem,\n   (...)\n   1517         ret,\n   1518     )\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1862, in Connection._execute_context(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\n   1859             branched.close()\n   1861 except BaseException as e:\n-&gt; 1862     self._handle_dbapi_exception(\n   1863         e, statement, parameters, cursor, context\n   1864     )\n   1866 return result\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/base.py:2043, in Connection._handle_dbapi_exception(self, e, statement, parameters, cursor, context)\n   2041     util.raise_(newraise, with_traceback=exc_info[2], from_=e)\n   2042 elif should_wrap:\n-&gt; 2043     util.raise_(\n   2044         sqlalchemy_exception, with_traceback=exc_info[2], from_=e\n   2045     )\n   2046 else:\n   2047     util.raise_(exc_info[1], with_traceback=exc_info[2])\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/util/compat.py:208, in raise_(***failed resolving arguments***)\n    205     exception.__cause__ = replace_context\n    207 try:\n--&gt; 208     raise exception\n    209 finally:\n    210     # credit to\n    211     # https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\n    212     # as the __traceback__ object creates a cycle\n    213     del exception, replace_context, from_, with_traceback\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1819, in Connection._execute_context(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\n   1817                 break\n   1818     if not evt_handled:\n-&gt; 1819         self.dialect.do_execute(\n   1820             cursor, statement, parameters, context\n   1821         )\n   1823 if self._has_events or self.engine._has_events:\n   1824     self.dispatch.after_cursor_execute(\n   1825         self,\n   1826         cursor,\n   (...)\n   1830         context.executemany,\n   1831     )\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/default.py:732, in DefaultDialect.do_execute(self, cursor, statement, parameters, context)\n    731 def do_execute(self, cursor, statement, parameters, context=None):\n--&gt; 732     cursor.execute(statement, parameters)\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/cursors.py:148, in Cursor.execute(self, query, args)\n    144     pass\n    146 query = self.mogrify(query, args)\n--&gt; 148 result = self._query(query)\n    149 self._executed = query\n    150 return result\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/cursors.py:310, in Cursor._query(self, q)\n    308 self._last_executed = q\n    309 self._clear_result()\n--&gt; 310 conn.query(q)\n    311 self._do_get_result()\n    312 return self.rowcount\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:548, in Connection.query(self, sql, unbuffered)\n    546     sql = sql.encode(self.encoding, \"surrogateescape\")\n    547 self._execute_command(COMMAND.COM_QUERY, sql)\n--&gt; 548 self._affected_rows = self._read_query_result(unbuffered=unbuffered)\n    549 return self._affected_rows\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:775, in Connection._read_query_result(self, unbuffered)\n    773 else:\n    774     result = MySQLResult(self)\n--&gt; 775     result.read()\n    776 self._result = result\n    777 if result.server_status is not None:\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:1156, in MySQLResult.read(self)\n   1154 def read(self):\n   1155     try:\n-&gt; 1156         first_packet = self.connection._read_packet()\n   1158         if first_packet.is_ok_packet():\n   1159             self._read_ok_packet(first_packet)\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:725, in Connection._read_packet(self, packet_type)\n    723     if self._result is not None and self._result.unbuffered_active is True:\n    724         self._result.unbuffered_active = False\n--&gt; 725     packet.raise_for_error()\n    726 return packet\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/protocol.py:221, in MysqlPacket.raise_for_error(self)\n    219 if DEBUG:\n    220     print(\"errno =\", errno)\n--&gt; 221 err.raise_mysql_exception(self._data)\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/err.py:143, in raise_mysql_exception(data)\n    141 if errorclass is None:\n    142     errorclass = InternalError if errno &lt; 1000 else OperationalError\n--&gt; 143 raise errorclass(errno, errval)\n\nIntegrityError: (pymysql.err.IntegrityError) (1062, \"Duplicate entry 'my_project' for key 'project.PRIMARY'\")\n[SQL: INSERT INTO project (project_name, description) VALUES (%(project_name)s, %(description)s)]\n[parameters: {'project_name': 'my_project', 'description': 'placeholder'}]\n(Background on this error at: https://sqlalche.me/e/14/gkpj)</pre> In\u00a0[11]: Copied! <pre>flow_id = model_db_service.store_flow(\n    project_name=project_name,\n    deployment_id=deployment_id,\n    flow_id=flow_id_placeholder,\n    flow_name=flow_name,\n)\n</pre> flow_id = model_db_service.store_flow(     project_name=project_name,     deployment_id=deployment_id,     flow_id=flow_id_placeholder,     flow_name=flow_name, ) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"developer/notebooks/CommonSetup/#lume-services","title":"LUME-services\u00b6","text":""},{"location":"developer/notebooks/CommonSetup/#configuration","title":"Configuration:\u00b6","text":"<p>Set the following environment variables:</p> <pre><code>export LUME_MODEL_DB__HOST=127.0.0.1\nexport LUME_MODEL_DB__USER=root\nexport LUME_MODEL_DB__PASSWORD=test\nexport LUME_MODEL_DB__PORT=3306\nexport LUME_MODEL_DB__DATABASE=model_db\nexport LUME_MODEL_DB__CONNECTION__POOL_SIZE=1\nexport LUME_PREFECT__SERVER__TAG=core-1.4.0\nexport LUME_PREFECT__SERVER__HOST_PORT=4200\nexport LUME_PREFECT__SERVER__HOST=http://localhost\nexport LUME_PREFECT__AGENT__HOST_PORT=5000\nexport LUME_PREFECT__AGENT__HOST=http://localhost\nexport LUME_RESULTS_DB__HOST=127.0.0.1\nexport LUME_RESULTS_DB__PORT=27017\nexport LUME_RESULTS_DB__USERNAME=root\nexport LUME_RESULTS_DB__PASSWORD=password\nexport LUME_RESULTS_DB__DATABASE=test\nexport LUME_PREFECT__BACKEND=server \nexport LUME_BACKEND=docker\n</code></pre> <p>Start the docker services using:</p> <pre><code>lume-services docker start-services\n</code></pre>"},{"location":"developer/notebooks/CommonSetup/#use-lume-services-config-to-create-injected-services","title":"Use LUME-services config to create injected services\u00b6","text":""},{"location":"developer/notebooks/CommonSetup/#store-example-model-deployment-flow-and-project","title":"Store example model, deployment, flow, and project\u00b6","text":""},{"location":"developer/notebooks/Model/","title":"LUME-services","text":"In\u00a0[1]: Copied! <pre>from lume_services.services.models.db import ModelDBConfig\nfrom lume_services.services.results.mongodb import MongodbResultsDBConfig\nfrom lume_services.services.scheduling.backends.server import (\n    PrefectAgentConfig,\n    PrefectConfig,\n    PrefectServerConfig,\n)\n\nfrom lume_services.services.files.filesystems import (\n    LocalFilesystem,\n    MountedFilesystem,\n)\nfrom lume_services.config import LUMEServicesSettings\nimport logging\nlogging.basicConfig(level=\"DEBUG\")\n\nmodel_db_config = ModelDBConfig(\n        host=\"127.0.0.1\",\n        port=\"3306\",\n        user=\"root\",\n        password=\"test\",\n        database=\"model_db\",\n)\n\nresults_db_config = MongodbResultsDBConfig(\n    port=\"27017\",\n    host=\"127.0.0.1\",\n    username=\"root\",\n    database=\"test\",\n    password=\"password\",\n)\n\nprefect_config = PrefectConfig(\n    server=PrefectServerConfig(\n        host=\"http://localhost\", host_port=\"4200\", tag=\"core-1.2.4\"\n    ),\n    agent=PrefectAgentConfig(host=\"http://localhost\", host_port=\"5000\"),\n    backend=\"server\",\n)\n\n\nsettings = LUMEServicesSettings(\n    model_db=model_db_config,\n    results_db=results_db_config,\n    prefect=prefect_config,\n    backend=\"docker\",\n    mounted_filesystem=None,\n)\n</pre> from lume_services.services.models.db import ModelDBConfig from lume_services.services.results.mongodb import MongodbResultsDBConfig from lume_services.services.scheduling.backends.server import (     PrefectAgentConfig,     PrefectConfig,     PrefectServerConfig, )  from lume_services.services.files.filesystems import (     LocalFilesystem,     MountedFilesystem, ) from lume_services.config import LUMEServicesSettings import logging logging.basicConfig(level=\"DEBUG\")  model_db_config = ModelDBConfig(         host=\"127.0.0.1\",         port=\"3306\",         user=\"root\",         password=\"test\",         database=\"model_db\", )  results_db_config = MongodbResultsDBConfig(     port=\"27017\",     host=\"127.0.0.1\",     username=\"root\",     database=\"test\",     password=\"password\", )  prefect_config = PrefectConfig(     server=PrefectServerConfig(         host=\"http://localhost\", host_port=\"4200\", tag=\"core-1.2.4\"     ),     agent=PrefectAgentConfig(host=\"http://localhost\", host_port=\"5000\"),     backend=\"server\", )   settings = LUMEServicesSettings(     model_db=model_db_config,     results_db=results_db_config,     prefect=prefect_config,     backend=\"docker\",     mounted_filesystem=None, ) In\u00a0[2]: Copied! <pre>from lume_services import config\nconfig.configure(settings)\n\n# initialize singletons\nmodel_db_service = config.context.model_db_service()\nresults_db_service = config.context.results_db_service()\nscheduling_service = config.context.scheduling_service()\n</pre> from lume_services import config config.configure(settings)  # initialize singletons model_db_service = config.context.model_db_service() results_db_service = config.context.results_db_service() scheduling_service = config.context.scheduling_service() <pre>INFO:lume_services.config:Configuring LUME-services environment...\nDEBUG:h5py._conv:Creating converter from 7 to 5\nDEBUG:h5py._conv:Creating converter from 5 to 7\nDEBUG:h5py._conv:Creating converter from 7 to 5\nDEBUG:h5py._conv:Creating converter from 5 to 7\nINFO:lume_services.config:Environment configured.\nDEBUG:lume_services.config:Environment configured using {'model_db': {'host': '127.0.0.1', 'port': 3306, 'user': 'root', 'database': 'model_db', 'connection': {'pool_size': None, 'pool_pre_ping': True}, 'dialect_str': 'mysql+pymysql'}, 'results_db': {'username': 'root', 'host': '127.0.0.1', 'port': 27017, 'authMechanism': 'DEFAULT'}, 'prefect': {'server': {'tag': 'core-1.2.4', 'host': 'http://localhost', 'host_port': '4200', 'host_ip': '127.0.0.1'}, 'ui': {'host': 'http://localhost', 'host_port': '8080', 'host_ip': '127.0.0.1', 'apollo_url': 'http://localhost:4200/graphql'}, 'telemetry': {'enabled': True}, 'agent': {'host': 'http://localhost', 'host_port': '5000'}, 'home_dir': '~/.prefect', 'debug': False, 'backend': 'server', 'default_image': 'jgarrahan/lume-services-prefect:latest', 'isolated': False}, 'mounted_filesystem': None, 'environment': {'local_pip_repository': '/Users/jacquelinegarrahan/sandbox/lume-services/dev/local_pip_repo', 'local_conda_channel_directory': '/Users/jacquelinegarrahan/sandbox/lume-services/dev/local_conda_channel', 'base_env_filepath': '/Users/jacquelinegarrahan/sandbox/lume-services/lume_services/docker/files/environment.yml', 'tmp_directory': '/tmp/lume-services', 'platform': 'linux-64', 'url_retry_count': 3, 'python_version': '3.9.13'}, 'backend': 'docker'}\n</pre> In\u00a0[3]: Copied! <pre>#model_db_service._reset()\n</pre> #model_db_service._reset() In\u00a0[4]: Copied! <pre>from lume_services.models import Model\n\nmodel = Model.create_model(\n    author = \"Jackie Garrahan\",\n    laboratory = \"slac\",\n    facility = \"lcls\",\n    beampath = \"cu_hxr\",\n    description = \"test_model\"\n)\n</pre> from lume_services.models import Model  model = Model.create_model(     author = \"Jackie Garrahan\",     laboratory = \"slac\",     facility = \"lcls\",     beampath = \"cu_hxr\",     description = \"test_model\" ) <pre>INFO:lume_services.services.models.db.db:ModelDB inserting: INSERT INTO model (author, laboratory, facility, beampath, description) VALUES (:author, :laboratory, :facility, :beampath, :description)\nDEBUG:lume_services.services.models.db.db:ModelDB creating session.\nDEBUG:lume_services.services.models.db.db:ModelDB session created.\nINFO:lume_services.services.models.db.db:Sucessfully executed: INSERT INTO model (author, laboratory, facility, beampath, description) VALUES (:author, :laboratory, :facility, :beampath, :description)\nINFO:lume_services.services.models.db.db:ModelDB selecting: SELECT model.model_id, model.created, model.author, model.laboratory, model.facility, model.beampath, model.description \nFROM model \nWHERE model.model_id = :model_id_1\nDEBUG:lume_services.services.models.db.db:ModelDB creating session.\nDEBUG:lume_services.services.models.db.db:ModelDB session created.\n</pre> In\u00a0[5]: Copied! <pre># create a project\nproject_name = model_db_service.store_project(\n    project_name=\"test\", description=\"my_description\"\n)\n</pre> # create a project project_name = model_db_service.store_project(     project_name=\"test\", description=\"my_description\" ) <pre>INFO:lume_services.services.models.db.db:ModelDB inserting: INSERT INTO project (project_name, description) VALUES (:project_name, :description)\nDEBUG:lume_services.services.models.db.db:ModelDB creating session.\nDEBUG:lume_services.services.models.db.db:ModelDB session created.\n</pre> <pre>\n---------------------------------------------------------------------------\nIntegrityError                            Traceback (most recent call last)\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1900, in Connection._execute_context(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\n   1899     if not evt_handled:\n-&gt; 1900         self.dialect.do_execute(\n   1901             cursor, statement, parameters, context\n   1902         )\n   1904 if self._has_events or self.engine._has_events:\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/default.py:736, in DefaultDialect.do_execute(self, cursor, statement, parameters, context)\n    735 def do_execute(self, cursor, statement, parameters, context=None):\n--&gt; 736     cursor.execute(statement, parameters)\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/cursors.py:148, in Cursor.execute(self, query, args)\n    146 query = self.mogrify(query, args)\n--&gt; 148 result = self._query(query)\n    149 self._executed = query\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/cursors.py:310, in Cursor._query(self, q)\n    309 self._clear_result()\n--&gt; 310 conn.query(q)\n    311 self._do_get_result()\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:548, in Connection.query(self, sql, unbuffered)\n    547 self._execute_command(COMMAND.COM_QUERY, sql)\n--&gt; 548 self._affected_rows = self._read_query_result(unbuffered=unbuffered)\n    549 return self._affected_rows\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:775, in Connection._read_query_result(self, unbuffered)\n    774     result = MySQLResult(self)\n--&gt; 775     result.read()\n    776 self._result = result\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:1156, in MySQLResult.read(self)\n   1155 try:\n-&gt; 1156     first_packet = self.connection._read_packet()\n   1158     if first_packet.is_ok_packet():\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:725, in Connection._read_packet(self, packet_type)\n    724         self._result.unbuffered_active = False\n--&gt; 725     packet.raise_for_error()\n    726 return packet\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/protocol.py:221, in MysqlPacket.raise_for_error(self)\n    220     print(\"errno =\", errno)\n--&gt; 221 err.raise_mysql_exception(self._data)\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/err.py:143, in raise_mysql_exception(data)\n    142     errorclass = InternalError if errno &lt; 1000 else OperationalError\n--&gt; 143 raise errorclass(errno, errval)\n\nIntegrityError: (1062, \"Duplicate entry 'test' for key 'project.PRIMARY'\")\n\nThe above exception was the direct cause of the following exception:\n\nIntegrityError                            Traceback (most recent call last)\nCell In [5], line 2\n      1 # create a project\n----&gt; 2 project_name = model_db_service.store_project(\n      3     project_name=\"test\", description=\"my_description\"\n      4 )\n\nFile ~/sandbox/lume-services/lume_services/services/models/service.py:130, in ModelDBService.store_project(self, project_name, description)\n    125 insert_stmt = insert(Project).values(\n    126     project_name=project_name, description=description\n    127 )\n    129 # store in db\n--&gt; 130 result = self._model_db.insert(insert_stmt)\n    132 # Return inserted project name\n    133 if len(result):\n\nFile ~/sandbox/lume-services/lume_services/services/models/db/db.py:206, in ModelDB.insert(self, sql)\n    203 logger.info(\"ModelDB inserting: %s\", str(sql))\n    204 with self.session() as session:\n--&gt; 206     res = session.execute(sql)\n    207     session.commit()\n    209 logger.info(\"Sucessfully executed: %s\", str(sql))\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/orm/session.py:1712, in Session.execute(self, statement, params, execution_options, bind_arguments, _parent_execute_state, _add_event, **kw)\n   1710 else:\n   1711     conn = self._connection_for_bind(bind)\n-&gt; 1712 result = conn._execute_20(statement, params or {}, execution_options)\n   1714 if compile_state_cls:\n   1715     result = compile_state_cls.orm_setup_cursor_result(\n   1716         self,\n   1717         statement,\n   (...)\n   1721         result,\n   1722     )\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1705, in Connection._execute_20(self, statement, parameters, execution_options)\n   1701     util.raise_(\n   1702         exc.ObjectNotExecutableError(statement), replace_context=err\n   1703     )\n   1704 else:\n-&gt; 1705     return meth(self, args_10style, kwargs_10style, execution_options)\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/sql/elements.py:333, in ClauseElement._execute_on_connection(self, connection, multiparams, params, execution_options, _force)\n    329 def _execute_on_connection(\n    330     self, connection, multiparams, params, execution_options, _force=False\n    331 ):\n    332     if _force or self.supports_execution:\n--&gt; 333         return connection._execute_clauseelement(\n    334             self, multiparams, params, execution_options\n    335         )\n    336     else:\n    337         raise exc.ObjectNotExecutableError(self)\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1572, in Connection._execute_clauseelement(self, elem, multiparams, params, execution_options)\n   1560 compiled_cache = execution_options.get(\n   1561     \"compiled_cache\", self.engine._compiled_cache\n   1562 )\n   1564 compiled_sql, extracted_params, cache_hit = elem._compile_w_cache(\n   1565     dialect=dialect,\n   1566     compiled_cache=compiled_cache,\n   (...)\n   1570     linting=self.dialect.compiler_linting | compiler.WARN_LINTING,\n   1571 )\n-&gt; 1572 ret = self._execute_context(\n   1573     dialect,\n   1574     dialect.execution_ctx_cls._init_compiled,\n   1575     compiled_sql,\n   1576     distilled_params,\n   1577     execution_options,\n   1578     compiled_sql,\n   1579     distilled_params,\n   1580     elem,\n   1581     extracted_params,\n   1582     cache_hit=cache_hit,\n   1583 )\n   1584 if has_events:\n   1585     self.dispatch.after_execute(\n   1586         self,\n   1587         elem,\n   (...)\n   1591         ret,\n   1592     )\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1943, in Connection._execute_context(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\n   1940             branched.close()\n   1942 except BaseException as e:\n-&gt; 1943     self._handle_dbapi_exception(\n   1944         e, statement, parameters, cursor, context\n   1945     )\n   1947 return result\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/base.py:2124, in Connection._handle_dbapi_exception(self, e, statement, parameters, cursor, context)\n   2122     util.raise_(newraise, with_traceback=exc_info[2], from_=e)\n   2123 elif should_wrap:\n-&gt; 2124     util.raise_(\n   2125         sqlalchemy_exception, with_traceback=exc_info[2], from_=e\n   2126     )\n   2127 else:\n   2128     util.raise_(exc_info[1], with_traceback=exc_info[2])\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/util/compat.py:208, in raise_(***failed resolving arguments***)\n    205     exception.__cause__ = replace_context\n    207 try:\n--&gt; 208     raise exception\n    209 finally:\n    210     # credit to\n    211     # https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\n    212     # as the __traceback__ object creates a cycle\n    213     del exception, replace_context, from_, with_traceback\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1900, in Connection._execute_context(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\n   1898                 break\n   1899     if not evt_handled:\n-&gt; 1900         self.dialect.do_execute(\n   1901             cursor, statement, parameters, context\n   1902         )\n   1904 if self._has_events or self.engine._has_events:\n   1905     self.dispatch.after_cursor_execute(\n   1906         self,\n   1907         cursor,\n   (...)\n   1911         context.executemany,\n   1912     )\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/sqlalchemy/engine/default.py:736, in DefaultDialect.do_execute(self, cursor, statement, parameters, context)\n    735 def do_execute(self, cursor, statement, parameters, context=None):\n--&gt; 736     cursor.execute(statement, parameters)\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/cursors.py:148, in Cursor.execute(self, query, args)\n    144     pass\n    146 query = self.mogrify(query, args)\n--&gt; 148 result = self._query(query)\n    149 self._executed = query\n    150 return result\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/cursors.py:310, in Cursor._query(self, q)\n    308 self._last_executed = q\n    309 self._clear_result()\n--&gt; 310 conn.query(q)\n    311 self._do_get_result()\n    312 return self.rowcount\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:548, in Connection.query(self, sql, unbuffered)\n    546     sql = sql.encode(self.encoding, \"surrogateescape\")\n    547 self._execute_command(COMMAND.COM_QUERY, sql)\n--&gt; 548 self._affected_rows = self._read_query_result(unbuffered=unbuffered)\n    549 return self._affected_rows\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:775, in Connection._read_query_result(self, unbuffered)\n    773 else:\n    774     result = MySQLResult(self)\n--&gt; 775     result.read()\n    776 self._result = result\n    777 if result.server_status is not None:\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:1156, in MySQLResult.read(self)\n   1154 def read(self):\n   1155     try:\n-&gt; 1156         first_packet = self.connection._read_packet()\n   1158         if first_packet.is_ok_packet():\n   1159             self._read_ok_packet(first_packet)\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/connections.py:725, in Connection._read_packet(self, packet_type)\n    723     if self._result is not None and self._result.unbuffered_active is True:\n    724         self._result.unbuffered_active = False\n--&gt; 725     packet.raise_for_error()\n    726 return packet\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/protocol.py:221, in MysqlPacket.raise_for_error(self)\n    219 if DEBUG:\n    220     print(\"errno =\", errno)\n--&gt; 221 err.raise_mysql_exception(self._data)\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/pymysql/err.py:143, in raise_mysql_exception(data)\n    141 if errorclass is None:\n    142     errorclass = InternalError if errno &lt; 1000 else OperationalError\n--&gt; 143 raise errorclass(errno, errval)\n\nIntegrityError: (pymysql.err.IntegrityError) (1062, \"Duplicate entry 'test' for key 'project.PRIMARY'\")\n[SQL: INSERT INTO project (project_name, description) VALUES (%(project_name)s, %(description)s)]\n[parameters: {'project_name': 'test', 'description': 'my_description'}]\n(Background on this error at: https://sqlalche.me/e/14/gkpj)</pre> In\u00a0[\u00a0]: Copied! <pre>scheduling_service.create_project(\"test\")\n</pre> scheduling_service.create_project(\"test\") In\u00a0[\u00a0]: Copied! <pre>source_path = \"https://github.com/jacquelinegarrahan/my-model/releases/download/v0.0.9/my_model-0.0.9.tar.gz\"\n# populates local channel\nmodel.store_deployment(source_path, project_name=\"test\")\n</pre> source_path = \"https://github.com/jacquelinegarrahan/my-model/releases/download/v0.0.9/my_model-0.0.9.tar.gz\" # populates local channel model.store_deployment(source_path, project_name=\"test\") In\u00a0[\u00a0]: Copied! <pre>model.run_and_return(parameters={\n                        \"input1\": \"hey\", \n                        \"input2\": \"jackie\", \n                        \"filename\": f\"/Users/jgarra/sandbox/lume-services/test_file.txt\", \n                        \"filesystem_identifier\":\"local\"}\n         )\n</pre> model.run_and_return(parameters={                         \"input1\": \"hey\",                          \"input2\": \"jackie\",                          \"filename\": f\"/Users/jgarra/sandbox/lume-services/test_file.txt\",                          \"filesystem_identifier\":\"local\"}          ) In\u00a0[\u00a0]: Copied! <pre>model.results()\n</pre> model.results() In\u00a0[6]: Copied! <pre>from lume_services.models import Model\n\nmodel = Model(model_id=1)\n</pre> from lume_services.models import Model  model = Model(model_id=1) <pre>INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT model.model_id, model.created, model.author, model.laboratory, model.facility, model.beampath, model.description \nFROM model \nWHERE model.model_id = :model_id_1\nDEBUG:lume_services.services.models.db.db:ModelDB creating session.\nDEBUG:lume_services.services.models.db.db:ModelDB session created.\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[7]: Copied! <pre>model.load_deployment()\n</pre> model.load_deployment() <pre>INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT deployment.sha256, deployment.deployment_id, deployment.version, deployment.deploy_date, deployment.package_import_name, deployment.asset_dir, deployment.source, deployment.image, deployment.is_live, deployment.model_id \nFROM deployment \nWHERE deployment.model_id = :model_id_1 ORDER BY deployment.deploy_date DESC\nDEBUG:lume_services.services.models.db.db:ModelDB creating session.\nDEBUG:lume_services.services.models.db.db:ModelDB session created.\nINFO:lume_services.services.models.db.db:ModelDB selecting: SELECT deployment_dependencies._id, deployment_dependencies.name, deployment_dependencies.source, deployment_dependencies.local_source, deployment_dependencies.version, deployment_dependencies.deployment_id, deployment_dependencies.dependency_type_id, dependency_type_1.id, dependency_type_1.type \nFROM deployment_dependencies LEFT OUTER JOIN dependency_type AS dependency_type_1 ON dependency_type_1.id = deployment_dependencies.dependency_type_id \nWHERE deployment_dependencies.deployment_id = :deployment_id_1\nDEBUG:lume_services.services.models.db.db:ModelDB creating session.\nDEBUG:lume_services.services.models.db.db:ModelDB session created.\nINFO:lume_services.services.models.db.db:ModelDB selecting: SELECT flow.flow_id, flow.flow_name, flow.project_name, flow.deployment_id \nFROM flow \nWHERE flow.deployment_id = :deployment_id_1\nDEBUG:lume_services.services.models.db.db:ModelDB creating session.\nDEBUG:lume_services.services.models.db.db:ModelDB session created.\nINFO:lume_services.services.models.db.db:ModelDB selecting: SELECT project.project_name, project.description \nFROM project \nWHERE project.project_name = :project_name_1\nDEBUG:lume_services.services.models.db.db:ModelDB creating session.\nDEBUG:lume_services.services.models.db.db:ModelDB session created.\nINFO:lume_services.services.models.db.db:ModelDB selecting: SELECT flow_of_flows._id, flow_of_flows.parent_flow_id, flow_of_flows.flow_id, flow_of_flows.position \nFROM flow_of_flows \nWHERE flow_of_flows.parent_flow_id = :parent_flow_id_1\nDEBUG:lume_services.services.models.db.db:ModelDB creating session.\nDEBUG:lume_services.services.models.db.db:ModelDB session created.\n</pre> In\u00a0[9]: Copied! <pre>model.run_and_return(parameters={\n                        \"input1\": \"hey\", \n                        \"input2\": \"jackie\", \n                        \"filename\": f\"/Users/jgarra/sandbox/lume-services/test_file.txt\", \n                        \"filesystem_identifier\":\"local\"\n        }\n)\n</pre> model.run_and_return(parameters={                         \"input1\": \"hey\",                          \"input2\": \"jackie\",                          \"filename\": f\"/Users/jgarra/sandbox/lume-services/test_file.txt\",                          \"filesystem_identifier\":\"local\"         } ) <pre>INFO:lume_services.services.scheduling.backends.server:Creating Prefect flow run for 7def8c7a-09a0-4449-ade1-ebfecf7c10af with parameters {'input1': 'hey', 'input2': 'jackie', 'filename': '/Users/jgarra/sandbox/lume-services/test_file.txt', 'filesystem_identifier': 'local'} and run_config {\"labels\": [\"lume-services\"], \"env\": {\"EXTRA_CONDA_PACKAGES\": \"_libgcc_mutex libstdcxx-ng ld_impl_linux-64 ca-certificates libgfortran5 libgomp libgfortran-ng _openmp_mutex libgcc-ng lzo xorg-libxdmcp xorg-libxau pthread-stubs libuuid libdeflate lerc jpeg libbrotlicommon libev c-ares libwebp-base ripgrep liblief patchelf patch lz4-c icu libopenblas reproc libiconv keyutils yaml-cpp xz openssl ncurses libzlib libffi bzip2 yaml libnsl libxcb libbrotlienc libbrotlidec libblas reproc-cpp libedit readline zstd libpng libxml2 libssh2 libnghttp2 tk libsqlite libsolv brotli-bin libcblas liblapack krb5 libtiff freetype libarchive sqlite brotli libcurl openjpeg lcms2 hdf5 libmamba tzdata pybind11-abi python wheel setuptools pip munkres cycler soupsieve cached_property unidecode text-unidecode charset-normalizer colorama certifi heapdict glob2 pkginfo filelock appdirs websocket-client six pycparser idna asn1crypto pytzdata typing_extensions natsort fsspec tblib locket toolz sortedcontainers zipp pyparsing toml tabulate pytz cloudpickle pymysql beautifulsoup4 cached-property python-slugify tqdm zict docker-pycreds python-dateutil typing-extensions partd importlib_resources packaging croniter marshmallow marshmallow-oneofschema python_abi unicodedata2 pillow kiwisolver ruamel.yaml.clib markupsafe pysocks ruamel_yaml pycosat python-libarchive-c py-lief conda-package-handling chardet cffi greenlet tornado psutil msgpack-python cytoolz pendulum mypy_extensions pymongo pyyaml numpy libmambapy dependency_injector pydantic click fonttools ruamel.yaml brotlipy cryptography sqlalchemy scipy pandas h5py matplotlib-base python-box jinja2 dask-core lume-model pyopenssl openpmd-beamphysics urllib3 lume-base requests distributed conda docker-py conda-build mamba prefect\", \"EXTRA_PIP_PACKAGES\": \"\", \"LOCAL_CHANNEL_ONLY\": false}, \"image\": \"jgarrahan/lume-services-prefect:latest\", \"host_config\": null, \"ports\": null}\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 75\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 9504\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 2911\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nINFO:lume_services.services.scheduling.backends.server:FlowRunLog(timestamp=DateTime(2022, 9, 8, 2, 19, 46, 675434, tzinfo=Timezone('+00:00')), level=20, message='Entered state &lt;Scheduled&gt;: Flow run scheduled.')\nINFO:lume_services.services.scheduling.backends.server:FlowRunLog(timestamp=DateTime(2022, 9, 8, 2, 19, 47, 905525, tzinfo=Timezone('+00:00')), level=20, message='Entered state &lt;Submitted&gt;: Submitted for execution')\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 215\nINFO:lume_services.services.scheduling.backends.server:FlowRunLog(timestamp=DateTime(2022, 9, 8, 2, 20, 4, 939289, tzinfo=Timezone('+00:00')), level=20, message='Submitted for execution: Container ID: ac2061f052bcc5bf7b4bfc86473bcb31fab806863bc7cba73fd8e7887a324149')\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.util.retry:Incremented Retry for (url='/'): Retry(total=5, connect=None, read=None, redirect=None, status=None)\nWARNING:urllib3.connectionpool:Retrying (Retry(total=5, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='localhost', port=4200): Read timed out. (read timeout=15)\")': /\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (2): localhost:4200\n</pre> <pre>DEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 36\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 3624\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4200\nDEBUG:urllib3.connectionpool:http://localhost:4200 \"POST / HTTP/1.1\" 200 51\n</pre> <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In [9], line 1\n----&gt; 1 model.run_and_return(parameters={\n      2                         \"input1\": \"hey\", \n      3                         \"input2\": \"jackie\", \n      4                         \"filename\": f\"/Users/jgarra/sandbox/lume-services/test_file.txt\", \n      5                         \"filesystem_identifier\":\"local\"\n      6         }\n      7 )\n\nFile src/dependency_injector/_cwiring.pyx:28, in dependency_injector._cwiring._get_sync_patched._patched()\n\nFile ~/sandbox/lume-services/lume_services/models/model.py:440, in Model.run_and_return(self, parameters, task_name, scheduling_service)\n    428 # all of this should be inside the scheduling service and not here...\n    429 \n    430 # move all of scheduling service interface into  Flow and then dependencies\n    431 run_config = scheduling_service.backend.run_config_type(\n    432     env={\n    433         \"EXTRA_CONDA_PACKAGES\": conda_dependencies,\n   (...)\n    437     image=scheduling_service.backend.config.default_image,\n    438 )\n--&gt; 440 self.deployment.flow.run_and_return(\n    441     parameters,\n    442     run_config=run_config,\n    443     task_name=task_name,\n    444     scheduling_service=scheduling_service,\n    445 )\n\nFile src/dependency_injector/_cwiring.pyx:28, in dependency_injector._cwiring._get_sync_patched._patched()\n\nFile ~/sandbox/lume-services/lume_services/flows/flow.py:254, in Flow.run_and_return(self, parameters, run_config, task_name, scheduling_service)\n    246     scheduling_service.run_and_return(\n    247         parameters=parameters,\n    248         run_config=run_config,\n    249         flow=self.prefect_flow,\n    250         task_name=task_name,\n    251     )\n    253 elif isinstance(scheduling_service.backend, (ServerBackend,)):\n--&gt; 254     scheduling_service.run_and_return(\n    255         parameters=parameters,\n    256         run_config=run_config,\n    257         flow_id=self.flow_id,\n    258         task_name=task_name,\n    259     )\n\nFile ~/sandbox/lume-services/lume_services/services/scheduling/service.py:152, in SchedulingService.run_and_return(self, parameters, run_config, task_name, **kwargs)\n    117 def run_and_return(\n    118     self,\n    119     parameters: Optional[Dict[str, Any]],\n   (...)\n    122     **kwargs\n    123 ) -&gt; Any:\n    124     \"\"\"Run a flow and return result. Implementations should cover instantiation of\n    125     run_config from kwargs as well as backend-specific kwargs.\n    126 \n   (...)\n    150 \n    151     \"\"\"\n--&gt; 152     return self.backend.run_and_return(parameters, run_config, task_name, **kwargs)\n\nFile ~/sandbox/lume-services/lume_services/services/scheduling/backends/server.py:341, in ServerBackend.run_and_return(self, parameters, run_config, task_name, flow_id, timeout, cancel_on_timeout, **kwargs)\n    339     if cancel_on_timeout:\n    340         client.cancel_flow_run(flow_run_id=flow_run_id)\n--&gt; 341     raise err\n    343 logger.debug(\"Watched flow completed.\")\n    344 flow_run = FlowRunView.from_flow_run_id(flow_run_id)\n\nFile ~/sandbox/lume-services/lume_services/services/scheduling/backends/server.py:331, in ServerBackend.run_and_return(self, parameters, run_config, task_name, flow_id, timeout, cancel_on_timeout, **kwargs)\n    329 # watch flow run and stream logs until timeout\n    330 try:\n--&gt; 331     for log in watch_flow_run(\n    332         flow_run_id,\n    333         stream_states=True,\n    334         stream_logs=True,\n    335         max_duration=timeout,\n    336     ):\n    337         logger.info(log)\n    338 except RuntimeError as err:\n\nFile ~/miniconda3/envs/lume-services-dev/lib/python3.9/site-packages/prefect/backend/flow_run.py:108, in watch_flow_run(flow_run_id, stream_states, stream_logs, max_duration)\n    106 # Check whether run has exceeded `max_duration`\n    107 if total_time_elapsed_rounded &gt; int(max_duration.total_seconds()):\n--&gt; 108     raise RuntimeError(\n    109         f\"`watch_flow_run` timed out after \"\n    110         f\"{24 * max_duration.days + round(max_duration.seconds / (60 * 60), 1)} \"\n    111         \"hours of waiting for completion. \"\n    112         f\"Your flow run is still in state: {flow_run.state}\"\n    113     )\n    115 if (\n    116     stream_states  # The agent warning is counted as a state log\n    117     and total_time_elapsed &gt;= agent_warning_initial_wait\n   (...)\n    123     )\n    124 ):\n    125     agent_msg = check_for_compatible_agents(flow_run.labels)\n\nRuntimeError: `watch_flow_run` timed out after 0.0 hours of waiting for completion. Your flow run is still in state: &lt;Submitted: \"Submitted for execution\"&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"developer/notebooks/Model/#lume-services","title":"LUME-services\u00b6","text":""},{"location":"developer/notebooks/Model/#configuration","title":"Configuration:\u00b6","text":"<p>Set the following environment variables:</p> <pre><code>export LUME_MODEL_DB__HOST=127.0.0.1\nexport LUME_MODEL_DB__USER=root\nexport LUME_MODEL_DB__PASSWORD=test\nexport LUME_MODEL_DB__PORT=3306\nexport LUME_MODEL_DB__DATABASE=model_db\nexport LUME_MODEL_DB__CONNECTION__POOL_SIZE=1\nexport LUME_PREFECT__SERVER__TAG=core-1.4.0\nexport LUME_PREFECT__SERVER__HOST_PORT=4200\nexport LUME_PREFECT__SERVER__HOST=http://localhost\nexport LUME_PREFECT__AGENT__HOST_PORT=5000\nexport LUME_PREFECT__AGENT__HOST=http://localhost\nexport LUME_RESULTS_DB__HOST=127.0.0.1\nexport LUME_RESULTS_DB__PORT=27017\nexport LUME_RESULTS_DB__USERNAME=root\nexport LUME_RESULTS_DB__PASSWORD=password\nexport LUME_RESULTS_DB__DATABASE=test\nexport LUME_PREFECT__BACKEND=server \nexport LUME_BACKEND=docker\nexport LUME_ENVIRONMENT__LOCAL_PIP_REPOSITORY=...\nexport LUME_ENVIRONMENT__LOCAL_CONDA_CHANNEL_DIRECTORY=...\n</code></pre> <p>Start the docker services using:</p> <pre><code>lume-services docker start-services\n</code></pre>"},{"location":"developer/notebooks/Model/#use-lume-services-config-to-create-injected-services","title":"Use LUME-services config to create injected services\u00b6","text":""},{"location":"developer/notebooks/Model/#if-youre-running-this-many-time-creation-will-fail-because-of-uniqueness-you-can-reset-since-this-is-a-dev-server","title":"if you're running this many time, creation will fail because of uniqueness... You can reset since this is a dev server\u00b6","text":""},{"location":"developer/notebooks/Model/#store-example-model-deployment-flow-and-project","title":"Store example model, deployment, flow, and project\u00b6","text":""},{"location":"developer/notebooks/Model/#lets-use-the-source-of-the-model-we-created-in-the-demo","title":"Let's use the source of the model we created in the demo\u00b6","text":""},{"location":"examples/Demo/","title":"LUME-services demo","text":"In\u00a0[1]: Copied! <pre>import logging\nlogging.basicConfig(level=logging.INFO)  # Lets check the logs\n</pre> import logging logging.basicConfig(level=logging.INFO)  # Lets check the logs In\u00a0[2]: Copied! <pre>from lume_services import config\nconfig.configure()\n</pre> from lume_services import config config.configure() <pre>INFO:lume_services.config:Configuring LUME-services environment...\nINFO:lume_services.config:Environment configured.\n</pre> In\u00a0[3]: Copied! <pre>#model_db_service._reset()\n</pre> #model_db_service._reset() In\u00a0[4]: Copied! <pre>from lume_services.models import Model\n\nmodel = Model.create_model(\n    author = \"Jackie Garrahan\",\n    laboratory = \"slac\",\n    facility = \"lcls\",\n    beampath = \"cu_hxr\",\n    description = \"test_model\"\n)\nmodel\n</pre> from lume_services.models import Model  model = Model.create_model(     author = \"Jackie Garrahan\",     laboratory = \"slac\",     facility = \"lcls\",     beampath = \"cu_hxr\",     description = \"test_model\" ) model <pre>INFO:lume_services.services.models.db.db:ModelDB selecting: SELECT model.model_id, model.created, model.author, model.laboratory, model.facility, model.beampath, model.description \nFROM model \nWHERE model.author = :author_1 AND model.laboratory = :laboratory_1 AND model.facility = :facility_1 AND model.beampath = :beampath_1 AND model.description = :description_1\nINFO:lume_services.services.models.db.db:ModelDB inserting: INSERT INTO model (author, laboratory, facility, beampath, description) VALUES (:author, :laboratory, :facility, :beampath, :description)\nINFO:lume_services.services.models.db.db:Sucessfully executed: INSERT INTO model (author, laboratory, facility, beampath, description) VALUES (:author, :laboratory, :facility, :beampath, :description)\nINFO:lume_services.services.models.db.db:ModelDB selecting: SELECT model.model_id, model.created, model.author, model.laboratory, model.facility, model.beampath, model.description \nFROM model \nWHERE model.model_id = :model_id_1\n</pre> Out[4]: <pre>Model(metadata=Model(                     model_id=1,                     created=datetime.datetime(2022, 10, 1, 6, 48, 27),                     author='Jackie Garrahan'),                     laboratory='slac',                     facility='lcls',                     beampath='cu_hxr',                     description='test_model'                 ), deployment=None, results=None)</pre> In\u00a0[5]: Copied! <pre>model_db_service = config.context.model_db_service()\nscheduling_service = config.context.scheduling_service()\n</pre> model_db_service = config.context.model_db_service() scheduling_service = config.context.scheduling_service() In\u00a0[6]: Copied! <pre>### NOTE: The below cell will raise an error if run 2x\n</pre> ### NOTE: The below cell will raise an error if run 2x In\u00a0[7]: Copied! <pre># create a project\nproject_name = model_db_service.store_project(\n    project_name=\"test\", description=\"my_description\"\n)\nscheduling_service.create_project(\"test\")\n</pre> # create a project project_name = model_db_service.store_project(     project_name=\"test\", description=\"my_description\" ) scheduling_service.create_project(\"test\") <pre>INFO:lume_services.services.models.db.db:ModelDB inserting: INSERT INTO project (project_name, description) VALUES (:project_name, :description)\nINFO:lume_services.services.models.db.db:Sucessfully executed: INSERT INTO project (project_name, description) VALUES (:project_name, :description)\n</pre> <p>You can now find this project in you Prefect UI at http://localhost:8080</p> <p></p> In\u00a0[8]: Copied! <pre>source_path = \"https://github.com/jacquelinegarrahan/my-model/releases/download/v0.0.44/my_model-0.0.44.tar.gz\"\n# populates local channel\nmodel.store_deployment(source_path, project_name=\"test\")\n</pre> source_path = \"https://github.com/jacquelinegarrahan/my-model/releases/download/v0.0.44/my_model-0.0.44.tar.gz\" # populates local channel model.store_deployment(source_path, project_name=\"test\") <pre>INFO:lume_services.models.model:installing package\nINFO:lume_services.environment.solver:https://github.com/jacquelinegarrahan/my-model/releases/download/v0.0.44/my_model-0.0.44.tar.gz saved to /var/folders/nh/g2v_nmtj7t1g94gmjtgjrk3r0000gn/T/tmp4c5jbhpr/my_model-0.0.44.tar.gz\nINFO:lume_services.environment.solver:Version 0.0.44 of my_model already installed.\nINFO:lume_services.services.models.db.db:ModelDB selecting: SELECT deployment.sha256, deployment.deployment_id, deployment.version, deployment.deploy_date, deployment.package_import_name, deployment.asset_dir, deployment.source, deployment.image, deployment.is_live, deployment.model_id \nFROM deployment \nWHERE deployment.model_id = :model_id_1 AND deployment.version = :version_1\nINFO:lume_services.services.models.db.db:ModelDB inserting: INSERT INTO deployment (version, package_import_name, asset_dir, source, sha256, image, is_live, model_id) VALUES (:version, :package_import_name, :asset_dir, :source, :sha256, :image, :is_live, :model_id)\nINFO:lume_services.services.models.db.db:Sucessfully executed: INSERT INTO deployment (version, package_import_name, asset_dir, source, sha256, image, is_live, model_id) VALUES (:version, :package_import_name, :asset_dir, :source, :sha256, :image, :is_live, :model_id)\nINFO:lume_services.services.models.db.db:ModelDB selecting: SELECT flow.flow_id, flow.flow_name, flow.project_name, flow.deployment_id \nFROM flow \nWHERE flow.deployment_id = :deployment_id_1\nINFO:lume_services.services.scheduling.backends.server:Flow run config is not empty. Clearing existing labels and assigning                     new.\n/Users/jacquelinegarrahan/miniconda3/envs/my-model-dev/lib/python3.10/site-packages/prefect/core/flow.py:1726: UserWarning: No result handler was specified on your Flow. Cloud features such as input caching and resuming task runs from failure may not work properly.\n  registered_flow = client.register(\nINFO:lume_services.services.models.db.db:ModelDB inserting: INSERT INTO flow (flow_id, flow_name, project_name, deployment_id) VALUES (:flow_id, :flow_name, :project_name, :deployment_id)\nINFO:lume_services.services.models.db.db:Sucessfully executed: INSERT INTO flow (flow_id, flow_name, project_name, deployment_id) VALUES (:flow_id, :flow_name, :project_name, :deployment_id)\nINFO:lume_services.models.model:Loading deployment 1\nINFO:lume_services.models.model:Loading deployment 1\nINFO:lume_services.services.models.db.db:ModelDB selecting: SELECT deployment.sha256, deployment.deployment_id, deployment.version, deployment.deploy_date, deployment.package_import_name, deployment.asset_dir, deployment.source, deployment.image, deployment.is_live, deployment.model_id \nFROM deployment \nWHERE deployment.model_id = :model_id_1 AND deployment.deployment_id = :deployment_id_1\nINFO:lume_services.models.model:Deployment loaded.\nINFO:lume_services.services.models.db.db:ModelDB selecting: SELECT flow.flow_id, flow.flow_name, flow.project_name, flow.deployment_id \nFROM flow \nWHERE flow.deployment_id = :deployment_id_1\nINFO:lume_services.services.models.db.db:ModelDB selecting: SELECT project.project_name, project.description \nFROM project \nWHERE project.project_name = :project_name_1\nINFO:lume_services.services.models.db.db:ModelDB selecting: SELECT flow_of_flows._id, flow_of_flows.parent_flow_id, flow_of_flows.flow_id, flow_of_flows.position \nFROM flow_of_flows \nWHERE flow_of_flows.parent_flow_id = :parent_flow_id_1\n</pre> <pre>Flow URL: http://localhost:8080/default/flow/c13c5222-b967-404c-bf41-5ff5c1f0f38e\n \u2514\u2500\u2500 ID: 8352b84e-ae5f-4e86-9143-921b0eb6d37d\n \u2514\u2500\u2500 Project: test\n \u2514\u2500\u2500 Labels: ['lume-services']\n</pre> In\u00a0[9]: Copied! <pre>flow_run = model.deployment.flow.prefect_flow.run(**{\n                        \"input1\": 1, \n                        \"input2\": 2, \n                        \"filename\": \"test_file.txt\", \n                        \"filesystem_identifier\":\"local\"\n    }\n)\n</pre> flow_run = model.deployment.flow.prefect_flow.run(**{                         \"input1\": 1,                          \"input2\": 2,                          \"filename\": \"test_file.txt\",                          \"filesystem_identifier\":\"local\"     } ) <pre>[2022-09-30 23:48:28-0700] INFO - prefect.FlowRunner | Beginning Flow run for 'my-model'\n</pre> <pre>INFO:prefect.FlowRunner:Beginning Flow run for 'my-model'\n</pre> <pre>[2022-09-30 23:48:28-0700] INFO - prefect.TaskRunner | Task 'configure_lume_services': Starting task run...\n</pre> <pre>INFO:prefect.TaskRunner:Task 'configure_lume_services': Starting task run...\nINFO:lume_services.config:Configuring LUME-services environment...\nINFO:lume_services.config:Environment configured.\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'configure_lume_services': Finished task run for task with final state: 'Success'\n</pre> <pre>INFO:prefect.TaskRunner:Task 'configure_lume_services': Finished task run for task with final state: 'Success'\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'filename': Starting task run...\n</pre> <pre>INFO:prefect.TaskRunner:Task 'filename': Starting task run...\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'filename': Finished task run for task with final state: 'Success'\n</pre> <pre>INFO:prefect.TaskRunner:Task 'filename': Finished task run for task with final state: 'Success'\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'check_local_execution': Starting task run...\n</pre> <pre>INFO:prefect.TaskRunner:Task 'check_local_execution': Starting task run...\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'check_local_execution': Finished task run for task with final state: 'Success'\n</pre> <pre>INFO:prefect.TaskRunner:Task 'check_local_execution': Finished task run for task with final state: 'Success'\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'input1': Starting task run...\n</pre> <pre>INFO:prefect.TaskRunner:Task 'input1': Starting task run...\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'input1': Finished task run for task with final state: 'Success'\n</pre> <pre>INFO:prefect.TaskRunner:Task 'input1': Finished task run for task with final state: 'Success'\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'input2': Starting task run...\n</pre> <pre>INFO:prefect.TaskRunner:Task 'input2': Starting task run...\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'input2': Finished task run for task with final state: 'Success'\n</pre> <pre>INFO:prefect.TaskRunner:Task 'input2': Finished task run for task with final state: 'Success'\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'filesystem_identifier': Starting task run...\n</pre> <pre>INFO:prefect.TaskRunner:Task 'filesystem_identifier': Starting task run...\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'filesystem_identifier': Finished task run for task with final state: 'Success'\n</pre> <pre>INFO:prefect.TaskRunner:Task 'filesystem_identifier': Finished task run for task with final state: 'Success'\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'case(False)': Starting task run...\n</pre> <pre>INFO:prefect.TaskRunner:Task 'case(False)': Starting task run...\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'case(False)': Finished task run for task with final state: 'Success'\n</pre> <pre>INFO:prefect.TaskRunner:Task 'case(False)': Finished task run for task with final state: 'Success'\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'List': Starting task run...\n</pre> <pre>INFO:prefect.TaskRunner:Task 'List': Starting task run...\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'List': Finished task run for task with final state: 'Success'\n</pre> <pre>INFO:prefect.TaskRunner:Task 'List': Finished task run for task with final state: 'Success'\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'Dict': Starting task run...\n</pre> <pre>INFO:prefect.TaskRunner:Task 'Dict': Starting task run...\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'Dict': Finished task run for task with final state: 'Success'\n</pre> <pre>INFO:prefect.TaskRunner:Task 'Dict': Finished task run for task with final state: 'Success'\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'prepare_lume_model_variables': Starting task run...\n</pre> <pre>INFO:prefect.TaskRunner:Task 'prepare_lume_model_variables': Starting task run...\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'prepare_lume_model_variables': Finished task run for task with final state: 'Success'\n</pre> <pre>INFO:prefect.TaskRunner:Task 'prepare_lume_model_variables': Finished task run for task with final state: 'Success'\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'evaluate': Starting task run...\n</pre> <pre>INFO:prefect.TaskRunner:Task 'evaluate': Starting task run...\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'evaluate': Finished task run for task with final state: 'Success'\n</pre> <pre>INFO:prefect.TaskRunner:Task 'evaluate': Finished task run for task with final state: 'Success'\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'format_file': Starting task run...\n</pre> <pre>INFO:prefect.TaskRunner:Task 'format_file': Starting task run...\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'format_file': Finished task run for task with final state: 'Success'\n</pre> <pre>INFO:prefect.TaskRunner:Task 'format_file': Finished task run for task with final state: 'Success'\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'save_file': Starting task run...\n</pre> <pre>INFO:prefect.TaskRunner:Task 'save_file': Starting task run...\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'save_file': Finished task run for task with final state: 'Success'\n</pre> <pre>INFO:prefect.TaskRunner:Task 'save_file': Finished task run for task with final state: 'Success'\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'format_result': Starting task run...\n</pre> <pre>INFO:prefect.TaskRunner:Task 'format_result': Starting task run...\nWARNING:lume_services.results.generic:No project_name passed to result\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'format_result': Finished task run for task with final state: 'Success'\n</pre> <pre>INFO:prefect.TaskRunner:Task 'format_result': Finished task run for task with final state: 'Success'\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'save_db_result': Starting task run...\n</pre> <pre>INFO:prefect.TaskRunner:Task 'save_db_result': Starting task run...\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.TaskRunner | Task 'save_db_result': Finished task run for task with final state: 'Success'\n</pre> <pre>INFO:prefect.TaskRunner:Task 'save_db_result': Finished task run for task with final state: 'Success'\n</pre> <pre>[2022-09-30 23:48:29-0700] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded\n</pre> <pre>INFO:prefect.FlowRunner:Flow run SUCCESS: all reference tasks succeeded\n</pre> In\u00a0[10]: Copied! <pre>model.run(\n    parameters={\n        \"input1\": 1, \n        \"input2\": 2, \n        \"filename\": \"/lume-services/data/test_file.txt\", \n        \"filesystem_identifier\":\"mounted\"}\n)\n</pre> model.run(     parameters={         \"input1\": 1,          \"input2\": 2,          \"filename\": \"/lume-services/data/test_file.txt\",          \"filesystem_identifier\":\"mounted\"} ) In\u00a0[\u00a0]: Copied! <pre>model.deployment.flow\n</pre> model.deployment.flow In\u00a0[\u00a0]: Copied! <pre>results = model.get_results()\nresults\n</pre> results = model.get_results() results In\u00a0[\u00a0]: Copied! <pre>results_df = model.get_results_df()\nresults_df\n</pre> results_df = model.get_results_df() results_df In\u00a0[\u00a0]: Copied! <pre>from importlib import reload \nimport lume_services\nreload(lume_services.models)\nfrom lume_services.models import Model\n\nmodel_id = model.metadata.model_id\nloaded_model = Model(model_id=model_id)\n</pre> from importlib import reload  import lume_services reload(lume_services.models) from lume_services.models import Model  model_id = model.metadata.model_id loaded_model = Model(model_id=model_id) In\u00a0[\u00a0]: Copied! <pre>loaded_model.metadata\n</pre> loaded_model.metadata In\u00a0[\u00a0]: Copied! <pre>loaded_model.load_deployment()\nloaded_model.deployment\n</pre> loaded_model.load_deployment() loaded_model.deployment In\u00a0[\u00a0]: Copied! <pre>results = loaded_model.run_and_return(\n    collection=\"test\",\n    parameters={\n                    \"input1\": 1.0, \n                    \"input2\": 2.0, \n                    \"filename\": \"/lume-services/data/test_file.txt\", \n                    \"filesystem_identifier\":\"local\"\n    },\n    task_name=\"save_db_result\" # Want to get the result from the save_db_result task\n)\nres\n</pre> results = loaded_model.run_and_return(     collection=\"test\",     parameters={                     \"input1\": 1.0,                      \"input2\": 2.0,                      \"filename\": \"/lume-services/data/test_file.txt\",                      \"filesystem_identifier\":\"local\"     },     task_name=\"save_db_result\" # Want to get the result from the save_db_result task ) res In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/Demo/#lume-services-demo","title":"LUME-services demo\u00b6","text":"<p>In this notebook, we will configure LUME-services to use the service configuration used to launch our docker-compose services. Make sure you've completed all steps outlined in https://slaclab.github.io/lume-services/demo/.</p>"},{"location":"examples/Demo/#configure-services","title":"Configure services\u00b6","text":"<p>LUME-services is packages with a configuration utility that reads environment variables and initializes services:</p>"},{"location":"examples/Demo/#if-youre-running-this-many-time-creation-will-fail-because-of-uniqueness-you-can-reset-since-this-is-a-dev-server","title":"if you're running this many time, creation will fail because of uniqueness... You can reset since this is a dev server\u00b6","text":""},{"location":"examples/Demo/#create-a-model","title":"Create a model\u00b6","text":"<p>The LUME-services Model provides an API to all model services and facilitates all model operations.</p>"},{"location":"examples/Demo/#create-a-project","title":"Create a project\u00b6","text":"<p>Workflows are organized by the Prefect scheduler into different projects. Below, we access the configured services directly (TODO create project registry utility)</p>"},{"location":"examples/Demo/#create-a-deployment-for-your-model","title":"Create a deployment for your model\u00b6","text":"<p>Replace <code>source_path</code> with the path to your release tarball below:</p>"},{"location":"examples/Demo/#run-the-prefect-workflow-directly","title":"Run the Prefect workflow directly\u00b6","text":""},{"location":"examples/Demo/#run-the-workflow-inside-the-service-cluster","title":"Run the workflow inside the service cluster\u00b6","text":"<p>We can use the model interface to directly deploy workflows. When sourcing our environment (<code>docs/examples/demo.env</code>), we defined a mount point for the file system using the alias <code>/lume-services/data</code>. Let's kick off this workflow and save the file output to that directory. After running the next cell, you'll be able to see the running container in your docker desktop and examine the flow using the Prefect UI at http://localhost:8080/default?flows.</p>"},{"location":"examples/Demo/#get-results","title":"Get results:\u00b6","text":""},{"location":"examples/Demo/#load-model-using-model-id","title":"Load model using model id\u00b6","text":"<p>Once your model has been registered, you can use the <code>Model</code> api object to interact with your model without running the above registration steps. Let's load a new model object using the model_id we registered above.</p>"},{"location":"examples/Demo/#load-existing-model-object","title":"Load existing model object\u00b6","text":"<p>Loading a model using the load_deployment method without passing a deployment_id will load the latest deployment registered for the model.</p>"},{"location":"examples/flow_of_flows/","title":"Flow of flows","text":"In\u00a0[\u00a0]: Copied! <pre>from lume_services.flows.flow_of_flows import FlowOfFlows\nfrom lume_services.services.files.filesystems.mounted import MountedFilesystem\nfrom lume_services.services.scheduling.backends.docker import DockerRunConfig\nfrom lume_services.tests.files import FLOW_OF_FLOWS_YAML\nfrom lume_services.tests.files.flows.flow1 import flow as flow1\nfrom lume_services.tests.files.flows.flow2 import flow as flow2\nfrom lume_services.tests.files.flows.flow3 import flow as flow3\nfrom lume_services import config\n\nimport os\nimport logging\n\nfrom lume_services.tests.scheduling.test_flows import project_name\n\nlogging.basicConfig(level=\"DEBUG\")\n#logger = logging.getLogger(__name__)\n#logger.setLevel(logging.DEBUG)\n#lume_services_logger = logging.getLogger(\"lume_services.services.scheduling.backends.server\")\n#lume_services_logger.setLevel(logging.DEBUG)\n\nos.environ[\"LUME_BACKEND\"] = \"docker\"\nos.environ[\"LUME_MOUNTED_FILESYSTEM__IDENTIFIER\"] = \"mounted\"\nos.environ[\"LUME_MOUNTED_FILESYSTEM__MOUNT_PATH\"] = \"/Users/jgarra/sandbox/lume-services\"\nos.environ[\"LUME_MOUNTED_FILESYSTEM__MOUNT_ALIAS\"] = \"/User/my_user/data\"\nmounted_filesystem =  MountedFilesystem(\n        mount_path=\"/Users/jgarra/sandbox/lume-services\", mount_alias=\"/User/my_user/data\", identifier=\"mounted\"\n    )\n\nlume_services_settings =  config.LUMEServicesSettings()\n\nconfig.configure(lume_services_settings)\n\n\nfrom lume_services.config import context\n\nscheduling_service = context.scheduling_service()\nbackend = scheduling_service.backend\n\nbackend.create_project(\"test\")\n\nflow1_id = backend.register_flow(\n        flow1,\n        project_name=\"test\",\n        image=\"jgarrahan/lume-services-prefect:latest\",\n        labels=[\"lume-services\"],\n    )\n\n\nflow2_id = backend.register_flow(\n        flow2,\n        project_name=\"test\",\n        image=\"jgarrahan/lume-services-prefect:latest\",\n        labels=[\"lume-services\"],\n    )\n\nflow3_id = backend.register_flow(\n        flow3,\n        project_name=\"test\",\n        image=\"jgarrahan/lume-services-prefect:latest\",\n        labels=[\"lume-services\"],\n    )\n\n\nfilename = f\"{mounted_filesystem.mount_alias}/test_docker_backend.txt\"\ndata1 = {\n    \"text1\": \"hey\",\n    \"text2\": \" you\",\n    \"filename\": filename,\n    \"filesystem_identifier\": \"mounted\",\n}\n\n\ndata2 = {\"file_rep\": {\"filename\": filename, \"filesystem_identifier\": \"mounted\",\"file_type_str\": \"lume_services.files.file:TextFile\"}}\ndata3 = {\"text1\": \"hey you\", \"text2\": \"hey you\"}\n\n\nflow_of_flows = FlowOfFlows.from_yaml(\n    FLOW_OF_FLOWS_YAML, scheduling_service=scheduling_service\n)\nflow_of_flows.compose()\nflow_id = flow_of_flows.register()\n\n\n\nlume_env = {name: val for name, val in os.environ.items() if \"LUME\" in name}\n# Need to convert to docker network hostnames\nlume_env[\"LUME_RESULTS_DB__HOST\"] = \"mongodb\"\nlume_env[\"LUME_MODEL_DB__HOST\"] = \"mysql\"\n\nhost_config = {\n    \"mounts\":  {\n                \"target\": mounted_filesystem.mount_alias,\n                \"source\": mounted_filesystem.mount_path,\n                \"type\": \"bind\",\n            }\n}\n\ndata = {'flow1-filesystem_identifier': \"mounted\", 'flow1-filename': filename, 'flow1-text1': \"hey\", 'flow1-text2': \" you\", 'flow3-text2': \"hey you\"}\n\nrun_config = DockerRunConfig(\n        image=\"jgarrahan/lume-services-prefect:latest\", env=lume_env, host_config=host_config\n    )\n    \n\nbackend.run_and_return(flow_id = flow_id, parameters=data, run_config = run_config)\n</pre>    from lume_services.flows.flow_of_flows import FlowOfFlows from lume_services.services.files.filesystems.mounted import MountedFilesystem from lume_services.services.scheduling.backends.docker import DockerRunConfig from lume_services.tests.files import FLOW_OF_FLOWS_YAML from lume_services.tests.files.flows.flow1 import flow as flow1 from lume_services.tests.files.flows.flow2 import flow as flow2 from lume_services.tests.files.flows.flow3 import flow as flow3 from lume_services import config  import os import logging  from lume_services.tests.scheduling.test_flows import project_name  logging.basicConfig(level=\"DEBUG\") #logger = logging.getLogger(__name__) #logger.setLevel(logging.DEBUG) #lume_services_logger = logging.getLogger(\"lume_services.services.scheduling.backends.server\") #lume_services_logger.setLevel(logging.DEBUG)  os.environ[\"LUME_BACKEND\"] = \"docker\" os.environ[\"LUME_MOUNTED_FILESYSTEM__IDENTIFIER\"] = \"mounted\" os.environ[\"LUME_MOUNTED_FILESYSTEM__MOUNT_PATH\"] = \"/Users/jgarra/sandbox/lume-services\" os.environ[\"LUME_MOUNTED_FILESYSTEM__MOUNT_ALIAS\"] = \"/User/my_user/data\" mounted_filesystem =  MountedFilesystem(         mount_path=\"/Users/jgarra/sandbox/lume-services\", mount_alias=\"/User/my_user/data\", identifier=\"mounted\"     )  lume_services_settings =  config.LUMEServicesSettings()  config.configure(lume_services_settings)   from lume_services.config import context  scheduling_service = context.scheduling_service() backend = scheduling_service.backend  backend.create_project(\"test\")  flow1_id = backend.register_flow(         flow1,         project_name=\"test\",         image=\"jgarrahan/lume-services-prefect:latest\",         labels=[\"lume-services\"],     )   flow2_id = backend.register_flow(         flow2,         project_name=\"test\",         image=\"jgarrahan/lume-services-prefect:latest\",         labels=[\"lume-services\"],     )  flow3_id = backend.register_flow(         flow3,         project_name=\"test\",         image=\"jgarrahan/lume-services-prefect:latest\",         labels=[\"lume-services\"],     )   filename = f\"{mounted_filesystem.mount_alias}/test_docker_backend.txt\" data1 = {     \"text1\": \"hey\",     \"text2\": \" you\",     \"filename\": filename,     \"filesystem_identifier\": \"mounted\", }   data2 = {\"file_rep\": {\"filename\": filename, \"filesystem_identifier\": \"mounted\",\"file_type_str\": \"lume_services.files.file:TextFile\"}} data3 = {\"text1\": \"hey you\", \"text2\": \"hey you\"}   flow_of_flows = FlowOfFlows.from_yaml(     FLOW_OF_FLOWS_YAML, scheduling_service=scheduling_service ) flow_of_flows.compose() flow_id = flow_of_flows.register()    lume_env = {name: val for name, val in os.environ.items() if \"LUME\" in name} # Need to convert to docker network hostnames lume_env[\"LUME_RESULTS_DB__HOST\"] = \"mongodb\" lume_env[\"LUME_MODEL_DB__HOST\"] = \"mysql\"  host_config = {     \"mounts\":  {                 \"target\": mounted_filesystem.mount_alias,                 \"source\": mounted_filesystem.mount_path,                 \"type\": \"bind\",             } }  data = {'flow1-filesystem_identifier': \"mounted\", 'flow1-filename': filename, 'flow1-text1': \"hey\", 'flow1-text2': \" you\", 'flow3-text2': \"hey you\"}  run_config = DockerRunConfig(         image=\"jgarrahan/lume-services-prefect:latest\", env=lume_env, host_config=host_config     )       backend.run_and_return(flow_id = flow_id, parameters=data, run_config = run_config)"},{"location":"files/env_vars/","title":"Base Configuration","text":"Name Type Default LUME_BACKEND string local"},{"location":"files/env_vars/#filesystem-configuration","title":"Filesystem Configuration","text":"Name Type Default LUME_MOUNTED_FILESYSTEM__IDENTIFIER string mounted LUME_MOUNTED_FILESYSTEM__MOUNT_PATH string LUME_MOUNTED_FILESYSTEM__MOUNT_ALIAS string LUME_MOUNTED_FILESYSTEM__MOUNT_TYPE string DirectoryOrCreate"},{"location":"files/env_vars/#model-database","title":"Model Database","text":"Name Type Default LUME_MODEL_DB__HOST string LUME_MODEL_DB__PORT integer LUME_MODEL_DB__USER string LUME_MODEL_DB__PASSWORD string LUME_MODEL_DB__DATABASE string LUME_MODEL_DB__CONNECTION__POOL_SIZE integer LUME_MODEL_DB__CONNECTION__POOL_PRE_PING boolean True LUME_MODEL_DB__DIALECT_STR string mysql+pymysql"},{"location":"files/env_vars/#results-database","title":"Results Database","text":"Name Type Default LUME_RESULTS_DB__DATABASE string LUME_RESULTS_DB__USERNAME string LUME_RESULTS_DB__HOST string LUME_RESULTS_DB__PASSWORD string LUME_RESULTS_DB__PORT integer LUME_RESULTS_DB__AUTHMECHANISM string DEFAULT LUME_RESULTS_DB__OPTIONS object {}"},{"location":"files/env_vars/#scheduling-service","title":"Scheduling Service","text":"Name Type Default LUME_PREFECT__SERVER__TAG string core-1.4.0 LUME_PREFECT__SERVER__HOST string http://localhost LUME_PREFECT__SERVER__HOST_PORT string 4200 LUME_PREFECT__SERVER__HOST_IP string 127.0.0.1 LUME_PREFECT__UI__HOST string http://localhost LUME_PREFECT__UI__HOST_PORT string 8080 LUME_PREFECT__UI__HOST_IP string 127.0.0.1 LUME_PREFECT__UI__APOLLO_URL string http://localhost:4200/graphql LUME_PREFECT__TELEMETRY__ENABLED boolean True LUME_PREFECT__AGENT__HOST string http://localhost LUME_PREFECT__AGENT__HOST_PORT string 5000 LUME_PREFECT__HOME_DIR string ~/.prefect LUME_PREFECT__DEBUG boolean False LUME_PREFECT__BACKEND string server LUME_PREFECT__IMAGE string jgarrahan/lume-services-prefect:latest LUME_PREFECT__ISOLATED boolean False"},{"location":"services/files/","title":"File service","text":"<p>The file service is intented to provide an abstraction to filesystem read/writes allowing for the the implementation of interfaces to remote or mounted resources. The file service can be configured to interface with multiple <code>Filesystem</code> resources.</p> <p>LUME-serices is packaged with <code>LocalFilesystem</code> and <code>MountedFilesystem</code> implementations, however, the <code>Filesystem</code> interface defined in <code>lume_services.services.files.filesystems.filesystem</code> can be used to implement any number of custom interfaces including remote cloud services.</p> <p></p>"},{"location":"services/files/#filesystems","title":"Filesystems","text":""},{"location":"services/files/#localfilesystem","title":"LocalFilesystem","text":"<p>The local filesystem defined at <code>lume_services.services.files.filesystems.local</code> uses the raw path provided to save and load methods for access to files.</p>"},{"location":"services/files/#mountedfilesystem","title":"MountedFilesystem","text":"<p>The mounted filesystem interface defined at <code>lume_services.services.files.filesystems.mounted</code> aims to provide a handler for filesystems or directories mounted to containerized services, as with Docker and Kubernetes. </p> <p>The handler checks the mount path for files and performs substring substitutions on full paths.</p> <p>The mounted filesystem accommodates various mount types:</p> <pre><code># types associated with mounting host filesystem to kubernetes\n# https://kubernetes.io/docs/concepts/storage/volumes/#hostpath\ndirectory = \"Directory\"  # directory must exist at given path\ndirectory_or_create = (\n    \"DirectoryOrCreate\"  # if directory does not exist, directory created\n)\nfile = \"File\"  # file must exist at path\nfile_or_create = \"FileOrCreate\"  # will create file if does not exist\n# socket = \"Socket\" # Unix socket must exist at given path\n# char_device = \"CharDevice\" # Character device must exist at given path\n# block_device = \"BlockDevice\" # block device must exist at given path\n</code></pre>"},{"location":"services/hpc/","title":"HPC service","text":"<p>The HPC service packaged here defines an interface for interacting with HPC compute tools...</p>"},{"location":"services/models/","title":"Model Service","text":"<p>The model service provides a registry for models and their metadata. Metadata is organized into the following tables:</p> <ol> <li> <p>Models The Models table tracks top-level information about a model. This includes:</p> </li> <li> <p>author</p> </li> <li>laboratory</li> <li>facility</li> <li>beampath</li> <li>description</li> <li>create date (generated on input)</li> <li> <p>model id (generated on input)</p> </li> <li> <p>Deployments Deployments are versioned releases of a registered model. Deployments include information about source code and container image:</p> </li> <li> <p>sha256: hash of the source</p> </li> <li>deployment_id (generated on input)</li> <li>version</li> <li>package_import_name: python import string</li> <li>asset_dir: directory for designated imports (** this should be flushed out)</li> <li>source: uri of package source</li> <li>image: Name of the container image</li> <li> <p>is_live: Whether the deployment is live in production</p> </li> <li> <p>Projects</p> </li> </ol> <p>Prefect organizes flows into projects. This table tracks projects as registered with Prefect and allows providing a description.</p> <ul> <li>Project name</li> <li> <p>Description</p> </li> <li> <p>Flows</p> </li> </ul> <p>Flows track the relationship between deployments and Prefect flow metadata:</p> <ul> <li>Flow ID: ID of flow generated by Prefect</li> <li>Flow name</li> <li>Project name</li> <li> <p>Deployment ID: ID of corresponding deployment</p> </li> <li> <p>Flow of flows</p> </li> </ul> <p>Workflows may be constructed by stitching together many subflows. This table tracks the stitching by a one-to-many mapping between the parent flow id in the Flows table, to entries the Flow of Flows table.</p> <ul> <li>Parent flow ID:  ID of parent flow</li> <li>Flow ID: ID of subflow</li> <li>Position: Relative position in the flow of flows composition. For example, the second flow would have 2 in this field.</li> </ul> <p>Schema is defined using the sqlalchemy API in <code>lume_services/services/models/db/schema.py</code>.:</p> <p></p> <p></p>"},{"location":"services/models/#updating-the-model-schema","title":"Updating the model schema","text":"<p>On any changes to the schema, the database init script for the docker-compose must be updated.</p> <p>From the repository root run, <pre><code>python scripts/update_docker_compose_schema.py build-docker-compose-schema\n</code></pre></p> <p>This will automatically render the schema file in <code>lume_services/docker/files/model-db-init.sql</code>. Now, you can add this file updated file to the git repository.</p>"},{"location":"services/models/#sqlalchemy-notes","title":"Sqlalchemy notes","text":"<p>Sqlalchemy can be configured to use a number of different dialects. The database implementation in <code>lume_services/services/models/db/db.py</code> defaults to using a <code>mysql</code> connection, as indicated with the <code>dialect_str=\"mysql+pymysql\"</code> attribute on the ModelDBConfig object. Additional dialects can be accomodated by assigning this dialect string.</p> <p>At present, LUME-services does not take advantage of all the features of sqlalchemy, most notably the ability to do joined loads to link data between tables using relationships.</p> <p>All queries could be adjusted to do things like joined loads for table relationships, etc.</p>"},{"location":"services/models/#api","title":"API","text":"<p>LUME-services defines an API for model objects in <code>lume_services.models.model</code>. </p> <p>A model's deployments can be accessed using the Model.deployments attribute, which will return a list of deployments associated with the model. This relationship is established in the sqlalchemy schema:</p>"},{"location":"services/models/#environment-resolution","title":"Environment resolution","text":"<p>LUME-services provides an interface to install packages from a given source. Below, we create a model and store a deployment.</p> <pre><code>from lume_services.models import Model\nfrom lume_services import config\nconfig.configure()\n\n\nmodel = Model.create_model(\n    author = \"Jackie Garrahan\",\n    laboratory = \"slac\",\n    facility = \"lcls\",\n    beampath = \"cu_hxr\",\n    description = \"test_model\"\n)\n\n\nmodel_db_service = config.context.model_db_service()\nscheduling_service = config.context.scheduling_service()\n\n\n# create a project\nproject_name = model_db_service.store_project(\n    project_name=\"test\", description=\"my_description\"\n)\nscheduling_service.create_project(\"test\")\n\n\nsource_path = \"https://github.com/jacquelinegarrahan/my-model/releases/download/v0.0.44/my_model-0.0.44.tar.gz\"\n# populates local channel\nmodel.store_deployment(source_path, project_name=\"test\")\n</code></pre>"},{"location":"services/results/","title":"Results DB Service","text":""},{"location":"services/results/#results-database-service","title":"Results database service","text":"<p>Queries using pymongo</p>"},{"location":"services/results/#model-documents","title":"Model documents","text":"<p>Model documents define the schema for a model's stored results</p>"},{"location":"services/results/#files","title":"Files","text":""},{"location":"services/results/#results-database-entries","title":"Results Database Entries","text":"<p>Results objects provide an easy interface with ...</p> <p>PrefectResults stored in Prefect Core's server</p>"},{"location":"services/results/#generic","title":"Generic","text":""},{"location":"services/results/#impact","title":"Impact","text":""},{"location":"services/results/#custom","title":"Custom","text":"<p>Custom results must subcalss the generic Result object.</p> <pre><code>class CustomResult(Result):\n    \"\"\"Extends Result base and implements model specific attributes\"\"\"\n</code></pre>"},{"location":"services/results/#_1","title":"Results DB Service","text":"<p>LUME-services is packaged with seveal pre-configured model results: <code>Impact</code>, <code>Generic</code></p> <p>The model categories</p>"},{"location":"services/results/#development","title":"Development","text":"<p>In the event that a different result storage scheme would like to be used the steps are as followed: 1. Subclass result     - 2. Creation of database service     - Implementation of <code>ResultsDB</code> class in <code>lume_services.services.results.db</code>.     - Methods should manage connections (multiprocessing and thread-safe) &amp; translate above custom document representations to database</p> <pre><code>from lume_services.services.results.db import ResultsDB, ResultsDBConfig\nfrom lume_services.services.results.results_service import ResultsDBService\n\nclass MyCustomDB(ResultsDB):\n    ...\n\nclass CustomDBServiceConfig(ResultsDBConfig):\n    url: str\n\ncustom_db_service_config = CustomDBServiceConfig(\n    url=\"blah://my-connection-url\",\n)\n\nmy_db_service = MyCustomDBService(\n    custom_db_service_config\n)\nresults_db_service = ResultsDBService(my_db_service)\n</code></pre>"},{"location":"services/results/#result-documents","title":"Result documents","text":"<p>Results are organized into artifacts called documents</p>"},{"location":"services/results/#custom-indices","title":"Custom indices","text":"<p>It may be useful to overwrite the indices given in the base class...</p>"},{"location":"services/results/#user-roles","title":"User roles","text":"<p>https://www.mongodb.com/docs/manual/core/collection-level-access-control/</p>"},{"location":"services/scheduling/","title":"Scheduling Service","text":"<p>https://docs.prefect.io/core/concepts/configuration.html#user-configuration</p> <ul> <li>Scheduling service provides interface to Prefect Scheduling</li> <li>Configurable backends</li> <li>Backend returns a Prefect Run Config</li> </ul> <p></p>"},{"location":"services/scheduling/#prefect","title":"Prefect","text":"<p>Core functionality - Ability to programatically create workflows     - functional organization of workflow steps     - mapping the outputs of one task to inputs of another     - result persistence - Specifications for how to store the workflows</p> <p>How LUME-services uses Prefect: - Creation of workflows     - Workflows defined a single flow file packaged with a project     -</p>"},{"location":"services/scheduling/#backends","title":"Backends","text":"<p>The Scheduling Service is set up to interface with different backends.</p>"},{"location":"services/scheduling/#kubernetes","title":"Kubernetes","text":""},{"location":"services/scheduling/#docker","title":"Docker","text":""},{"location":"services/scheduling/#agents","title":"Agents","text":""},{"location":"services/scheduling/#kubernetes_1","title":"Kubernetes","text":"<ul> <li>Configuring with dictionary</li> </ul>"},{"location":"services/scheduling/#local","title":"Local","text":"<ul> <li>Configuring with dictionary</li> </ul>"},{"location":"services/scheduling/#labels","title":"Labels","text":""},{"location":"services/scheduling/#results","title":"Results","text":"<ul> <li>Results stored in Prefect backend</li> </ul>"},{"location":"services/scheduling/#files","title":"Files","text":"<p>Files</p> <p>FileResults require passing of a serializer</p> <p>Packaged with HDF5Result and TextResult</p> <p>get_file_result task will load results with packaged types</p>"},{"location":"services/scheduling/#db-result","title":"DB Result","text":"<p>Other results can be created very easily by composing a class from the generic <code>lume_services.files.File</code> class with a custom serializer with base class <code>lume.serializers.base.SerializerBase</code>. For example:</p> <pre><code>from typing import List\nimport csv\n\nfrom lume.serializers.base import SerializerBase\nfrom lume_services.files import File\n\nclass CSVSerializer(SerializerBase):\n\n    # serialize a csv with data represented as list of list w/ fields as first element\n    def serialize(self, filename, data: List[list]):\n\n        # writing to csv file\n        with open(filename, 'w') as csvfile:\n            # creating a csv writer object\n            csvwriter = csv.writer(csvfile)\n\n            # writing the fields\n            csvwriter.writerow(list[0])\n\n            # writing the data rows\n            csvwriter.writerows(list[1:])\n\n    @classmethod\n    def deserialize(cls, filename):\n        data = []\n\n        # opening the CSV file\n        with open(filename, mode ='r')as file:\n\n            # reading the CSV file\n            csv_file = csv.reader(file)\n\n            for line in csv_file:\n                data += line\n\n\nCSVFile = File[CSVSerializer]\n</code></pre> <p>MongoDB ...</p>"},{"location":"services/scheduling/#flow-of-flows-composition","title":"Flow-of-flows composition","text":"<p>Flow-of-flows may be registered with Prefect using a YAML spec file describing the flow progression and mapping of flow outputs to input parameters. In the spec, flows are defined in semi-ordered execution, with dependencies occuring before flow definition if using upstream parameters.</p> <p>Validation with pydantic models</p> <p>FlowOfFlows </p>"}]}